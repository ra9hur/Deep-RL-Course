{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "from sample_factory.algo.utils.context import global_model_factory\n",
    "from sample_factory.cfg.arguments import parse_full_cfg, parse_sf_args\n",
    "from sample_factory.envs.env_utils import register_env\n",
    "from sample_factory.train import run_rl\n",
    "\n",
    "from sf_examples.vizdoom.doom.doom_model import make_vizdoom_encoder\n",
    "from sf_examples.vizdoom.doom.doom_params import add_doom_env_args, doom_override_defaults\n",
    "from sf_examples.vizdoom.doom.doom_utils import DOOM_ENVS, make_doom_env_from_spec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registers all the ViZDoom environments\n",
    "def register_vizdoom_envs():\n",
    "    for env_spec in DOOM_ENVS:\n",
    "        make_env_func = functools.partial(make_doom_env_from_spec, env_spec)\n",
    "        register_env(env_spec.name, make_env_func)\n",
    "\n",
    "\n",
    "# Sample Factory allows the registration of a custom Neural Network architecture\n",
    "# See https://github.com/alex-petrenko/sample-factory/blob/master/sf_examples/vizdoom/doom/doom_model.py for more details\n",
    "def register_vizdoom_models():\n",
    "    global_model_factory().register_encoder_factory(make_vizdoom_encoder)\n",
    "\n",
    "\n",
    "def register_vizdoom_components():\n",
    "    register_vizdoom_envs()\n",
    "    register_vizdoom_models()\n",
    "\n",
    "\n",
    "# parse the command line args and create a config\n",
    "def parse_vizdoom_cfg(argv=None, evaluation=False):\n",
    "    parser, _ = parse_sf_args(argv=argv, evaluation=evaluation)\n",
    "    # parameters specific to Doom envs\n",
    "    add_doom_env_args(parser)\n",
    "    # override Doom default values for algo parameters\n",
    "    doom_override_defaults(parser)\n",
    "    # second parsing pass yields the final configuration\n",
    "    final_cfg = parse_full_cfg(parser, argv)\n",
    "    return final_cfg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[2024-07-03 21:59:52,524][50933] register_encoder_factory: <function make_vizdoom_encoder at 0x720221f6a560>\u001b[0m\n",
      "\u001b[33m[2024-07-03 21:59:52,531][50933] Saved parameter configuration for experiment default_experiment not found!\u001b[0m\n",
      "\u001b[33m[2024-07-03 21:59:52,533][50933] Starting experiment from scratch!\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:52,539][50933] Experiment dir /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment already exists!\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:52,539][50933] Resuming existing experiment from /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment...\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:52,540][50933] Weights and Biases integration disabled\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:55,257][50933] Queried available GPUs: 0\n",
      "\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 21:59:55,257][50933] Environment var CUDA_VISIBLE_DEVICES is 0\n",
      "\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:57,119][52223] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-03 21:59:57,120][52223] Env info: EnvInfo(obs_space=Dict('obs': Box(0, 255, (3, 72, 128), uint8)), action_space=Discrete(5), num_agents=1, gpu_actions=False, gpu_observations=True, action_splits=None, all_discrete=None, frameskip=4, reward_shaping_scheme=None, env_info_protocol_version=1)\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:57,635][50933] Automatically setting recurrence to 32\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:57,636][50933] Starting experiment with the following configuration:\n",
      "help=False\n",
      "algo=APPO\n",
      "env=doom_health_gathering_supreme\n",
      "experiment=default_experiment\n",
      "train_dir=/home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir\n",
      "restart_behavior=resume\n",
      "device=gpu\n",
      "seed=None\n",
      "num_policies=1\n",
      "async_rl=True\n",
      "serial_mode=False\n",
      "batched_sampling=False\n",
      "num_batches_to_accumulate=2\n",
      "worker_num_splits=2\n",
      "policy_workers_per_policy=1\n",
      "max_policy_lag=1000\n",
      "num_workers=8\n",
      "num_envs_per_worker=4\n",
      "batch_size=1024\n",
      "num_batches_per_epoch=1\n",
      "num_epochs=1\n",
      "rollout=32\n",
      "recurrence=32\n",
      "shuffle_minibatches=False\n",
      "gamma=0.99\n",
      "reward_scale=1.0\n",
      "reward_clip=1000.0\n",
      "value_bootstrap=False\n",
      "normalize_returns=True\n",
      "exploration_loss_coeff=0.001\n",
      "value_loss_coeff=0.5\n",
      "kl_loss_coeff=0.0\n",
      "exploration_loss=symmetric_kl\n",
      "gae_lambda=0.95\n",
      "ppo_clip_ratio=0.1\n",
      "ppo_clip_value=0.2\n",
      "with_vtrace=False\n",
      "vtrace_rho=1.0\n",
      "vtrace_c=1.0\n",
      "optimizer=adam\n",
      "adam_eps=1e-06\n",
      "adam_beta1=0.9\n",
      "adam_beta2=0.999\n",
      "max_grad_norm=4.0\n",
      "learning_rate=0.0001\n",
      "lr_schedule=constant\n",
      "lr_schedule_kl_threshold=0.008\n",
      "lr_adaptive_min=1e-06\n",
      "lr_adaptive_max=0.01\n",
      "obs_subtract_mean=0.0\n",
      "obs_scale=255.0\n",
      "normalize_input=True\n",
      "normalize_input_keys=None\n",
      "decorrelate_experience_max_seconds=0\n",
      "decorrelate_envs_on_one_worker=True\n",
      "actor_worker_gpus=[]\n",
      "set_workers_cpu_affinity=True\n",
      "force_envs_single_thread=False\n",
      "default_niceness=0\n",
      "log_to_file=True\n",
      "experiment_summaries_interval=10\n",
      "flush_summaries_interval=30\n",
      "stats_avg=100\n",
      "summaries_use_frameskip=True\n",
      "heartbeat_interval=20\n",
      "heartbeat_reporting_interval=600\n",
      "train_for_env_steps=4000000\n",
      "train_for_seconds=10000000000\n",
      "save_every_sec=120\n",
      "keep_checkpoints=2\n",
      "load_checkpoint_kind=latest\n",
      "save_milestones_sec=-1\n",
      "save_best_every_sec=5\n",
      "save_best_metric=reward\n",
      "save_best_after=100000\n",
      "benchmark=False\n",
      "encoder_mlp_layers=[512, 512]\n",
      "encoder_conv_architecture=convnet_simple\n",
      "encoder_conv_mlp_layers=[512]\n",
      "use_rnn=True\n",
      "rnn_size=512\n",
      "rnn_type=gru\n",
      "rnn_num_layers=1\n",
      "decoder_mlp_layers=[]\n",
      "nonlinearity=elu\n",
      "policy_initialization=orthogonal\n",
      "policy_init_gain=1.0\n",
      "actor_critic_share_weights=True\n",
      "adaptive_stddev=True\n",
      "continuous_tanh_scale=0.0\n",
      "initial_stddev=1.0\n",
      "use_env_info_cache=False\n",
      "env_gpu_actions=False\n",
      "env_gpu_observations=True\n",
      "env_frameskip=4\n",
      "env_framestack=1\n",
      "pixel_format=CHW\n",
      "use_record_episode_statistics=False\n",
      "with_wandb=False\n",
      "wandb_user=None\n",
      "wandb_project=sample_factory\n",
      "wandb_group=None\n",
      "wandb_job_type=SF\n",
      "wandb_tags=[]\n",
      "with_pbt=False\n",
      "pbt_mix_policies_in_one_env=True\n",
      "pbt_period_env_steps=5000000\n",
      "pbt_start_mutation=20000000\n",
      "pbt_replace_fraction=0.3\n",
      "pbt_mutation_rate=0.15\n",
      "pbt_replace_reward_gap=0.1\n",
      "pbt_replace_reward_gap_absolute=1e-06\n",
      "pbt_optimize_gamma=False\n",
      "pbt_target_objective=true_objective\n",
      "pbt_perturb_min=1.1\n",
      "pbt_perturb_max=1.5\n",
      "num_agents=-1\n",
      "num_humans=0\n",
      "num_bots=-1\n",
      "start_bot_difficulty=None\n",
      "timelimit=None\n",
      "res_w=128\n",
      "res_h=72\n",
      "wide_aspect_ratio=False\n",
      "eval_env_frameskip=1\n",
      "fps=35\n",
      "command_line=--env=doom_health_gathering_supreme --num_workers=8 --num_envs_per_worker=4 --train_for_env_steps=4000000\n",
      "cli_args={'env': 'doom_health_gathering_supreme', 'num_workers': 8, 'num_envs_per_worker': 4, 'train_for_env_steps': 4000000}\n",
      "git_hash=unknown\n",
      "git_repo_name=not a git repository\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:57,637][50933] Saving configuration to /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/config.json...\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:57,638][50933] Rollout worker 0 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:57,639][50933] Rollout worker 1 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:57,639][50933] Rollout worker 2 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:57,639][50933] Rollout worker 3 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:57,639][50933] Rollout worker 4 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:57,640][50933] Rollout worker 5 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:57,640][50933] Rollout worker 6 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:57,640][50933] Rollout worker 7 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:57,684][50933] Using GPUs [0] for process 0 (actually maps to GPUs [0])\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 21:59:57,685][50933] InferenceWorker_p0-w0: min num requests: 2\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:57,712][50933] Starting all processes...\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:57,713][50933] Starting process learner_proc0\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:58,429][50933] Starting all processes...\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:58,435][50933] Starting process inference_proc0-0\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:58,436][50933] Starting process rollout_proc0\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:58,436][50933] Starting process rollout_proc1\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:58,436][50933] Starting process rollout_proc2\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:58,437][50933] Starting process rollout_proc3\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:58,437][50933] Starting process rollout_proc4\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:58,437][50933] Starting process rollout_proc5\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:58,438][50933] Starting process rollout_proc6\u001b[0m\n",
      "\u001b[36m[2024-07-03 21:59:58,438][50933] Starting process rollout_proc7\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,015][52280] Rollout worker 0 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:01,016][52280] ROLLOUT worker 0\tpid 52280\tparent 50933\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,016][52280] Worker 0 uses CPU cores [0, 1]\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,103][52286] Rollout worker 3 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:01,104][52286] ROLLOUT worker 3\tpid 52286\tparent 50933\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,104][52286] Worker 3 uses CPU cores [6, 7]\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,185][52287] Rollout worker 7 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:01,185][52287] ROLLOUT worker 7\tpid 52287\tparent 50933\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,197][52287] Worker 7 uses CPU cores [14, 15]\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,245][52285] Rollout worker 5 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:01,245][52285] ROLLOUT worker 5\tpid 52285\tparent 50933\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,246][52285] Worker 5 uses CPU cores [10, 11]\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,363][52281] Rollout worker 1 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:01,363][52281] ROLLOUT worker 1\tpid 52281\tparent 50933\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,369][52284] Rollout worker 4 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:01,369][52284] ROLLOUT worker 4\tpid 52284\tparent 50933\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,369][52284] Worker 4 uses CPU cores [8, 9]\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,369][52281] Worker 1 uses CPU cores [2, 3]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:01,378][52267] LearnerWorker_p0\tpid 52267\tparent 50933\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,378][52267] Using GPUs [0] for process 0 (actually maps to GPUs [0])\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:01,378][52267] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for learning process 0\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,424][52283] Rollout worker 2 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:01,424][52283] ROLLOUT worker 2\tpid 52283\tparent 50933\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,424][52267] Num visible devices: 1\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,425][52283] Worker 2 uses CPU cores [4, 5]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:01,442][52267] Starting seed is not provided\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,443][52267] Using GPUs [0] for process 0 (actually maps to GPUs [0])\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,443][52267] Initializing actor-critic model on device cuda:0\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,443][52267] RunningMeanStd input shape: (3, 72, 128)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,452][52267] RunningMeanStd input shape: (1,)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,460][52267] ConvEncoder: input_channels=3\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:01,501][52282] InferenceWorker_p0-w0\tpid 52282\tparent 50933\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,501][52282] Using GPUs [0] for process 0 (actually maps to GPUs [0])\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:01,501][52282] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for inference process 0\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,540][52267] Conv encoder output size: 512\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,540][52267] Policy head output size: 512\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,546][52282] Num visible devices: 1\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,556][52267] Created Actor Critic model with architecture:\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,556][52267] ActorCriticSharedWeights(\n",
      "  (obs_normalizer): ObservationNormalizer(\n",
      "    (running_mean_std): RunningMeanStdDictInPlace(\n",
      "      (running_mean_std): ModuleDict(\n",
      "        (obs): RunningMeanStdInPlace()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)\n",
      "  (encoder): VizdoomEncoder(\n",
      "    (basic_encoder): ConvEncoder(\n",
      "      (enc): RecursiveScriptModule(\n",
      "        original_name=ConvEncoderImpl\n",
      "        (conv_head): RecursiveScriptModule(\n",
      "          original_name=Sequential\n",
      "          (0): RecursiveScriptModule(original_name=Conv2d)\n",
      "          (1): RecursiveScriptModule(original_name=ELU)\n",
      "          (2): RecursiveScriptModule(original_name=Conv2d)\n",
      "          (3): RecursiveScriptModule(original_name=ELU)\n",
      "          (4): RecursiveScriptModule(original_name=Conv2d)\n",
      "          (5): RecursiveScriptModule(original_name=ELU)\n",
      "        )\n",
      "        (mlp_layers): RecursiveScriptModule(\n",
      "          original_name=Sequential\n",
      "          (0): RecursiveScriptModule(original_name=Linear)\n",
      "          (1): RecursiveScriptModule(original_name=ELU)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (core): ModelCoreRNN(\n",
      "    (core): GRU(512, 512)\n",
      "  )\n",
      "  (decoder): MlpDecoder(\n",
      "    (mlp): Identity()\n",
      "  )\n",
      "  (critic_linear): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (action_parameterization): ActionParameterizationDefault(\n",
      "    (distribution_linear): Linear(in_features=512, out_features=5, bias=True)\n",
      "  )\n",
      ")\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,566][52288] Rollout worker 6 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:01,566][52288] ROLLOUT worker 6\tpid 52288\tparent 50933\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,567][52288] Worker 6 uses CPU cores [12, 13]\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:01,675][52267] Using optimizer <class 'torch.optim.adam.Adam'>\u001b[0m\n",
      "\u001b[33m[2024-07-03 22:00:02,200][52267] No checkpoints found\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:02,200][52267] Did not load from checkpoint, starting from scratch!\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:02,200][52267] Initialized policy 0 weights for model version 0\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:02,202][52267] LearnerWorker_p0 finished initialization!\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:02,202][52267] Using GPUs [0] for process 0 (actually maps to GPUs [0])\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:02,268][52282] RunningMeanStd input shape: (3, 72, 128)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:02,268][52282] RunningMeanStd input shape: (1,)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:02,276][52282] ConvEncoder: input_channels=3\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:02,331][52282] Conv encoder output size: 512\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:02,331][52282] Policy head output size: 512\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:02,365][50933] Inference worker 0-0 is ready!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:02,366][50933] All inference workers are ready! Signal rollout workers to start!\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:02,406][52284] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-03 22:00:02,412][52281] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-03 22:00:02,414][52280] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-03 22:00:02,416][52285] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:02,416][52283] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:02,416][52288] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-03 22:00:02,419][52286] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:02,419][52287] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-03 22:00:02,543][50933] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 0. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:02,910][52285] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:02,910][52286] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:02,910][52284] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:02,910][52288] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:02,911][52287] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:03,061][52286] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:03,062][52284] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:03,062][52288] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:03,063][52285] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:03,228][52287] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:03,251][52281] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:03,259][52288] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:03,259][52286] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:03,269][52285] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:03,403][52281] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:03,404][52284] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:03,431][52286] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:03,431][52288] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:03,576][52287] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:03,576][52284] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:03,597][52285] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:03,597][52281] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:03,745][52287] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:03,762][52280] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:03,788][52281] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:03,916][52280] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:04,113][52283] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:04,120][52280] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:04,273][52283] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:04,455][52280] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:04,478][52283] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:04,483][52267] Signal inference workers to stop experience collection...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:04,488][52282] InferenceWorker_p0-w0: stopping experience collection\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:04,653][52283] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:05,734][52267] Signal inference workers to resume experience collection...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:05,735][52282] InferenceWorker_p0-w0: resuming experience collection\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:07,144][52282] Updated weights for policy 0, policy_version 10 (0.0096)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:07,543][50933] Fps is (10 sec: 9830.8, 60 sec: 9830.8, 300 sec: 9830.8). Total num frames: 49152. Throughput: 0: 491.6. Samples: 2458. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:07,543][50933] Avg episode reward: [(0, '4.343')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:08,657][52282] Updated weights for policy 0, policy_version 20 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:10,147][52282] Updated weights for policy 0, policy_version 30 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:11,627][52282] Updated weights for policy 0, policy_version 40 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:12,543][50933] Fps is (10 sec: 18841.9, 60 sec: 18841.9, 300 sec: 18841.9). Total num frames: 188416. Throughput: 0: 3962.1. Samples: 39620. Policy #0 lag: (min: 0.0, avg: 0.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:12,543][50933] Avg episode reward: [(0, '4.315')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:12,546][52267] Saving new best policy, reward=4.315!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:13,106][52282] Updated weights for policy 0, policy_version 50 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:14,546][52282] Updated weights for policy 0, policy_version 60 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:16,014][52282] Updated weights for policy 0, policy_version 70 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:17,463][52282] Updated weights for policy 0, policy_version 80 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:17,543][50933] Fps is (10 sec: 27852.7, 60 sec: 21845.6, 300 sec: 21845.6). Total num frames: 327680. Throughput: 0: 5455.0. Samples: 81824. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:17,543][50933] Avg episode reward: [(0, '4.443')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:17,544][52267] Saving new best policy, reward=4.443!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:17,677][50933] Heartbeat connected on Batcher_0\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:17,680][50933] Heartbeat connected on LearnerWorker_p0\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:17,687][50933] Heartbeat connected on InferenceWorker_p0-w0\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:17,691][50933] Heartbeat connected on RolloutWorker_w0\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:17,693][50933] Heartbeat connected on RolloutWorker_w1\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:17,696][50933] Heartbeat connected on RolloutWorker_w2\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:17,699][50933] Heartbeat connected on RolloutWorker_w3\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:17,703][50933] Heartbeat connected on RolloutWorker_w4\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:17,705][50933] Heartbeat connected on RolloutWorker_w5\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:17,708][50933] Heartbeat connected on RolloutWorker_w6\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:17,713][50933] Heartbeat connected on RolloutWorker_w7\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:18,934][52282] Updated weights for policy 0, policy_version 90 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:20,382][52282] Updated weights for policy 0, policy_version 100 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:21,925][52282] Updated weights for policy 0, policy_version 110 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:22,543][50933] Fps is (10 sec: 27852.9, 60 sec: 23347.5, 300 sec: 23347.5). Total num frames: 466944. Throughput: 0: 5144.2. Samples: 102882. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:22,543][50933] Avg episode reward: [(0, '4.404')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:23,477][52282] Updated weights for policy 0, policy_version 120 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:24,948][52282] Updated weights for policy 0, policy_version 130 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:26,408][52282] Updated weights for policy 0, policy_version 140 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:27,543][50933] Fps is (10 sec: 27443.4, 60 sec: 24084.7, 300 sec: 24084.7). Total num frames: 602112. Throughput: 0: 5749.5. Samples: 143736. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:27,543][50933] Avg episode reward: [(0, '4.278')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:27,842][52282] Updated weights for policy 0, policy_version 150 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:29,278][52282] Updated weights for policy 0, policy_version 160 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:30,728][52282] Updated weights for policy 0, policy_version 170 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:32,148][52282] Updated weights for policy 0, policy_version 180 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:32,543][50933] Fps is (10 sec: 27852.7, 60 sec: 24849.2, 300 sec: 24849.2). Total num frames: 745472. Throughput: 0: 6213.5. Samples: 186404. Policy #0 lag: (min: 0.0, avg: 0.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:32,543][50933] Avg episode reward: [(0, '4.483')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:32,546][52267] Saving new best policy, reward=4.483!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:33,627][52282] Updated weights for policy 0, policy_version 190 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:35,062][52282] Updated weights for policy 0, policy_version 200 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:36,504][52282] Updated weights for policy 0, policy_version 210 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:37,543][50933] Fps is (10 sec: 28672.0, 60 sec: 25395.3, 300 sec: 25395.3). Total num frames: 888832. Throughput: 0: 5928.0. Samples: 207480. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:37,543][50933] Avg episode reward: [(0, '4.433')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:37,965][52282] Updated weights for policy 0, policy_version 220 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:39,441][52282] Updated weights for policy 0, policy_version 230 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:40,885][52282] Updated weights for policy 0, policy_version 240 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:42,382][52282] Updated weights for policy 0, policy_version 250 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:42,543][50933] Fps is (10 sec: 28262.2, 60 sec: 25702.5, 300 sec: 25702.5). Total num frames: 1028096. Throughput: 0: 6235.4. Samples: 249414. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:42,543][50933] Avg episode reward: [(0, '4.472')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:43,824][52282] Updated weights for policy 0, policy_version 260 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:45,266][52282] Updated weights for policy 0, policy_version 270 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:46,707][52282] Updated weights for policy 0, policy_version 280 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:47,543][50933] Fps is (10 sec: 27852.6, 60 sec: 25941.4, 300 sec: 25941.4). Total num frames: 1167360. Throughput: 0: 6483.3. Samples: 291748. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:47,544][50933] Avg episode reward: [(0, '4.797')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:47,544][52267] Saving new best policy, reward=4.797!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:48,213][52282] Updated weights for policy 0, policy_version 290 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:49,746][52282] Updated weights for policy 0, policy_version 300 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:51,238][52282] Updated weights for policy 0, policy_version 310 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:52,543][50933] Fps is (10 sec: 27852.8, 60 sec: 26132.5, 300 sec: 26132.5). Total num frames: 1306624. Throughput: 0: 6881.9. Samples: 312146. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:52,544][50933] Avg episode reward: [(0, '4.885')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:52,546][52267] Saving new best policy, reward=4.885!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:52,687][52282] Updated weights for policy 0, policy_version 320 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:54,137][52282] Updated weights for policy 0, policy_version 330 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:55,574][52282] Updated weights for policy 0, policy_version 340 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:57,017][52282] Updated weights for policy 0, policy_version 350 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:57,543][50933] Fps is (10 sec: 27852.9, 60 sec: 26289.0, 300 sec: 26289.0). Total num frames: 1445888. Throughput: 0: 6993.8. Samples: 354340. Policy #0 lag: (min: 0.0, avg: 0.5, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:00:57,543][50933] Avg episode reward: [(0, '4.610')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:58,451][52282] Updated weights for policy 0, policy_version 360 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:00:59,994][52282] Updated weights for policy 0, policy_version 370 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:01,493][52282] Updated weights for policy 0, policy_version 380 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:01:02,543][50933] Fps is (10 sec: 27443.1, 60 sec: 26351.0, 300 sec: 26351.0). Total num frames: 1581056. Throughput: 0: 6970.4. Samples: 395494. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:01:02,544][50933] Avg episode reward: [(0, '4.584')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:03,020][52282] Updated weights for policy 0, policy_version 390 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:04,454][52282] Updated weights for policy 0, policy_version 400 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:05,889][52282] Updated weights for policy 0, policy_version 410 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:07,336][52282] Updated weights for policy 0, policy_version 420 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:01:07,543][50933] Fps is (10 sec: 27852.7, 60 sec: 27921.0, 300 sec: 26529.5). Total num frames: 1724416. Throughput: 0: 6976.0. Samples: 416802. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:01:07,543][50933] Avg episode reward: [(0, '4.922')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:07,544][52267] Saving new best policy, reward=4.922!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:08,773][52282] Updated weights for policy 0, policy_version 430 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:10,227][52282] Updated weights for policy 0, policy_version 440 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:11,666][52282] Updated weights for policy 0, policy_version 450 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:01:12,543][50933] Fps is (10 sec: 28672.2, 60 sec: 27989.3, 300 sec: 26682.6). Total num frames: 1867776. Throughput: 0: 7014.9. Samples: 459406. Policy #0 lag: (min: 0.0, avg: 0.6, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:01:12,543][50933] Avg episode reward: [(0, '4.441')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:13,110][52282] Updated weights for policy 0, policy_version 460 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:14,584][52282] Updated weights for policy 0, policy_version 470 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:16,030][52282] Updated weights for policy 0, policy_version 480 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:17,495][52282] Updated weights for policy 0, policy_version 490 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:01:17,543][50933] Fps is (10 sec: 28262.5, 60 sec: 27989.3, 300 sec: 26760.6). Total num frames: 2007040. Throughput: 0: 7008.0. Samples: 501762. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:01:17,543][50933] Avg episode reward: [(0, '4.585')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:18,991][52282] Updated weights for policy 0, policy_version 500 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:20,449][52282] Updated weights for policy 0, policy_version 510 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:21,967][52282] Updated weights for policy 0, policy_version 520 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:01:22,543][50933] Fps is (10 sec: 27442.9, 60 sec: 27921.0, 300 sec: 26777.6). Total num frames: 2142208. Throughput: 0: 7002.1. Samples: 522574. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:01:22,544][50933] Avg episode reward: [(0, '4.604')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:23,448][52282] Updated weights for policy 0, policy_version 530 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:24,898][52282] Updated weights for policy 0, policy_version 540 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:26,351][52282] Updated weights for policy 0, policy_version 550 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:01:27,543][50933] Fps is (10 sec: 27852.8, 60 sec: 28057.6, 300 sec: 26889.1). Total num frames: 2285568. Throughput: 0: 6995.5. Samples: 564212. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:01:27,543][50933] Avg episode reward: [(0, '4.713')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:27,808][52282] Updated weights for policy 0, policy_version 560 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:29,252][52282] Updated weights for policy 0, policy_version 570 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:30,694][52282] Updated weights for policy 0, policy_version 580 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:32,150][52282] Updated weights for policy 0, policy_version 590 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:01:32,543][50933] Fps is (10 sec: 28262.5, 60 sec: 27989.3, 300 sec: 26942.6). Total num frames: 2424832. Throughput: 0: 6997.7. Samples: 606646. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:01:32,543][50933] Avg episode reward: [(0, '4.567')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:33,604][52282] Updated weights for policy 0, policy_version 600 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:35,046][52282] Updated weights for policy 0, policy_version 610 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:36,494][52282] Updated weights for policy 0, policy_version 620 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:01:37,543][50933] Fps is (10 sec: 28262.5, 60 sec: 27989.3, 300 sec: 27033.7). Total num frames: 2568192. Throughput: 0: 7013.8. Samples: 627768. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:01:37,543][50933] Avg episode reward: [(0, '4.946')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:37,544][52267] Saving new best policy, reward=4.946!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:37,956][52282] Updated weights for policy 0, policy_version 630 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:39,427][52282] Updated weights for policy 0, policy_version 640 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:40,877][52282] Updated weights for policy 0, policy_version 650 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:42,330][52282] Updated weights for policy 0, policy_version 660 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:01:42,543][50933] Fps is (10 sec: 28262.4, 60 sec: 27989.3, 300 sec: 27074.6). Total num frames: 2707456. Throughput: 0: 7012.8. Samples: 669918. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:01:42,544][50933] Avg episode reward: [(0, '5.278')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:42,546][52267] Saving new best policy, reward=5.278!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:43,776][52282] Updated weights for policy 0, policy_version 670 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:45,229][52282] Updated weights for policy 0, policy_version 680 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:46,677][52282] Updated weights for policy 0, policy_version 690 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:01:47,543][50933] Fps is (10 sec: 27852.5, 60 sec: 27989.3, 300 sec: 27111.6). Total num frames: 2846720. Throughput: 0: 7042.0. Samples: 712382. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:01:47,544][50933] Avg episode reward: [(0, '4.903')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:48,130][52282] Updated weights for policy 0, policy_version 700 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:49,575][52282] Updated weights for policy 0, policy_version 710 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:51,030][52282] Updated weights for policy 0, policy_version 720 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:52,479][52282] Updated weights for policy 0, policy_version 730 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:01:52,543][50933] Fps is (10 sec: 28262.5, 60 sec: 28057.6, 300 sec: 27182.6). Total num frames: 2990080. Throughput: 0: 7039.1. Samples: 733560. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:01:52,544][50933] Avg episode reward: [(0, '4.446')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:52,546][52267] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000000730_2990080.pth...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:53,969][52282] Updated weights for policy 0, policy_version 740 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:55,412][52282] Updated weights for policy 0, policy_version 750 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:56,890][52282] Updated weights for policy 0, policy_version 760 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:01:57,543][50933] Fps is (10 sec: 28262.5, 60 sec: 28057.6, 300 sec: 27211.7). Total num frames: 3129344. Throughput: 0: 7028.6. Samples: 775692. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:01:57,544][50933] Avg episode reward: [(0, '4.802')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:58,332][52282] Updated weights for policy 0, policy_version 770 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:01:59,764][52282] Updated weights for policy 0, policy_version 780 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:01,218][52282] Updated weights for policy 0, policy_version 790 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:02,543][50933] Fps is (10 sec: 27852.8, 60 sec: 28125.9, 300 sec: 27238.4). Total num frames: 3268608. Throughput: 0: 7022.9. Samples: 817794. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:02,544][50933] Avg episode reward: [(0, '5.027')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:02,701][52282] Updated weights for policy 0, policy_version 800 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:04,196][52282] Updated weights for policy 0, policy_version 810 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:05,648][52282] Updated weights for policy 0, policy_version 820 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:07,099][52282] Updated weights for policy 0, policy_version 830 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:07,543][50933] Fps is (10 sec: 28262.3, 60 sec: 28125.8, 300 sec: 27295.8). Total num frames: 3411968. Throughput: 0: 7021.3. Samples: 838534. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:07,543][50933] Avg episode reward: [(0, '4.713')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:08,556][52282] Updated weights for policy 0, policy_version 840 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:10,015][52282] Updated weights for policy 0, policy_version 850 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:11,468][52282] Updated weights for policy 0, policy_version 860 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:12,543][50933] Fps is (10 sec: 28262.4, 60 sec: 28057.6, 300 sec: 27317.2). Total num frames: 3551232. Throughput: 0: 7036.9. Samples: 880872. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:12,543][50933] Avg episode reward: [(0, '4.729')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:12,923][52282] Updated weights for policy 0, policy_version 870 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:14,392][52282] Updated weights for policy 0, policy_version 880 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:15,852][52282] Updated weights for policy 0, policy_version 890 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:17,339][52282] Updated weights for policy 0, policy_version 900 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:17,543][50933] Fps is (10 sec: 27852.9, 60 sec: 28057.6, 300 sec: 27337.0). Total num frames: 3690496. Throughput: 0: 7021.6. Samples: 922616. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:17,543][50933] Avg episode reward: [(0, '4.525')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:18,797][52282] Updated weights for policy 0, policy_version 910 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:20,247][52282] Updated weights for policy 0, policy_version 920 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:21,693][52282] Updated weights for policy 0, policy_version 930 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:22,543][50933] Fps is (10 sec: 27852.3, 60 sec: 28125.8, 300 sec: 27355.4). Total num frames: 3829760. Throughput: 0: 7023.3. Samples: 943816. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:22,544][50933] Avg episode reward: [(0, '4.576')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:23,161][52282] Updated weights for policy 0, policy_version 940 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:24,604][52282] Updated weights for policy 0, policy_version 950 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:26,058][52282] Updated weights for policy 0, policy_version 960 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:27,515][52282] Updated weights for policy 0, policy_version 970 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:27,543][50933] Fps is (10 sec: 28262.6, 60 sec: 28125.9, 300 sec: 27400.9). Total num frames: 3973120. Throughput: 0: 7024.6. Samples: 986024. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:27,543][50933] Avg episode reward: [(0, '4.954')]\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,661][50933] Component Batcher_0 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,661][52267] Stopping Batcher_0...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,661][52267] Loop batcher_evt_loop terminating...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:28,661][52267] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,670][50933] Component RolloutWorker_w4 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,670][52284] Stopping RolloutWorker_w4...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,670][52286] Stopping RolloutWorker_w3...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,670][52285] Stopping RolloutWorker_w5...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,670][52284] Loop rollout_proc4_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,670][52286] Loop rollout_proc3_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,670][52285] Loop rollout_proc5_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,670][52281] Stopping RolloutWorker_w1...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,670][52281] Loop rollout_proc1_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,670][52288] Stopping RolloutWorker_w6...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,670][52287] Stopping RolloutWorker_w7...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,670][52288] Loop rollout_proc6_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,670][52283] Stopping RolloutWorker_w2...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,671][50933] Component RolloutWorker_w3 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,670][52287] Loop rollout_proc7_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,671][52283] Loop rollout_proc2_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,671][52280] Stopping RolloutWorker_w0...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,671][52280] Loop rollout_proc0_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,671][50933] Component RolloutWorker_w5 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,672][50933] Component RolloutWorker_w1 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,673][50933] Component RolloutWorker_w6 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,674][50933] Component RolloutWorker_w7 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,674][50933] Component RolloutWorker_w2 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,675][50933] Component RolloutWorker_w0 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,688][52282] Weights refcount: 2 0\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,690][50933] Component InferenceWorker_p0-w0 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,690][52282] Stopping InferenceWorker_p0-w0...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,690][52282] Loop inference_proc0-0_evt_loop terminating...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:28,726][52267] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000000978_4005888.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,812][50933] Component LearnerWorker_p0 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,812][52267] Stopping LearnerWorker_p0...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,813][52267] Loop learner_proc0_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:28,814][50933] Waiting for process learner_proc0 to stop...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:29,814][50933] Waiting for process inference_proc0-0 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:29,814][50933] Waiting for process rollout_proc0 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:29,815][50933] Waiting for process rollout_proc1 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:29,816][50933] Waiting for process rollout_proc2 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:29,817][50933] Waiting for process rollout_proc3 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:29,817][50933] Waiting for process rollout_proc4 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:29,817][50933] Waiting for process rollout_proc5 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:29,818][50933] Waiting for process rollout_proc6 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:29,818][50933] Waiting for process rollout_proc7 to join...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:29,818][50933] Batcher 0 profile tree view:\n",
      "batching: 5.8192, releasing_batches: 0.0206\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:29,818][50933] InferenceWorker_p0-w0 profile tree view:\n",
      "wait_policy: 0.0000\n",
      "  wait_policy_total: 2.6022\n",
      "update_model: 2.1613\n",
      "  weight_update: 0.0006\n",
      "one_step: 0.0022\n",
      "  handle_policy_step: 133.9007\n",
      "    deserialize: 5.6377, stack: 0.7367, obs_to_device_normalize: 28.7127, forward: 73.9404, send_messages: 6.5120\n",
      "    prepare_outputs: 13.6292\n",
      "      to_cpu: 7.9695\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:29,819][50933] Learner 0 profile tree view:\n",
      "misc: 0.0036, prepare_batch: 8.9830\n",
      "train: 20.5424\n",
      "  epoch_init: 0.0030, minibatch_init: 0.0042, losses_postprocess: 0.1365, kl_divergence: 0.1275, after_optimizer: 8.7303\n",
      "  calculate_losses: 7.5125\n",
      "    losses_init: 0.0018, forward_head: 0.4894, bptt_initial: 5.4018, tail: 0.3618, advantages_returns: 0.0883, losses: 0.4649\n",
      "    bptt: 0.6121\n",
      "      bptt_forward_core: 0.5857\n",
      "  update: 3.8011\n",
      "    clip: 0.4336\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:29,819][50933] RolloutWorker_w0 profile tree view:\n",
      "wait_for_trajectories: 0.0687, enqueue_policy_requests: 4.1133, env_step: 59.0065, overhead: 4.4516, complete_rollouts: 0.1292\n",
      "save_policy_outputs: 3.8450\n",
      "  split_output_tensors: 1.8805\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:29,819][50933] RolloutWorker_w7 profile tree view:\n",
      "wait_for_trajectories: 0.0727, enqueue_policy_requests: 4.1815, env_step: 55.6920, overhead: 4.5741, complete_rollouts: 0.1285\n",
      "save_policy_outputs: 3.9624\n",
      "  split_output_tensors: 1.9538\u001b[0m\n",
      "\u001b[36m[2024-07-03 22:02:29,820][50933] Loop Runner_EvtLoop terminating...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:29,820][50933] Runner profile tree view:\n",
      "main_loop: 152.1079\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-03 22:02:29,820][50933] Collected {0: 4005888}, FPS: 26335.8\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Start the training, this should take around 15 minutes\n",
    "register_vizdoom_components()\n",
    "\n",
    "# The scenario we train on today is health gathering\n",
    "# other scenarios include \"doom_basic\", \"doom_two_colors_easy\", \"doom_dm\", \"doom_dwango5\", \"doom_my_way_home\", \"doom_deadly_corridor\", \"doom_defend_the_center\", \"doom_defend_the_line\"\n",
    "env = \"doom_health_gathering_supreme\"\n",
    "cfg = parse_vizdoom_cfg(\n",
    "    argv=[f\"--env={env}\", \"--num_workers=8\", \"--num_envs_per_worker=4\", \"--train_for_env_steps=20000000\"]\n",
    ")\n",
    "\n",
    "# status = run_rl(cfg)\n",
    "run_rl(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \"--algo\"=\"APPO\",                              Default\n",
    "    \"--env\"=\"doom_health_gathering_supreme\",\n",
    "    \"--experiment\"=\"vizdoom_doom_health_gathering_supreme_2222\",\n",
    "    \"--train_dir\"=\"/scratch/sample_factory/train_dir/vizdoom\",\n",
    "    \"--seed=2222\",\n",
    "    \"--num_policies=1\",                           Default\n",
    "    \"--num_workers=20\",                           12\n",
    "    \"--num_envs_per_worker=12\",                   2   Number of envs on a single CPU actor, in high throughput configurations this should be in 10-30 range for \n",
    "                                                      Atari/VizDoomMust be even for double-buffered sampling! (default: 2)\n",
    "    \"--batch_size=2048\",                          1024\n",
    "    \"--num_epochs=1\",                             Default\n",
    "    \"--rollout=32\",                               Default\n",
    "    \"--recurrence=32\",                            Default value (-1) sets recurrence to rollout length for RNNs and to 1 (no recurrence) for feed-forward nets.\n",
    "    \"--gamma=0.99\",                               Default\n",
    "    \"--max_grad_norm=0.0\",                        Max L2 norm of the gradient vector, set to 0 to disable gradient clipping (default: 4.0)\n",
    "    \"--decorrelate_experience_max_seconds=1\",     0\n",
    "    \"--heartbeat_reporting_interval=300\",         180     How often in seconds the runner checks for heartbeats\n",
    "    \"--train_for_seconds=3600000\",                10000000000     Stop training after this many seconds\n",
    "    \"--benchmark=false\",                          Default\n",
    "    \"--use_rnn=true\",                             Default\n",
    "    \"--rnn_type=\"lstm\"\",                          gru\n",
    "    \"--nonlinearity=\"relu\"\"                       elu\n",
    "\n",
    "    \"--num_batches_per_epoch\"                     1\n",
    "\n",
    "  --encoder_conv_architecture {convnet_simple,convnet_impala,convnet_atari,resnet_impala}\n",
    "                        Architecture of the convolutional encoder. See\n",
    "                        models.py for details. VizDoom and DMLab examples\n",
    "                        demonstrate how to define custom architectures.\n",
    "                        (default: convnet_simple)\n",
    "\n",
    "sampling_size = num_workers * num_envs_per_worker * rollout\n",
    "\n",
    "If sampling_size >> batch_size then we will need many iterations of training to go through the data, which will make some experience stale by the time it is used for training (policy lag)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above six parameters (batch_size, num_batches_per_epoch, rollout, num_epochs, num_workers, num_envs_per_worker) have the biggest influence on the data regime of the RL algorithm and thus on the sample efficiency and the training speed.\n",
    "\n",
    "num_workers, num_envs_per_worker, and rollout define how many samples are collected per iteration (one rollout for all envs), which is sampling_size = num_workers * num_envs_per_worker * rollout (note that this is further multiplied by env's num_agents for multi-agent envs).\n",
    "\n",
    "batch_size and num_batches_per_epoch define how many samples are used for training per iteration.\n",
    "\n",
    "If sampling_size >> batch_size then we will need many iterations of training to go through the data, which will make some experience stale by the time it is used for training (policy lag). See Policy Lag for additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[2024-07-05 14:54:06,141][03423] register_encoder_factory: <function make_vizdoom_encoder at 0x7e40f2f2b9a0>\u001b[0m\n",
      "\u001b[33m[2024-07-05 14:54:06,153][03423] Loading existing experiment configuration from /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/config.json\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:06,154][03423] Overriding arg 'train_for_env_steps' with value 600000000 passed from command line\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:06,163][03423] Experiment dir /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment already exists!\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:06,164][03423] Resuming existing experiment from /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment...\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:06,164][03423] Weights and Biases integration disabled\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:09,617][03423] Queried available GPUs: 0\n",
      "\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:09,618][03423] Environment var CUDA_VISIBLE_DEVICES is 0\n",
      "\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:11,536][03911] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-05 14:54:11,537][03911] Env info: EnvInfo(obs_space=Dict('obs': Box(0, 255, (3, 72, 128), uint8)), action_space=Discrete(5), num_agents=1, gpu_actions=False, gpu_observations=True, action_splits=None, all_discrete=None, frameskip=4, reward_shaping_scheme=None, env_info_protocol_version=1)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:11,990][03423] Starting experiment with the following configuration:\n",
      "help=False\n",
      "algo=APPO\n",
      "env=doom_health_gathering_supreme\n",
      "experiment=default_experiment\n",
      "train_dir=/home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir\n",
      "restart_behavior=resume\n",
      "device=gpu\n",
      "seed=200\n",
      "num_policies=1\n",
      "async_rl=True\n",
      "serial_mode=False\n",
      "batched_sampling=False\n",
      "num_batches_to_accumulate=2\n",
      "worker_num_splits=2\n",
      "policy_workers_per_policy=1\n",
      "max_policy_lag=1000\n",
      "num_workers=16\n",
      "num_envs_per_worker=8\n",
      "batch_size=2048\n",
      "num_batches_per_epoch=1\n",
      "num_epochs=1\n",
      "rollout=32\n",
      "recurrence=32\n",
      "shuffle_minibatches=False\n",
      "gamma=0.99\n",
      "reward_scale=1.0\n",
      "reward_clip=1000.0\n",
      "value_bootstrap=False\n",
      "normalize_returns=True\n",
      "exploration_loss_coeff=0.001\n",
      "value_loss_coeff=0.5\n",
      "kl_loss_coeff=0.0\n",
      "exploration_loss=symmetric_kl\n",
      "gae_lambda=0.95\n",
      "ppo_clip_ratio=0.1\n",
      "ppo_clip_value=0.2\n",
      "with_vtrace=False\n",
      "vtrace_rho=1.0\n",
      "vtrace_c=1.0\n",
      "optimizer=adam\n",
      "adam_eps=1e-06\n",
      "adam_beta1=0.9\n",
      "adam_beta2=0.999\n",
      "max_grad_norm=4.0\n",
      "learning_rate=0.0001\n",
      "lr_schedule=constant\n",
      "lr_schedule_kl_threshold=0.008\n",
      "lr_adaptive_min=1e-06\n",
      "lr_adaptive_max=0.01\n",
      "obs_subtract_mean=0.0\n",
      "obs_scale=255.0\n",
      "normalize_input=True\n",
      "normalize_input_keys=None\n",
      "decorrelate_experience_max_seconds=0\n",
      "decorrelate_envs_on_one_worker=True\n",
      "actor_worker_gpus=[]\n",
      "set_workers_cpu_affinity=True\n",
      "force_envs_single_thread=False\n",
      "default_niceness=0\n",
      "log_to_file=True\n",
      "experiment_summaries_interval=10\n",
      "flush_summaries_interval=30\n",
      "stats_avg=100\n",
      "summaries_use_frameskip=True\n",
      "heartbeat_interval=20\n",
      "heartbeat_reporting_interval=600\n",
      "train_for_env_steps=600000000\n",
      "train_for_seconds=10000000000\n",
      "save_every_sec=120\n",
      "keep_checkpoints=2\n",
      "load_checkpoint_kind=latest\n",
      "save_milestones_sec=-1\n",
      "save_best_every_sec=5\n",
      "save_best_metric=reward\n",
      "save_best_after=100000\n",
      "benchmark=False\n",
      "encoder_mlp_layers=[512, 512]\n",
      "encoder_conv_architecture=convnet_simple\n",
      "encoder_conv_mlp_layers=[512]\n",
      "use_rnn=True\n",
      "rnn_size=512\n",
      "rnn_type=gru\n",
      "rnn_num_layers=1\n",
      "decoder_mlp_layers=[]\n",
      "nonlinearity=elu\n",
      "policy_initialization=orthogonal\n",
      "policy_init_gain=1.0\n",
      "actor_critic_share_weights=True\n",
      "adaptive_stddev=True\n",
      "continuous_tanh_scale=0.0\n",
      "initial_stddev=1.0\n",
      "use_env_info_cache=False\n",
      "env_gpu_actions=False\n",
      "env_gpu_observations=True\n",
      "env_frameskip=4\n",
      "env_framestack=1\n",
      "pixel_format=CHW\n",
      "use_record_episode_statistics=False\n",
      "with_wandb=False\n",
      "wandb_user=None\n",
      "wandb_project=sample_factory\n",
      "wandb_group=None\n",
      "wandb_job_type=SF\n",
      "wandb_tags=[]\n",
      "with_pbt=False\n",
      "pbt_mix_policies_in_one_env=True\n",
      "pbt_period_env_steps=5000000\n",
      "pbt_start_mutation=20000000\n",
      "pbt_replace_fraction=0.3\n",
      "pbt_mutation_rate=0.15\n",
      "pbt_replace_reward_gap=0.1\n",
      "pbt_replace_reward_gap_absolute=1e-06\n",
      "pbt_optimize_gamma=False\n",
      "pbt_target_objective=true_objective\n",
      "pbt_perturb_min=1.1\n",
      "pbt_perturb_max=1.5\n",
      "num_agents=-1\n",
      "num_humans=0\n",
      "num_bots=-1\n",
      "start_bot_difficulty=None\n",
      "timelimit=None\n",
      "res_w=128\n",
      "res_h=72\n",
      "wide_aspect_ratio=False\n",
      "eval_env_frameskip=1\n",
      "fps=35\n",
      "command_line=--env=doom_health_gathering_supreme --num_workers=8 --num_envs_per_worker=4 --train_for_env_steps=20000000\n",
      "cli_args={'env': 'doom_health_gathering_supreme', 'num_workers': 8, 'num_envs_per_worker': 4, 'train_for_env_steps': 20000000}\n",
      "git_hash=unknown\n",
      "git_repo_name=not a git repository\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:11,991][03423] Saving configuration to /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/config.json...\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:11,992][03423] Rollout worker 0 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:11,992][03423] Rollout worker 1 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:11,993][03423] Rollout worker 2 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:11,993][03423] Rollout worker 3 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:11,993][03423] Rollout worker 4 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:11,994][03423] Rollout worker 5 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:11,994][03423] Rollout worker 6 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:11,994][03423] Rollout worker 7 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:11,995][03423] Rollout worker 8 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:11,995][03423] Rollout worker 9 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:11,995][03423] Rollout worker 10 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:11,996][03423] Rollout worker 11 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:11,996][03423] Rollout worker 12 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:11,996][03423] Rollout worker 13 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:11,996][03423] Rollout worker 14 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:11,997][03423] Rollout worker 15 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:12,107][03423] Using GPUs [0] for process 0 (actually maps to GPUs [0])\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:12,108][03423] InferenceWorker_p0-w0: min num requests: 5\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:12,180][03423] Starting all processes...\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:12,181][03423] Starting process learner_proc0\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:12,753][03423] Starting all processes...\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:12,759][03423] Starting process inference_proc0-0\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:12,761][03423] Starting process rollout_proc0\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:12,761][03423] Starting process rollout_proc1\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:12,762][03423] Starting process rollout_proc2\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:12,763][03423] Starting process rollout_proc3\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:12,763][03423] Starting process rollout_proc4\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:12,768][03423] Starting process rollout_proc5\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:12,770][03423] Starting process rollout_proc6\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:12,771][03423] Starting process rollout_proc7\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:12,772][03423] Starting process rollout_proc8\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:12,774][03423] Starting process rollout_proc9\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:12,774][03423] Starting process rollout_proc10\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:12,774][03423] Starting process rollout_proc11\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:12,774][03423] Starting process rollout_proc12\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:12,776][03423] Starting process rollout_proc13\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:12,776][03423] Starting process rollout_proc14\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:12,810][03423] Starting process rollout_proc15\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:16,940][04007] Rollout worker 15 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:16,941][04007] ROLLOUT worker 15\tpid 4007\tparent 3423\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:16,942][04007] Worker 15 uses CPU cores [15]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:17,112][03956] LearnerWorker_p0\tpid 3956\tparent 3423\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,113][03956] Using GPUs [0] for process 0 (actually maps to GPUs [0])\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:17,114][03956] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for learning process 0\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:17,152][03976] InferenceWorker_p0-w0\tpid 3976\tparent 3423\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,153][03976] Using GPUs [0] for process 0 (actually maps to GPUs [0])\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:17,153][03976] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for inference process 0\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,156][04005] Rollout worker 12 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:17,156][04005] ROLLOUT worker 12\tpid 4005\tparent 3423\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,165][04005] Worker 12 uses CPU cores [12]\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,191][03956] Num visible devices: 1\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,198][03984] Rollout worker 8 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:17,198][03984] ROLLOUT worker 8\tpid 3984\tparent 3423\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,206][03984] Worker 8 uses CPU cores [8]\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,246][03979] Rollout worker 3 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:17,246][03979] ROLLOUT worker 3\tpid 3979\tparent 3423\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:17,249][03956] Setting fixed seed 200\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,249][03976] Num visible devices: 1\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,254][03979] Worker 3 uses CPU cores [3]\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,257][03980] Rollout worker 4 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:17,258][03980] ROLLOUT worker 4\tpid 3980\tparent 3423\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,265][03956] Using GPUs [0] for process 0 (actually maps to GPUs [0])\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,265][03956] Initializing actor-critic model on device cuda:0\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,266][03956] RunningMeanStd input shape: (3, 72, 128)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,266][03980] Worker 4 uses CPU cores [4]\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,268][03956] RunningMeanStd input shape: (1,)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,282][04006] Rollout worker 14 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:17,282][04006] ROLLOUT worker 14\tpid 4006\tparent 3423\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,282][03956] ConvEncoder: input_channels=3\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,298][04006] Worker 14 uses CPU cores [14]\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,313][04004] Rollout worker 13 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:17,313][04004] ROLLOUT worker 13\tpid 4004\tparent 3423\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,314][04004] Worker 13 uses CPU cores [13]\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,346][03978] Rollout worker 1 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:17,346][03978] ROLLOUT worker 1\tpid 3978\tparent 3423\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,354][03978] Worker 1 uses CPU cores [1]\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,386][03985] Rollout worker 7 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:17,386][03985] ROLLOUT worker 7\tpid 3985\tparent 3423\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,390][03985] Worker 7 uses CPU cores [7]\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,403][03981] Rollout worker 2 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:17,403][03981] ROLLOUT worker 2\tpid 3981\tparent 3423\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,406][03981] Worker 2 uses CPU cores [2]\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,421][03982] Rollout worker 6 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:17,421][03982] ROLLOUT worker 6\tpid 3982\tparent 3423\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,422][03982] Worker 6 uses CPU cores [6]\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,426][03956] Conv encoder output size: 512\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,427][03956] Policy head output size: 512\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,435][03986] Rollout worker 10 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:17,435][03986] ROLLOUT worker 10\tpid 3986\tparent 3423\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,436][03987] Rollout worker 11 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:17,436][03987] ROLLOUT worker 11\tpid 3987\tparent 3423\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,436][03987] Worker 11 uses CPU cores [11]\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,438][03986] Worker 10 uses CPU cores [10]\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,450][03956] Created Actor Critic model with architecture:\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,450][03956] ActorCriticSharedWeights(\n",
      "  (obs_normalizer): ObservationNormalizer(\n",
      "    (running_mean_std): RunningMeanStdDictInPlace(\n",
      "      (running_mean_std): ModuleDict(\n",
      "        (obs): RunningMeanStdInPlace()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)\n",
      "  (encoder): VizdoomEncoder(\n",
      "    (basic_encoder): ConvEncoder(\n",
      "      (enc): RecursiveScriptModule(\n",
      "        original_name=ConvEncoderImpl\n",
      "        (conv_head): RecursiveScriptModule(\n",
      "          original_name=Sequential\n",
      "          (0): RecursiveScriptModule(original_name=Conv2d)\n",
      "          (1): RecursiveScriptModule(original_name=ELU)\n",
      "          (2): RecursiveScriptModule(original_name=Conv2d)\n",
      "          (3): RecursiveScriptModule(original_name=ELU)\n",
      "          (4): RecursiveScriptModule(original_name=Conv2d)\n",
      "          (5): RecursiveScriptModule(original_name=ELU)\n",
      "        )\n",
      "        (mlp_layers): RecursiveScriptModule(\n",
      "          original_name=Sequential\n",
      "          (0): RecursiveScriptModule(original_name=Linear)\n",
      "          (1): RecursiveScriptModule(original_name=ELU)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (core): ModelCoreRNN(\n",
      "    (core): GRU(512, 512)\n",
      "  )\n",
      "  (decoder): MlpDecoder(\n",
      "    (mlp): Identity()\n",
      "  )\n",
      "  (critic_linear): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (action_parameterization): ActionParameterizationDefault(\n",
      "    (distribution_linear): Linear(in_features=512, out_features=5, bias=True)\n",
      "  )\n",
      ")\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,550][03983] Rollout worker 5 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:17,551][03983] ROLLOUT worker 5\tpid 3983\tparent 3423\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,551][03983] Worker 5 uses CPU cores [5]\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,583][03977] Rollout worker 0 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:17,583][03977] ROLLOUT worker 0\tpid 3977\tparent 3423\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,583][03977] Worker 0 uses CPU cores [0]\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,589][03956] Using optimizer <class 'torch.optim.adam.Adam'>\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,617][03988] Rollout worker 9 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:17,617][03988] ROLLOUT worker 9\tpid 3988\tparent 3423\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:17,618][03988] Worker 9 uses CPU cores [9]\u001b[0m\n",
      "\u001b[33m[2024-07-05 14:54:18,093][03956] Loading state from checkpoint /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000057375_450011136.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:18,140][03956] Loading model from checkpoint\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:18,141][03956] Loaded experiment state at self.train_step=57375, self.env_steps=450011136\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:18,141][03956] Initialized policy 0 weights for model version 57375\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:18,142][03956] LearnerWorker_p0 finished initialization!\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:18,142][03956] Using GPUs [0] for process 0 (actually maps to GPUs [0])\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:18,230][03976] RunningMeanStd input shape: (3, 72, 128)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:18,230][03976] RunningMeanStd input shape: (1,)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:18,237][03976] ConvEncoder: input_channels=3\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:18,292][03976] Conv encoder output size: 512\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:18,292][03976] Policy head output size: 512\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:18,328][03423] Inference worker 0-0 is ready!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:18,328][03423] All inference workers are ready! Signal rollout workers to start!\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:18,399][03983] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-05 14:54:18,404][03985] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:18,405][03988] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:18,405][04006] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:18,406][03979] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:18,406][04005] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-05 14:54:18,407][03978] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-05 14:54:18,408][03977] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-05 14:54:18,410][03984] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:18,411][03987] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-05 14:54:18,414][03981] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:18,418][03986] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:18,420][03980] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:18,422][04007] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:18,423][03982] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:18,423][04004] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:18,783][03987] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,130][03980] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,132][04006] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,132][03985] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,133][03978] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,133][04005] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,135][03988] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,135][03979] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,137][03984] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,322][03988] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,326][03979] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,335][03987] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,341][03977] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,385][04004] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,386][04007] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,388][04005] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,512][03984] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,513][03985] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,519][03977] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,523][03987] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,567][03981] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,567][04004] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,588][03983] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,722][03985] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,743][03984] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,758][03983] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,765][03977] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,770][03978] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,833][03982] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,922][03987] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,948][03983] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,968][03977] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:19,991][04005] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,017][03988] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,021][04004] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,172][03982] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,196][03980] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,207][03981] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,229][03986] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,233][03978] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,234][03983] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,254][04004] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,425][04005] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,428][03982] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,436][04006] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,436][03977] Decorrelating experience for 128 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,476][03987] Decorrelating experience for 128 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,528][03983] Decorrelating experience for 128 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,551][03986] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,651][03979] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,667][03978] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,673][04004] Decorrelating experience for 128 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,696][03988] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,724][03982] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,741][03985] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,756][03987] Decorrelating experience for 160 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,912][04007] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,932][03984] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,938][03983] Decorrelating experience for 160 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,957][03981] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:20,964][03979] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:21,012][03986] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:21,047][03980] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:21,113][03982] Decorrelating experience for 128 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:21,163][04006] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:21,168][03423] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 450011136. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:21,176][04005] Decorrelating experience for 128 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:21,182][03987] Decorrelating experience for 192 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:21,222][03988] Decorrelating experience for 128 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:21,272][03981] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:21,350][03985] Decorrelating experience for 128 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:21,412][03979] Decorrelating experience for 128 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:21,415][03984] Decorrelating experience for 128 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:21,450][03980] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:21,466][03982] Decorrelating experience for 160 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:21,474][04005] Decorrelating experience for 160 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:21,526][03983] Decorrelating experience for 192 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:21,547][03987] Decorrelating experience for 224 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:21,600][03988] Decorrelating experience for 160 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:21,664][04004] Decorrelating experience for 160 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:21,711][03979] Decorrelating experience for 160 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:21,716][03977] Decorrelating experience for 160 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:21,779][03984] Decorrelating experience for 160 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:21,836][03982] Decorrelating experience for 192 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:21,932][03983] Decorrelating experience for 224 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:21,960][03985] Decorrelating experience for 160 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:21,974][03980] Decorrelating experience for 128 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:22,030][04004] Decorrelating experience for 192 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:22,069][04006] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:22,069][04007] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:22,083][03977] Decorrelating experience for 192 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:22,174][04005] Decorrelating experience for 192 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:22,217][03988] Decorrelating experience for 192 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:22,297][03981] Decorrelating experience for 128 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:22,297][03979] Decorrelating experience for 192 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:22,299][03980] Decorrelating experience for 160 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:22,310][03986] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:22,462][03982] Decorrelating experience for 224 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:22,467][04006] Decorrelating experience for 128 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:22,541][03988] Decorrelating experience for 224 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:22,554][04005] Decorrelating experience for 224 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:22,588][04007] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:22,594][04004] Decorrelating experience for 224 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:22,622][03980] Decorrelating experience for 192 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:22,836][03985] Decorrelating experience for 192 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:22,866][03986] Decorrelating experience for 128 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:22,888][03978] Decorrelating experience for 128 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:22,944][03981] Decorrelating experience for 160 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:22,996][03979] Decorrelating experience for 224 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:23,179][03984] Decorrelating experience for 192 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:23,269][03977] Decorrelating experience for 224 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:23,281][03986] Decorrelating experience for 160 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:23,318][04007] Decorrelating experience for 128 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:23,366][03985] Decorrelating experience for 224 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:23,421][03978] Decorrelating experience for 160 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:23,584][03980] Decorrelating experience for 224 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:23,702][03981] Decorrelating experience for 192 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:23,745][03986] Decorrelating experience for 192 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:23,859][03978] Decorrelating experience for 192 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:23,970][04006] Decorrelating experience for 160 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:24,063][04007] Decorrelating experience for 160 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:24,131][03981] Decorrelating experience for 224 frames...\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:24,153][03956] Signal inference workers to stop experience collection...\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:24,163][03976] InferenceWorker_p0-w0: stopping experience collection\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:24,177][03986] Decorrelating experience for 224 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:24,260][03978] Decorrelating experience for 224 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:24,280][03984] Decorrelating experience for 224 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:24,413][04007] Decorrelating experience for 192 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:24,539][04006] Decorrelating experience for 192 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:24,718][04007] Decorrelating experience for 224 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:24,848][04006] Decorrelating experience for 224 frames...\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:25,813][03956] Signal inference workers to resume experience collection...\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:25,814][03976] InferenceWorker_p0-w0: resuming experience collection\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:26,168][03423] Fps is (10 sec: 1638.4, 60 sec: 1638.4, 300 sec: 1638.4). Total num frames: 450019328. Throughput: 0: 1153.6. Samples: 5768. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:26,168][03423] Avg episode reward: [(0, '0.894')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:28,279][03976] Updated weights for policy 0, policy_version 57385 (0.0102)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:30,718][03976] Updated weights for policy 0, policy_version 57395 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:31,168][03423] Fps is (10 sec: 17202.7, 60 sec: 17202.7, 300 sec: 17202.7). Total num frames: 450183168. Throughput: 0: 4035.1. Samples: 40352. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:31,169][03423] Avg episode reward: [(0, '16.075')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:32,101][03423] Heartbeat connected on Batcher_0\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:32,126][03423] Heartbeat connected on RolloutWorker_w1\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:32,127][03423] Heartbeat connected on RolloutWorker_w4\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:32,140][03423] Heartbeat connected on RolloutWorker_w2\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:32,141][03423] Heartbeat connected on RolloutWorker_w5\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:32,143][03423] Heartbeat connected on RolloutWorker_w7\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:32,144][03423] Heartbeat connected on InferenceWorker_p0-w0\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:32,146][03423] Heartbeat connected on RolloutWorker_w0\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:32,157][03423] Heartbeat connected on RolloutWorker_w8\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:32,163][03423] Heartbeat connected on RolloutWorker_w9\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:32,164][03423] Heartbeat connected on RolloutWorker_w6\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:32,168][03423] Heartbeat connected on RolloutWorker_w10\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:32,171][03423] Heartbeat connected on RolloutWorker_w12\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:32,174][03423] Heartbeat connected on RolloutWorker_w13\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:32,175][03423] Heartbeat connected on RolloutWorker_w3\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:32,180][03423] Heartbeat connected on RolloutWorker_w15\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:32,181][03423] Heartbeat connected on RolloutWorker_w11\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:32,187][03423] Heartbeat connected on RolloutWorker_w14\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:32,189][03423] Heartbeat connected on LearnerWorker_p0\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:33,142][03976] Updated weights for policy 0, policy_version 57405 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:35,712][03976] Updated weights for policy 0, policy_version 57415 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:36,168][03423] Fps is (10 sec: 33587.2, 60 sec: 22937.6, 300 sec: 22937.6). Total num frames: 450355200. Throughput: 0: 4407.5. Samples: 66112. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:36,169][03423] Avg episode reward: [(0, '53.339')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:37,918][03976] Updated weights for policy 0, policy_version 57425 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:40,045][03976] Updated weights for policy 0, policy_version 57435 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:41,171][03423] Fps is (10 sec: 35214.9, 60 sec: 26210.0, 300 sec: 26210.0). Total num frames: 450535424. Throughput: 0: 5935.2. Samples: 118724. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:41,173][03423] Avg episode reward: [(0, '55.334')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:42,227][03976] Updated weights for policy 0, policy_version 57445 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:44,395][03976] Updated weights for policy 0, policy_version 57455 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:46,168][03423] Fps is (10 sec: 37683.1, 60 sec: 28835.8, 300 sec: 28835.8). Total num frames: 450732032. Throughput: 0: 6993.9. Samples: 174848. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:46,168][03423] Avg episode reward: [(0, '52.575')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:46,595][03976] Updated weights for policy 0, policy_version 57465 (0.0013)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:48,810][03976] Updated weights for policy 0, policy_version 57475 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:51,177][03423] Fps is (10 sec: 36849.0, 60 sec: 29756.9, 300 sec: 29756.9). Total num frames: 450904064. Throughput: 0: 6757.8. Samples: 202784. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:51,182][03423] Avg episode reward: [(0, '55.054')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:51,223][03976] Updated weights for policy 0, policy_version 57485 (0.0014)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:53,638][03976] Updated weights for policy 0, policy_version 57495 (0.0014)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:56,112][03976] Updated weights for policy 0, policy_version 57505 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:56,168][03423] Fps is (10 sec: 34405.7, 60 sec: 30427.3, 300 sec: 30427.3). Total num frames: 451076096. Throughput: 0: 7264.2. Samples: 254248. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:54:56,169][03423] Avg episode reward: [(0, '54.890')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:54:58,466][03976] Updated weights for policy 0, policy_version 57515 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:55:00,852][03976] Updated weights for policy 0, policy_version 57525 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:55:01,168][03423] Fps is (10 sec: 34432.0, 60 sec: 30924.8, 300 sec: 30924.8). Total num frames: 451248128. Throughput: 0: 7638.9. Samples: 305556. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:55:01,169][03423] Avg episode reward: [(0, '54.842')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:55:03,151][03976] Updated weights for policy 0, policy_version 57535 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:55:05,500][03976] Updated weights for policy 0, policy_version 57545 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:55:06,168][03423] Fps is (10 sec: 35226.2, 60 sec: 31493.7, 300 sec: 31493.7). Total num frames: 451428352. Throughput: 0: 7378.0. Samples: 332008. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:55:06,169][03423] Avg episode reward: [(0, '53.592')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:55:07,758][03976] Updated weights for policy 0, policy_version 57555 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:55:09,913][03976] Updated weights for policy 0, policy_version 57565 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:55:11,167][03423] Fps is (10 sec: 36045.1, 60 sec: 31948.8, 300 sec: 31948.8). Total num frames: 451608576. Throughput: 0: 8471.0. Samples: 386964. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:55:11,169][03423] Avg episode reward: [(0, '54.978')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:55:12,307][03976] Updated weights for policy 0, policy_version 57575 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:55:14,566][03976] Updated weights for policy 0, policy_version 57585 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:55:16,168][03423] Fps is (10 sec: 36044.5, 60 sec: 32321.1, 300 sec: 32321.1). Total num frames: 451788800. Throughput: 0: 8870.8. Samples: 439536. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:55:16,169][03423] Avg episode reward: [(0, '51.653')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:55:16,809][03976] Updated weights for policy 0, policy_version 57595 (0.0013)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:55:18,981][03976] Updated weights for policy 0, policy_version 57605 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:55:21,153][03976] Updated weights for policy 0, policy_version 57615 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:55:21,167][03423] Fps is (10 sec: 36864.0, 60 sec: 32768.0, 300 sec: 32768.0). Total num frames: 451977216. Throughput: 0: 8920.4. Samples: 467528. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:55:21,168][03423] Avg episode reward: [(0, '54.931')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:55:23,360][03976] Updated weights for policy 0, policy_version 57625 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:55:25,610][03976] Updated weights for policy 0, policy_version 57635 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:55:26,168][03423] Fps is (10 sec: 36864.2, 60 sec: 35635.2, 300 sec: 33020.0). Total num frames: 452157440. Throughput: 0: 8995.1. Samples: 523472. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:55:26,169][03423] Avg episode reward: [(0, '55.077')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:55:27,867][03976] Updated weights for policy 0, policy_version 57645 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:55:30,137][03976] Updated weights for policy 0, policy_version 57655 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:55:31,168][03423] Fps is (10 sec: 36044.4, 60 sec: 35908.4, 300 sec: 33236.1). Total num frames: 452337664. Throughput: 0: 8958.8. Samples: 577996. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:55:31,169][03423] Avg episode reward: [(0, '54.934')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:55:32,388][03976] Updated weights for policy 0, policy_version 57665 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:55:34,619][03976] Updated weights for policy 0, policy_version 57675 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:55:36,168][03423] Fps is (10 sec: 36864.0, 60 sec: 36181.3, 300 sec: 33532.6). Total num frames: 452526080. Throughput: 0: 8946.0. Samples: 605288. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:55:36,169][03423] Avg episode reward: [(0, '55.470')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:55:36,759][03976] Updated weights for policy 0, policy_version 57685 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:55:39,016][03976] Updated weights for policy 0, policy_version 57695 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:55:41,168][03423] Fps is (10 sec: 36863.7, 60 sec: 36183.3, 300 sec: 33689.5). Total num frames: 452706304. Throughput: 0: 9045.3. Samples: 661284. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:55:41,169][03423] Avg episode reward: [(0, '52.742')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:55:41,265][03976] Updated weights for policy 0, policy_version 57705 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:55:43,512][03976] Updated weights for policy 0, policy_version 57715 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:55:45,772][03976] Updated weights for policy 0, policy_version 57725 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:55:46,168][03423] Fps is (10 sec: 36045.0, 60 sec: 35908.3, 300 sec: 33828.1). Total num frames: 452886528. Throughput: 0: 9126.3. Samples: 716240. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:55:46,169][03423] Avg episode reward: [(0, '53.182')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:55:48,014][03976] Updated weights for policy 0, policy_version 57735 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:55:50,137][03976] Updated weights for policy 0, policy_version 57745 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:55:51,176][03423] Fps is (10 sec: 36840.1, 60 sec: 36181.8, 300 sec: 34039.8). Total num frames: 453074944. Throughput: 0: 9143.6. Samples: 743532. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:55:51,178][03423] Avg episode reward: [(0, '51.911')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:55:52,389][03976] Updated weights for policy 0, policy_version 57755 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:55:54,771][03976] Updated weights for policy 0, policy_version 57765 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:55:56,168][03423] Fps is (10 sec: 36864.0, 60 sec: 36318.0, 300 sec: 34147.7). Total num frames: 453255168. Throughput: 0: 9120.9. Samples: 797404. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:55:56,168][03423] Avg episode reward: [(0, '52.203')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:55:57,057][03976] Updated weights for policy 0, policy_version 57775 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:55:59,417][03976] Updated weights for policy 0, policy_version 57785 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:56:01,168][03423] Fps is (10 sec: 35248.9, 60 sec: 36317.9, 300 sec: 34160.6). Total num frames: 453427200. Throughput: 0: 9132.7. Samples: 850508. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:56:01,168][03423] Avg episode reward: [(0, '54.525')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:01,742][03976] Updated weights for policy 0, policy_version 57795 (0.0013)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:03,943][03976] Updated weights for policy 0, policy_version 57805 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:56:06,168][03423] Fps is (10 sec: 35225.4, 60 sec: 36317.9, 300 sec: 34250.3). Total num frames: 453607424. Throughput: 0: 9116.7. Samples: 877780. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:56:06,180][03423] Avg episode reward: [(0, '54.195')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:06,227][03976] Updated weights for policy 0, policy_version 57815 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:06,228][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000057815_453615616.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:56:06,308][03956] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000057046_447315968.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:08,439][03976] Updated weights for policy 0, policy_version 57825 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:10,609][03976] Updated weights for policy 0, policy_version 57835 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:56:11,168][03423] Fps is (10 sec: 36863.7, 60 sec: 36454.3, 300 sec: 34406.4). Total num frames: 453795840. Throughput: 0: 9100.3. Samples: 932984. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:56:11,169][03423] Avg episode reward: [(0, '54.645')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:12,772][03976] Updated weights for policy 0, policy_version 57845 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:14,977][03976] Updated weights for policy 0, policy_version 57855 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:56:16,168][03423] Fps is (10 sec: 37683.2, 60 sec: 36591.0, 300 sec: 34548.9). Total num frames: 453984256. Throughput: 0: 9146.3. Samples: 989580. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:56:16,169][03423] Avg episode reward: [(0, '53.463')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:17,153][03976] Updated weights for policy 0, policy_version 57865 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:19,265][03976] Updated weights for policy 0, policy_version 57875 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:56:21,168][03423] Fps is (10 sec: 37683.4, 60 sec: 36590.9, 300 sec: 34679.5). Total num frames: 454172672. Throughput: 0: 9168.9. Samples: 1017888. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:56:21,169][03423] Avg episode reward: [(0, '55.815')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:21,489][03976] Updated weights for policy 0, policy_version 57885 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:23,947][03976] Updated weights for policy 0, policy_version 57895 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:56:26,167][03423] Fps is (10 sec: 36045.2, 60 sec: 36454.5, 300 sec: 34668.6). Total num frames: 454344704. Throughput: 0: 9122.2. Samples: 1071780. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:56:26,168][03423] Avg episode reward: [(0, '55.978')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:26,322][03976] Updated weights for policy 0, policy_version 57905 (0.0016)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:28,493][03976] Updated weights for policy 0, policy_version 57915 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:30,637][03976] Updated weights for policy 0, policy_version 57925 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:56:31,168][03423] Fps is (10 sec: 36044.9, 60 sec: 36591.0, 300 sec: 34784.5). Total num frames: 454533120. Throughput: 0: 9113.6. Samples: 1126352. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:56:31,169][03423] Avg episode reward: [(0, '55.718')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:32,862][03976] Updated weights for policy 0, policy_version 57935 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:35,051][03976] Updated weights for policy 0, policy_version 57945 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:56:36,168][03423] Fps is (10 sec: 37682.3, 60 sec: 36590.9, 300 sec: 34891.8). Total num frames: 454721536. Throughput: 0: 9110.2. Samples: 1153432. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:56:36,169][03423] Avg episode reward: [(0, '55.003')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:37,255][03976] Updated weights for policy 0, policy_version 57955 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:39,462][03976] Updated weights for policy 0, policy_version 57965 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:56:41,167][03423] Fps is (10 sec: 36864.2, 60 sec: 36591.0, 300 sec: 34933.0). Total num frames: 454901760. Throughput: 0: 9174.7. Samples: 1210264. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:56:41,168][03423] Avg episode reward: [(0, '54.972')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:41,675][03976] Updated weights for policy 0, policy_version 57975 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:43,801][03976] Updated weights for policy 0, policy_version 57985 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:46,016][03976] Updated weights for policy 0, policy_version 57995 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:56:46,168][03423] Fps is (10 sec: 36864.5, 60 sec: 36727.4, 300 sec: 35027.9). Total num frames: 455090176. Throughput: 0: 9237.5. Samples: 1266196. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:56:46,169][03423] Avg episode reward: [(0, '54.300')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:48,206][03976] Updated weights for policy 0, policy_version 58005 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:50,394][03976] Updated weights for policy 0, policy_version 58015 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:56:51,167][03423] Fps is (10 sec: 37683.2, 60 sec: 36731.5, 300 sec: 35116.4). Total num frames: 455278592. Throughput: 0: 9261.3. Samples: 1294536. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:56:51,168][03423] Avg episode reward: [(0, '54.951')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:52,583][03976] Updated weights for policy 0, policy_version 58025 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:54,766][03976] Updated weights for policy 0, policy_version 58035 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:56:56,175][03423] Fps is (10 sec: 37660.1, 60 sec: 36860.2, 300 sec: 35197.8). Total num frames: 455467008. Throughput: 0: 9280.7. Samples: 1350672. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:56:56,176][03423] Avg episode reward: [(0, '54.598')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:56,934][03976] Updated weights for policy 0, policy_version 58045 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:56:59,083][03976] Updated weights for policy 0, policy_version 58055 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:57:01,167][03423] Fps is (10 sec: 37683.2, 60 sec: 37137.1, 300 sec: 35276.8). Total num frames: 455655424. Throughput: 0: 9287.8. Samples: 1407532. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:57:01,180][03423] Avg episode reward: [(0, '53.331')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:57:01,233][03976] Updated weights for policy 0, policy_version 58065 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:57:03,436][03976] Updated weights for policy 0, policy_version 58075 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:57:05,750][03976] Updated weights for policy 0, policy_version 58085 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:57:06,179][03423] Fps is (10 sec: 36848.3, 60 sec: 37130.6, 300 sec: 35297.8). Total num frames: 455835648. Throughput: 0: 9280.5. Samples: 1435608. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:57:06,181][03423] Avg episode reward: [(0, '54.406')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:57:08,183][03976] Updated weights for policy 0, policy_version 58095 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:57:10,365][03976] Updated weights for policy 0, policy_version 58105 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:57:11,168][03423] Fps is (10 sec: 36044.8, 60 sec: 37000.6, 300 sec: 35322.0). Total num frames: 456015872. Throughput: 0: 9255.9. Samples: 1488296. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:57:11,169][03423] Avg episode reward: [(0, '54.517')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:57:12,548][03976] Updated weights for policy 0, policy_version 58115 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:57:14,716][03976] Updated weights for policy 0, policy_version 58125 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:57:16,177][03423] Fps is (10 sec: 36875.7, 60 sec: 36996.1, 300 sec: 35388.0). Total num frames: 456204288. Throughput: 0: 9293.2. Samples: 1544612. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:57:16,180][03423] Avg episode reward: [(0, '55.071')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:57:16,890][03976] Updated weights for policy 0, policy_version 58135 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:57:19,069][03976] Updated weights for policy 0, policy_version 58145 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:57:21,167][03423] Fps is (10 sec: 37683.3, 60 sec: 37000.6, 300 sec: 35453.2). Total num frames: 456392704. Throughput: 0: 9323.1. Samples: 1572968. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:57:21,168][03423] Avg episode reward: [(0, '54.803')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:57:21,253][03976] Updated weights for policy 0, policy_version 58155 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:57:23,394][03976] Updated weights for policy 0, policy_version 58165 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:57:25,779][03976] Updated weights for policy 0, policy_version 58175 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:57:26,168][03423] Fps is (10 sec: 36890.7, 60 sec: 37137.0, 300 sec: 35469.1). Total num frames: 456572928. Throughput: 0: 9316.4. Samples: 1629504. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:57:26,169][03423] Avg episode reward: [(0, '53.444')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:57:28,280][03976] Updated weights for policy 0, policy_version 58185 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:57:30,839][03976] Updated weights for policy 0, policy_version 58195 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:57:31,168][03423] Fps is (10 sec: 34406.2, 60 sec: 36727.5, 300 sec: 35398.1). Total num frames: 456736768. Throughput: 0: 9145.1. Samples: 1677724. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:57:31,169][03423] Avg episode reward: [(0, '55.069')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:57:33,358][03976] Updated weights for policy 0, policy_version 58205 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:57:35,804][03976] Updated weights for policy 0, policy_version 58215 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:57:36,168][03423] Fps is (10 sec: 32767.4, 60 sec: 36317.8, 300 sec: 35330.6). Total num frames: 456900608. Throughput: 0: 9062.8. Samples: 1702364. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:57:36,169][03423] Avg episode reward: [(0, '53.463')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:57:38,018][03976] Updated weights for policy 0, policy_version 58225 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:57:40,276][03976] Updated weights for policy 0, policy_version 58235 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:57:41,168][03423] Fps is (10 sec: 35225.6, 60 sec: 36454.4, 300 sec: 35389.4). Total num frames: 457089024. Throughput: 0: 9002.9. Samples: 1755748. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:57:41,169][03423] Avg episode reward: [(0, '55.821')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:57:42,530][03976] Updated weights for policy 0, policy_version 58245 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:57:44,795][03976] Updated weights for policy 0, policy_version 58255 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:57:46,168][03423] Fps is (10 sec: 36045.3, 60 sec: 36181.3, 300 sec: 35365.4). Total num frames: 457261056. Throughput: 0: 8939.4. Samples: 1809804. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:57:46,169][03423] Avg episode reward: [(0, '54.269')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:57:47,175][03976] Updated weights for policy 0, policy_version 58265 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:57:49,546][03976] Updated weights for policy 0, policy_version 58275 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:57:51,168][03423] Fps is (10 sec: 35225.5, 60 sec: 36044.8, 300 sec: 35381.6). Total num frames: 457441280. Throughput: 0: 8893.3. Samples: 1835712. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:57:51,169][03423] Avg episode reward: [(0, '54.371')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:57:51,792][03976] Updated weights for policy 0, policy_version 58285 (0.0013)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:57:54,108][03976] Updated weights for policy 0, policy_version 58295 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:57:56,168][03423] Fps is (10 sec: 35225.9, 60 sec: 35775.4, 300 sec: 35359.0). Total num frames: 457613312. Throughput: 0: 8896.9. Samples: 1888656. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:57:56,169][03423] Avg episode reward: [(0, '57.122')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:57:56,548][03976] Updated weights for policy 0, policy_version 58305 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:57:58,961][03976] Updated weights for policy 0, policy_version 58315 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:58:01,168][03423] Fps is (10 sec: 34406.5, 60 sec: 35498.7, 300 sec: 35337.3). Total num frames: 457785344. Throughput: 0: 8768.4. Samples: 1939128. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:58:01,169][03423] Avg episode reward: [(0, '54.177')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:01,367][03976] Updated weights for policy 0, policy_version 58325 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:03,696][03976] Updated weights for policy 0, policy_version 58335 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:05,934][03976] Updated weights for policy 0, policy_version 58345 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:58:06,168][03423] Fps is (10 sec: 34406.3, 60 sec: 35368.3, 300 sec: 35316.6). Total num frames: 457957376. Throughput: 0: 8729.8. Samples: 1965812. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:58:06,169][03423] Avg episode reward: [(0, '53.883')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:06,213][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000058346_457965568.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:58:06,298][03956] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000057375_450011136.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:08,307][03976] Updated weights for policy 0, policy_version 58355 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:10,628][03976] Updated weights for policy 0, policy_version 58365 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:58:11,177][03423] Fps is (10 sec: 35194.7, 60 sec: 35356.9, 300 sec: 35331.1). Total num frames: 458137600. Throughput: 0: 8660.4. Samples: 2019296. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:58:11,179][03423] Avg episode reward: [(0, '54.865')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:12,985][03976] Updated weights for policy 0, policy_version 58375 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:15,266][03976] Updated weights for policy 0, policy_version 58385 (0.0016)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:58:16,168][03423] Fps is (10 sec: 36045.0, 60 sec: 35229.9, 300 sec: 35347.6). Total num frames: 458317824. Throughput: 0: 8772.3. Samples: 2072476. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:58:16,169][03423] Avg episode reward: [(0, '55.044')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:17,494][03976] Updated weights for policy 0, policy_version 58395 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:19,791][03976] Updated weights for policy 0, policy_version 58405 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:58:21,168][03423] Fps is (10 sec: 36076.5, 60 sec: 35089.0, 300 sec: 35362.1). Total num frames: 458498048. Throughput: 0: 8825.2. Samples: 2099496. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:58:21,169][03423] Avg episode reward: [(0, '57.133')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:22,026][03976] Updated weights for policy 0, policy_version 58415 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:24,303][03976] Updated weights for policy 0, policy_version 58425 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:58:26,168][03423] Fps is (10 sec: 36044.9, 60 sec: 35089.1, 300 sec: 35376.1). Total num frames: 458678272. Throughput: 0: 8823.5. Samples: 2152804. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:58:26,169][03423] Avg episode reward: [(0, '55.695')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:26,604][03976] Updated weights for policy 0, policy_version 58435 (0.0015)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:28,919][03976] Updated weights for policy 0, policy_version 58445 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:58:31,167][03423] Fps is (10 sec: 35225.8, 60 sec: 35225.6, 300 sec: 35356.7). Total num frames: 458850304. Throughput: 0: 8824.8. Samples: 2206920. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:58:31,168][03423] Avg episode reward: [(0, '55.300')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:31,189][03976] Updated weights for policy 0, policy_version 58455 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:33,460][03976] Updated weights for policy 0, policy_version 58465 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:35,744][03976] Updated weights for policy 0, policy_version 58475 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:58:36,168][03423] Fps is (10 sec: 35225.4, 60 sec: 35498.8, 300 sec: 35370.2). Total num frames: 459030528. Throughput: 0: 8850.7. Samples: 2233992. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:58:36,169][03423] Avg episode reward: [(0, '53.255')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:38,026][03976] Updated weights for policy 0, policy_version 58485 (0.0019)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:39,849][03976] Updated weights for policy 0, policy_version 58495 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:58:41,167][03423] Fps is (10 sec: 39321.4, 60 sec: 35908.3, 300 sec: 35509.2). Total num frames: 459243520. Throughput: 0: 8935.0. Samples: 2290732. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:58:41,168][03423] Avg episode reward: [(0, '53.908')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:41,542][03976] Updated weights for policy 0, policy_version 58505 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:43,270][03976] Updated weights for policy 0, policy_version 58515 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:44,983][03976] Updated weights for policy 0, policy_version 58525 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:58:46,167][03423] Fps is (10 sec: 45056.3, 60 sec: 37000.6, 300 sec: 35735.7). Total num frames: 459481088. Throughput: 0: 9387.2. Samples: 2361552. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:58:46,168][03423] Avg episode reward: [(0, '54.732')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:46,709][03976] Updated weights for policy 0, policy_version 58535 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:48,460][03976] Updated weights for policy 0, policy_version 58545 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:50,220][03976] Updated weights for policy 0, policy_version 58555 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:58:51,168][03423] Fps is (10 sec: 47513.2, 60 sec: 37956.2, 300 sec: 35953.8). Total num frames: 459718656. Throughput: 0: 9591.3. Samples: 2397420. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:58:51,169][03423] Avg episode reward: [(0, '54.748')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:51,955][03976] Updated weights for policy 0, policy_version 58565 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:53,644][03976] Updated weights for policy 0, policy_version 58575 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:55,386][03976] Updated weights for policy 0, policy_version 58585 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:58:56,167][03423] Fps is (10 sec: 47513.6, 60 sec: 39048.6, 300 sec: 36164.0). Total num frames: 459956224. Throughput: 0: 9985.3. Samples: 2468548. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:58:56,169][03423] Avg episode reward: [(0, '54.516')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:57,079][03976] Updated weights for policy 0, policy_version 58595 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:58:58,827][03976] Updated weights for policy 0, policy_version 58605 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:00,532][03976] Updated weights for policy 0, policy_version 58615 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:59:01,168][03423] Fps is (10 sec: 47513.9, 60 sec: 40140.8, 300 sec: 36366.6). Total num frames: 460193792. Throughput: 0: 10382.6. Samples: 2539692. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:59:01,169][03423] Avg episode reward: [(0, '52.939')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:02,231][03976] Updated weights for policy 0, policy_version 58625 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:03,954][03976] Updated weights for policy 0, policy_version 58635 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:05,684][03976] Updated weights for policy 0, policy_version 58645 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:59:06,168][03423] Fps is (10 sec: 47513.5, 60 sec: 41233.1, 300 sec: 36562.2). Total num frames: 460431360. Throughput: 0: 10571.2. Samples: 2575200. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:59:06,169][03423] Avg episode reward: [(0, '54.407')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:07,403][03976] Updated weights for policy 0, policy_version 58655 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:09,189][03976] Updated weights for policy 0, policy_version 58665 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:10,936][03976] Updated weights for policy 0, policy_version 58675 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:59:11,168][03423] Fps is (10 sec: 47513.6, 60 sec: 42195.0, 300 sec: 36751.0). Total num frames: 460668928. Throughput: 0: 10967.6. Samples: 2646344. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:59:11,169][03423] Avg episode reward: [(0, '53.556')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:12,632][03976] Updated weights for policy 0, policy_version 58685 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:14,400][03976] Updated weights for policy 0, policy_version 58695 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:16,108][03976] Updated weights for policy 0, policy_version 58705 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:59:16,168][03423] Fps is (10 sec: 47513.0, 60 sec: 43144.5, 300 sec: 36933.4). Total num frames: 460906496. Throughput: 0: 11349.7. Samples: 2717660. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:59:16,169][03423] Avg episode reward: [(0, '54.244')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:17,823][03976] Updated weights for policy 0, policy_version 58715 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:19,550][03976] Updated weights for policy 0, policy_version 58725 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:59:21,167][03423] Fps is (10 sec: 47514.0, 60 sec: 44100.3, 300 sec: 37711.0). Total num frames: 461144064. Throughput: 0: 11533.8. Samples: 2753012. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:59:21,168][03423] Avg episode reward: [(0, '56.034')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:21,282][03976] Updated weights for policy 0, policy_version 58735 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:23,025][03976] Updated weights for policy 0, policy_version 58745 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:24,722][03976] Updated weights for policy 0, policy_version 58755 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:59:26,168][03423] Fps is (10 sec: 47513.9, 60 sec: 45056.0, 300 sec: 37960.9). Total num frames: 461381632. Throughput: 0: 11854.0. Samples: 2824164. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:59:26,169][03423] Avg episode reward: [(0, '52.873')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:26,456][03976] Updated weights for policy 0, policy_version 58765 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:28,173][03976] Updated weights for policy 0, policy_version 58775 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:29,920][03976] Updated weights for policy 0, policy_version 58785 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:59:31,167][03423] Fps is (10 sec: 47513.5, 60 sec: 46148.3, 300 sec: 38183.1). Total num frames: 461619200. Throughput: 0: 11859.2. Samples: 2895216. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:59:31,169][03423] Avg episode reward: [(0, '53.722')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:31,688][03976] Updated weights for policy 0, policy_version 58795 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:33,427][03976] Updated weights for policy 0, policy_version 58805 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:35,153][03976] Updated weights for policy 0, policy_version 58815 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:59:36,176][03423] Fps is (10 sec: 47514.1, 60 sec: 47104.1, 300 sec: 38377.9). Total num frames: 461856768. Throughput: 0: 11848.6. Samples: 2930604. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:59:36,177][03423] Avg episode reward: [(0, '51.851')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:36,823][03976] Updated weights for policy 0, policy_version 58825 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:38,595][03976] Updated weights for policy 0, policy_version 58835 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:40,345][03976] Updated weights for policy 0, policy_version 58845 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:59:41,167][03423] Fps is (10 sec: 46695.0, 60 sec: 47377.2, 300 sec: 38488.5). Total num frames: 462086144. Throughput: 0: 11840.8. Samples: 3001380. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:59:41,168][03423] Avg episode reward: [(0, '54.314')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:42,066][03976] Updated weights for policy 0, policy_version 58855 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:43,781][03976] Updated weights for policy 0, policy_version 58865 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:45,517][03976] Updated weights for policy 0, policy_version 58875 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:59:46,168][03423] Fps is (10 sec: 46693.7, 60 sec: 47377.0, 300 sec: 38711.6). Total num frames: 462323712. Throughput: 0: 11837.8. Samples: 3072392. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:59:46,169][03423] Avg episode reward: [(0, '52.513')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:47,265][03976] Updated weights for policy 0, policy_version 58885 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:48,986][03976] Updated weights for policy 0, policy_version 58895 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:50,694][03976] Updated weights for policy 0, policy_version 58905 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:59:51,168][03423] Fps is (10 sec: 47512.8, 60 sec: 47377.1, 300 sec: 38932.9). Total num frames: 462561280. Throughput: 0: 11838.9. Samples: 3107952. Policy #0 lag: (min: 0.0, avg: 1.5, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:59:51,168][03423] Avg episode reward: [(0, '55.318')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:52,423][03976] Updated weights for policy 0, policy_version 58915 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:54,154][03976] Updated weights for policy 0, policy_version 58925 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:55,968][03976] Updated weights for policy 0, policy_version 58935 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:59:56,168][03423] Fps is (10 sec: 46694.8, 60 sec: 47240.5, 300 sec: 39127.2). Total num frames: 462790656. Throughput: 0: 11827.8. Samples: 3178596. Policy #0 lag: (min: 0.0, avg: 1.5, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 14:59:56,168][03423] Avg episode reward: [(0, '53.648')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:57,770][03976] Updated weights for policy 0, policy_version 58945 (0.0013)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 14:59:59,669][03976] Updated weights for policy 0, policy_version 58955 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:00:01,168][03423] Fps is (10 sec: 45874.4, 60 sec: 47103.9, 300 sec: 39293.8). Total num frames: 463020032. Throughput: 0: 11748.9. Samples: 3246360. Policy #0 lag: (min: 0.0, avg: 1.5, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:00:01,169][03423] Avg episode reward: [(0, '55.904')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:01,430][03976] Updated weights for policy 0, policy_version 58965 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:03,202][03976] Updated weights for policy 0, policy_version 58975 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:04,984][03976] Updated weights for policy 0, policy_version 58985 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:00:06,167][03423] Fps is (10 sec: 45875.6, 60 sec: 46967.5, 300 sec: 39460.4). Total num frames: 463249408. Throughput: 0: 11736.4. Samples: 3281148. Policy #0 lag: (min: 0.0, avg: 1.5, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:00:06,168][03423] Avg episode reward: [(0, '52.742')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:06,186][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000058992_463257600.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:00:06,257][03956] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000057815_453615616.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:06,745][03976] Updated weights for policy 0, policy_version 58995 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:08,479][03976] Updated weights for policy 0, policy_version 59005 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:10,207][03976] Updated weights for policy 0, policy_version 59015 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:00:11,167][03423] Fps is (10 sec: 46695.3, 60 sec: 46967.5, 300 sec: 39654.9). Total num frames: 463486976. Throughput: 0: 11711.5. Samples: 3351180. Policy #0 lag: (min: 0.0, avg: 1.5, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:00:11,168][03423] Avg episode reward: [(0, '52.301')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:11,944][03976] Updated weights for policy 0, policy_version 59025 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:13,678][03976] Updated weights for policy 0, policy_version 59035 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:15,435][03976] Updated weights for policy 0, policy_version 59045 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:00:16,168][03423] Fps is (10 sec: 47513.2, 60 sec: 46967.5, 300 sec: 39821.4). Total num frames: 463724544. Throughput: 0: 11701.3. Samples: 3421776. Policy #0 lag: (min: 0.0, avg: 1.5, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:00:16,168][03423] Avg episode reward: [(0, '55.758')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:17,135][03976] Updated weights for policy 0, policy_version 59055 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:18,856][03976] Updated weights for policy 0, policy_version 59065 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:20,604][03976] Updated weights for policy 0, policy_version 59075 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:00:21,167][03423] Fps is (10 sec: 46694.6, 60 sec: 46830.9, 300 sec: 39988.1). Total num frames: 463953920. Throughput: 0: 11713.9. Samples: 3457728. Policy #0 lag: (min: 0.0, avg: 1.5, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:00:21,168][03423] Avg episode reward: [(0, '53.018')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:22,321][03976] Updated weights for policy 0, policy_version 59085 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:24,128][03976] Updated weights for policy 0, policy_version 59095 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:25,894][03976] Updated weights for policy 0, policy_version 59105 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:00:26,168][03423] Fps is (10 sec: 46694.5, 60 sec: 46831.0, 300 sec: 40182.5). Total num frames: 464191488. Throughput: 0: 11699.8. Samples: 3527872. Policy #0 lag: (min: 0.0, avg: 1.5, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:00:26,169][03423] Avg episode reward: [(0, '53.065')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:27,666][03976] Updated weights for policy 0, policy_version 59115 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:29,414][03976] Updated weights for policy 0, policy_version 59125 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:31,154][03976] Updated weights for policy 0, policy_version 59135 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:00:31,167][03423] Fps is (10 sec: 47513.4, 60 sec: 46830.9, 300 sec: 40349.1). Total num frames: 464429056. Throughput: 0: 11674.3. Samples: 3597732. Policy #0 lag: (min: 0.0, avg: 1.5, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:00:31,168][03423] Avg episode reward: [(0, '54.625')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:32,895][03976] Updated weights for policy 0, policy_version 59145 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:34,629][03976] Updated weights for policy 0, policy_version 59155 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:00:36,168][03423] Fps is (10 sec: 46694.4, 60 sec: 46694.4, 300 sec: 40515.7). Total num frames: 464658432. Throughput: 0: 11662.0. Samples: 3632740. Policy #0 lag: (min: 0.0, avg: 1.5, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:00:36,168][03423] Avg episode reward: [(0, '53.949')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:36,410][03976] Updated weights for policy 0, policy_version 59165 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:38,135][03976] Updated weights for policy 0, policy_version 59175 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:39,872][03976] Updated weights for policy 0, policy_version 59185 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:00:41,168][03423] Fps is (10 sec: 46694.3, 60 sec: 46830.8, 300 sec: 40710.1). Total num frames: 464896000. Throughput: 0: 11656.7. Samples: 3703148. Policy #0 lag: (min: 0.0, avg: 1.5, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:00:41,169][03423] Avg episode reward: [(0, '55.156')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:41,647][03976] Updated weights for policy 0, policy_version 59195 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:43,382][03976] Updated weights for policy 0, policy_version 59205 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:45,093][03976] Updated weights for policy 0, policy_version 59215 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:00:46,168][03423] Fps is (10 sec: 47513.0, 60 sec: 46830.9, 300 sec: 40877.6). Total num frames: 465133568. Throughput: 0: 11719.6. Samples: 3773740. Policy #0 lag: (min: 0.0, avg: 1.5, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:00:46,169][03423] Avg episode reward: [(0, '55.029')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:46,803][03976] Updated weights for policy 0, policy_version 59225 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:48,581][03976] Updated weights for policy 0, policy_version 59235 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:50,239][03976] Updated weights for policy 0, policy_version 59245 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:00:51,168][03423] Fps is (10 sec: 47513.3, 60 sec: 46830.9, 300 sec: 41071.1). Total num frames: 465371136. Throughput: 0: 11732.5. Samples: 3809112. Policy #0 lag: (min: 0.0, avg: 1.5, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:00:51,169][03423] Avg episode reward: [(0, '54.550')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:51,952][03976] Updated weights for policy 0, policy_version 59255 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:53,747][03976] Updated weights for policy 0, policy_version 59265 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:55,501][03976] Updated weights for policy 0, policy_version 59275 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:00:56,168][03423] Fps is (10 sec: 46694.2, 60 sec: 46830.8, 300 sec: 41265.4). Total num frames: 465600512. Throughput: 0: 11750.5. Samples: 3879956. Policy #0 lag: (min: 0.0, avg: 1.5, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:00:56,169][03423] Avg episode reward: [(0, '54.584')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:57,256][03976] Updated weights for policy 0, policy_version 59285 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:00:59,000][03976] Updated weights for policy 0, policy_version 59295 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:00,799][03976] Updated weights for policy 0, policy_version 59305 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:01:01,167][03423] Fps is (10 sec: 46694.8, 60 sec: 46967.6, 300 sec: 41459.9). Total num frames: 465838080. Throughput: 0: 11732.0. Samples: 3949716. Policy #0 lag: (min: 0.0, avg: 1.5, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:01:01,168][03423] Avg episode reward: [(0, '53.295')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:02,558][03976] Updated weights for policy 0, policy_version 59315 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:04,324][03976] Updated weights for policy 0, policy_version 59325 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:06,085][03976] Updated weights for policy 0, policy_version 59335 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:01:06,168][03423] Fps is (10 sec: 46695.1, 60 sec: 46967.4, 300 sec: 41598.7). Total num frames: 466067456. Throughput: 0: 11712.2. Samples: 3984776. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:01:06,169][03423] Avg episode reward: [(0, '52.675')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:07,808][03976] Updated weights for policy 0, policy_version 59345 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:09,584][03976] Updated weights for policy 0, policy_version 59355 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:01:11,167][03423] Fps is (10 sec: 46694.8, 60 sec: 46967.5, 300 sec: 41765.3). Total num frames: 466305024. Throughput: 0: 11714.0. Samples: 4055000. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:01:11,169][03423] Avg episode reward: [(0, '53.435')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:11,265][03976] Updated weights for policy 0, policy_version 59365 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:13,057][03976] Updated weights for policy 0, policy_version 59375 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:14,849][03976] Updated weights for policy 0, policy_version 59385 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:01:16,168][03423] Fps is (10 sec: 46694.1, 60 sec: 46830.9, 300 sec: 41904.2). Total num frames: 466534400. Throughput: 0: 11721.6. Samples: 4125204. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:01:16,169][03423] Avg episode reward: [(0, '52.197')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:16,548][03976] Updated weights for policy 0, policy_version 59395 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:18,305][03976] Updated weights for policy 0, policy_version 59405 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:20,067][03976] Updated weights for policy 0, policy_version 59415 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:01:21,168][03423] Fps is (10 sec: 45874.8, 60 sec: 46830.9, 300 sec: 42098.5). Total num frames: 466763776. Throughput: 0: 11736.2. Samples: 4160868. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:01:21,169][03423] Avg episode reward: [(0, '54.014')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:21,857][03976] Updated weights for policy 0, policy_version 59425 (0.0013)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:23,583][03976] Updated weights for policy 0, policy_version 59435 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:25,352][03976] Updated weights for policy 0, policy_version 59445 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:01:26,167][03423] Fps is (10 sec: 46695.0, 60 sec: 46830.9, 300 sec: 42265.2). Total num frames: 467001344. Throughput: 0: 11709.3. Samples: 4230064. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:01:26,168][03423] Avg episode reward: [(0, '53.253')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:27,122][03976] Updated weights for policy 0, policy_version 59455 (0.0014)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:28,809][03976] Updated weights for policy 0, policy_version 59465 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:30,481][03976] Updated weights for policy 0, policy_version 59475 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:01:31,167][03423] Fps is (10 sec: 48333.0, 60 sec: 46967.5, 300 sec: 42459.6). Total num frames: 467247104. Throughput: 0: 11731.9. Samples: 4301672. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:01:31,169][03423] Avg episode reward: [(0, '53.312')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:32,174][03976] Updated weights for policy 0, policy_version 59485 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:33,852][03976] Updated weights for policy 0, policy_version 59495 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:35,528][03976] Updated weights for policy 0, policy_version 59505 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:01:36,168][03423] Fps is (10 sec: 48332.5, 60 sec: 47104.0, 300 sec: 42653.9). Total num frames: 467484672. Throughput: 0: 11757.5. Samples: 4338200. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:01:36,169][03423] Avg episode reward: [(0, '54.207')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:37,213][03976] Updated weights for policy 0, policy_version 59515 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:38,900][03976] Updated weights for policy 0, policy_version 59525 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:40,583][03976] Updated weights for policy 0, policy_version 59535 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:01:41,168][03423] Fps is (10 sec: 48332.6, 60 sec: 47240.5, 300 sec: 42848.3). Total num frames: 467730432. Throughput: 0: 11805.2. Samples: 4411188. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:01:41,169][03423] Avg episode reward: [(0, '51.055')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:42,273][03976] Updated weights for policy 0, policy_version 59545 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:43,942][03976] Updated weights for policy 0, policy_version 59555 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:45,641][03976] Updated weights for policy 0, policy_version 59565 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:01:46,168][03423] Fps is (10 sec: 49152.2, 60 sec: 47377.2, 300 sec: 43042.7). Total num frames: 467976192. Throughput: 0: 11871.1. Samples: 4483916. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:01:46,169][03423] Avg episode reward: [(0, '54.671')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:47,334][03976] Updated weights for policy 0, policy_version 59575 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:49,027][03976] Updated weights for policy 0, policy_version 59585 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:50,707][03976] Updated weights for policy 0, policy_version 59595 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:01:51,167][03423] Fps is (10 sec: 48332.9, 60 sec: 47377.1, 300 sec: 43210.2). Total num frames: 468213760. Throughput: 0: 11900.4. Samples: 4520292. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:01:51,169][03423] Avg episode reward: [(0, '53.708')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:52,466][03976] Updated weights for policy 0, policy_version 59605 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:54,135][03976] Updated weights for policy 0, policy_version 59615 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:55,797][03976] Updated weights for policy 0, policy_version 59625 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:01:56,168][03423] Fps is (10 sec: 48332.7, 60 sec: 47650.2, 300 sec: 43403.7). Total num frames: 468459520. Throughput: 0: 11957.0. Samples: 4593068. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:01:56,169][03423] Avg episode reward: [(0, '54.609')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:57,488][03976] Updated weights for policy 0, policy_version 59635 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:01:59,163][03976] Updated weights for policy 0, policy_version 59645 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:00,844][03976] Updated weights for policy 0, policy_version 59655 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:02:01,168][03423] Fps is (10 sec: 48332.7, 60 sec: 47650.1, 300 sec: 43599.7). Total num frames: 468697088. Throughput: 0: 12010.2. Samples: 4665660. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:02:01,168][03423] Avg episode reward: [(0, '55.321')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:02,544][03976] Updated weights for policy 0, policy_version 59665 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:04,235][03976] Updated weights for policy 0, policy_version 59675 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:05,913][03976] Updated weights for policy 0, policy_version 59685 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:02:06,168][03423] Fps is (10 sec: 48332.8, 60 sec: 47923.2, 300 sec: 43820.3). Total num frames: 468942848. Throughput: 0: 12030.3. Samples: 4702232. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:02:06,169][03423] Avg episode reward: [(0, '51.857')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:06,172][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000059686_468942848.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:02:06,244][03956] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000058346_457965568.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:07,602][03976] Updated weights for policy 0, policy_version 59695 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:09,312][03976] Updated weights for policy 0, policy_version 59705 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:10,994][03976] Updated weights for policy 0, policy_version 59715 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:02:11,167][03423] Fps is (10 sec: 49152.1, 60 sec: 48059.7, 300 sec: 44015.7). Total num frames: 469188608. Throughput: 0: 12106.9. Samples: 4774876. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:02:11,168][03423] Avg episode reward: [(0, '54.271')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:12,696][03976] Updated weights for policy 0, policy_version 59725 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:14,491][03976] Updated weights for policy 0, policy_version 59735 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:02:16,168][03423] Fps is (10 sec: 47512.6, 60 sec: 48059.6, 300 sec: 44153.4). Total num frames: 469417984. Throughput: 0: 12092.5. Samples: 4845840. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:02:16,169][03423] Avg episode reward: [(0, '55.025')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:16,232][03976] Updated weights for policy 0, policy_version 59745 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:18,050][03976] Updated weights for policy 0, policy_version 59755 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:19,880][03976] Updated weights for policy 0, policy_version 59765 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:02:21,168][03423] Fps is (10 sec: 45875.1, 60 sec: 48059.7, 300 sec: 44320.1). Total num frames: 469647360. Throughput: 0: 12050.4. Samples: 4880468. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:02:21,168][03423] Avg episode reward: [(0, '52.236')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:21,609][03976] Updated weights for policy 0, policy_version 59775 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:23,339][03976] Updated weights for policy 0, policy_version 59785 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:25,096][03976] Updated weights for policy 0, policy_version 59795 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:02:26,168][03423] Fps is (10 sec: 46695.6, 60 sec: 48059.7, 300 sec: 44570.0). Total num frames: 469884928. Throughput: 0: 11976.0. Samples: 4950108. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:02:26,168][03423] Avg episode reward: [(0, '54.095')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:26,865][03976] Updated weights for policy 0, policy_version 59805 (0.0014)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:28,618][03976] Updated weights for policy 0, policy_version 59815 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:30,325][03976] Updated weights for policy 0, policy_version 59825 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:02:31,167][03423] Fps is (10 sec: 46694.5, 60 sec: 47786.6, 300 sec: 44792.2). Total num frames: 470114304. Throughput: 0: 11904.4. Samples: 5019612. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:02:31,168][03423] Avg episode reward: [(0, '56.489')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:32,119][03976] Updated weights for policy 0, policy_version 59835 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:33,918][03976] Updated weights for policy 0, policy_version 59845 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:35,686][03976] Updated weights for policy 0, policy_version 59855 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:02:36,168][03423] Fps is (10 sec: 45875.0, 60 sec: 47650.1, 300 sec: 44931.0). Total num frames: 470343680. Throughput: 0: 11869.9. Samples: 5054440. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:02:36,169][03423] Avg episode reward: [(0, '51.818')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:37,469][03976] Updated weights for policy 0, policy_version 59865 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:39,277][03976] Updated weights for policy 0, policy_version 59875 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:40,999][03976] Updated weights for policy 0, policy_version 59885 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:02:41,167][03423] Fps is (10 sec: 45875.2, 60 sec: 47377.1, 300 sec: 45125.4). Total num frames: 470573056. Throughput: 0: 11785.1. Samples: 5123396. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:02:41,168][03423] Avg episode reward: [(0, '51.753')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:42,748][03976] Updated weights for policy 0, policy_version 59895 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:44,419][03976] Updated weights for policy 0, policy_version 59905 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:02:46,168][03423] Fps is (10 sec: 46694.1, 60 sec: 47240.5, 300 sec: 45319.8). Total num frames: 470810624. Throughput: 0: 11749.7. Samples: 5194396. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:02:46,169][03423] Avg episode reward: [(0, '52.989')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:46,194][03976] Updated weights for policy 0, policy_version 59915 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:47,901][03976] Updated weights for policy 0, policy_version 59925 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:49,696][03976] Updated weights for policy 0, policy_version 59935 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:02:51,168][03423] Fps is (10 sec: 47513.1, 60 sec: 47240.4, 300 sec: 45542.0). Total num frames: 471048192. Throughput: 0: 11717.3. Samples: 5229512. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:02:51,169][03423] Avg episode reward: [(0, '52.452')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:51,440][03976] Updated weights for policy 0, policy_version 59945 (0.0015)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:53,155][03976] Updated weights for policy 0, policy_version 59955 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:54,908][03976] Updated weights for policy 0, policy_version 59965 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:02:56,167][03423] Fps is (10 sec: 47514.2, 60 sec: 47104.0, 300 sec: 45764.1). Total num frames: 471285760. Throughput: 0: 11674.5. Samples: 5300228. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:02:56,168][03423] Avg episode reward: [(0, '50.796')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:56,655][03976] Updated weights for policy 0, policy_version 59975 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:02:58,364][03976] Updated weights for policy 0, policy_version 59985 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:00,097][03976] Updated weights for policy 0, policy_version 59995 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:03:01,168][03423] Fps is (10 sec: 47513.9, 60 sec: 47104.0, 300 sec: 45986.3). Total num frames: 471523328. Throughput: 0: 11678.3. Samples: 5371360. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:03:01,169][03423] Avg episode reward: [(0, '53.154')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:01,774][03976] Updated weights for policy 0, policy_version 60005 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:03,634][03976] Updated weights for policy 0, policy_version 60015 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:05,458][03976] Updated weights for policy 0, policy_version 60025 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:03:06,168][03423] Fps is (10 sec: 46694.3, 60 sec: 46831.0, 300 sec: 46154.3). Total num frames: 471752704. Throughput: 0: 11679.2. Samples: 5406032. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:03:06,169][03423] Avg episode reward: [(0, '55.342')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:07,201][03976] Updated weights for policy 0, policy_version 60035 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:08,948][03976] Updated weights for policy 0, policy_version 60045 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:10,736][03976] Updated weights for policy 0, policy_version 60055 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:03:11,168][03423] Fps is (10 sec: 45875.0, 60 sec: 46557.8, 300 sec: 46319.5). Total num frames: 471982080. Throughput: 0: 11663.7. Samples: 5474976. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:03:11,169][03423] Avg episode reward: [(0, '55.011')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:12,512][03976] Updated weights for policy 0, policy_version 60065 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:14,351][03976] Updated weights for policy 0, policy_version 60075 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:16,098][03976] Updated weights for policy 0, policy_version 60085 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:03:16,168][03423] Fps is (10 sec: 45875.2, 60 sec: 46558.1, 300 sec: 46486.1). Total num frames: 472211456. Throughput: 0: 11647.8. Samples: 5543764. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:03:16,168][03423] Avg episode reward: [(0, '54.775')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:17,901][03976] Updated weights for policy 0, policy_version 60095 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:19,623][03976] Updated weights for policy 0, policy_version 60105 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:03:21,167][03423] Fps is (10 sec: 46695.0, 60 sec: 46694.4, 300 sec: 46680.5). Total num frames: 472449024. Throughput: 0: 11655.9. Samples: 5578956. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:03:21,169][03423] Avg episode reward: [(0, '54.990')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:21,337][03976] Updated weights for policy 0, policy_version 60115 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:23,075][03976] Updated weights for policy 0, policy_version 60125 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:24,877][03976] Updated weights for policy 0, policy_version 60135 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:03:26,167][03423] Fps is (10 sec: 46694.7, 60 sec: 46557.9, 300 sec: 46874.9). Total num frames: 472678400. Throughput: 0: 11677.5. Samples: 5648884. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:03:26,168][03423] Avg episode reward: [(0, '53.826')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:26,664][03976] Updated weights for policy 0, policy_version 60145 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:28,390][03976] Updated weights for policy 0, policy_version 60155 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:30,174][03976] Updated weights for policy 0, policy_version 60165 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:03:31,168][03423] Fps is (10 sec: 45874.6, 60 sec: 46557.8, 300 sec: 47041.5). Total num frames: 472907776. Throughput: 0: 11640.0. Samples: 5718196. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:03:31,169][03423] Avg episode reward: [(0, '55.282')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:31,893][03976] Updated weights for policy 0, policy_version 60175 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:33,639][03976] Updated weights for policy 0, policy_version 60185 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:35,350][03976] Updated weights for policy 0, policy_version 60195 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:03:36,167][03423] Fps is (10 sec: 46694.4, 60 sec: 46694.5, 300 sec: 47124.8). Total num frames: 473145344. Throughput: 0: 11646.0. Samples: 5753580. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:03:36,168][03423] Avg episode reward: [(0, '56.023')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:37,031][03976] Updated weights for policy 0, policy_version 60205 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:38,737][03976] Updated weights for policy 0, policy_version 60215 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:40,504][03976] Updated weights for policy 0, policy_version 60225 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:03:41,167][03423] Fps is (10 sec: 47514.3, 60 sec: 46831.0, 300 sec: 47124.8). Total num frames: 473382912. Throughput: 0: 11680.5. Samples: 5825848. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:03:41,168][03423] Avg episode reward: [(0, '53.047')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:42,160][03976] Updated weights for policy 0, policy_version 60235 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:43,877][03976] Updated weights for policy 0, policy_version 60245 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:45,578][03976] Updated weights for policy 0, policy_version 60255 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:03:46,167][03423] Fps is (10 sec: 48332.7, 60 sec: 46967.6, 300 sec: 47152.6). Total num frames: 473628672. Throughput: 0: 11708.2. Samples: 5898228. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:03:46,169][03423] Avg episode reward: [(0, '53.796')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:47,261][03976] Updated weights for policy 0, policy_version 60265 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:48,911][03976] Updated weights for policy 0, policy_version 60275 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:50,609][03976] Updated weights for policy 0, policy_version 60285 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:03:51,168][03423] Fps is (10 sec: 49151.5, 60 sec: 47104.0, 300 sec: 47180.4). Total num frames: 473874432. Throughput: 0: 11749.9. Samples: 5934776. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:03:51,168][03423] Avg episode reward: [(0, '53.197')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:52,285][03976] Updated weights for policy 0, policy_version 60295 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:54,003][03976] Updated weights for policy 0, policy_version 60305 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:55,715][03976] Updated weights for policy 0, policy_version 60315 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:03:56,168][03423] Fps is (10 sec: 48332.3, 60 sec: 47103.9, 300 sec: 47180.4). Total num frames: 474112000. Throughput: 0: 11823.3. Samples: 6007024. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:03:56,169][03423] Avg episode reward: [(0, '55.208')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:57,378][03976] Updated weights for policy 0, policy_version 60325 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:03:59,098][03976] Updated weights for policy 0, policy_version 60335 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:00,816][03976] Updated weights for policy 0, policy_version 60345 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:04:01,168][03423] Fps is (10 sec: 48332.3, 60 sec: 47240.4, 300 sec: 47208.1). Total num frames: 474357760. Throughput: 0: 11906.2. Samples: 6079544. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:04:01,168][03423] Avg episode reward: [(0, '53.853')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:02,476][03976] Updated weights for policy 0, policy_version 60355 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:04,172][03976] Updated weights for policy 0, policy_version 60365 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:05,892][03976] Updated weights for policy 0, policy_version 60375 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:04:06,167][03423] Fps is (10 sec: 48333.5, 60 sec: 47377.1, 300 sec: 47208.1). Total num frames: 474595328. Throughput: 0: 11930.5. Samples: 6115828. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:04:06,168][03423] Avg episode reward: [(0, '54.233')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:06,216][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000060377_474603520.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:04:06,286][03956] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000058992_463257600.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:07,577][03976] Updated weights for policy 0, policy_version 60385 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:09,304][03976] Updated weights for policy 0, policy_version 60395 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:10,970][03976] Updated weights for policy 0, policy_version 60405 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:04:11,168][03423] Fps is (10 sec: 48332.7, 60 sec: 47650.0, 300 sec: 47235.9). Total num frames: 474841088. Throughput: 0: 11986.9. Samples: 6188296. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:04:11,169][03423] Avg episode reward: [(0, '56.566')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:12,692][03976] Updated weights for policy 0, policy_version 60415 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:14,376][03976] Updated weights for policy 0, policy_version 60425 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:16,049][03976] Updated weights for policy 0, policy_version 60435 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:04:16,168][03423] Fps is (10 sec: 48332.6, 60 sec: 47786.7, 300 sec: 47235.9). Total num frames: 475078656. Throughput: 0: 12053.1. Samples: 6260584. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:04:16,168][03423] Avg episode reward: [(0, '54.650')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:17,772][03976] Updated weights for policy 0, policy_version 60445 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:19,447][03976] Updated weights for policy 0, policy_version 60455 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:04:21,168][03423] Fps is (10 sec: 48333.7, 60 sec: 47923.2, 300 sec: 47263.7). Total num frames: 475324416. Throughput: 0: 12076.7. Samples: 6297032. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:04:21,169][03423] Avg episode reward: [(0, '50.422')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:21,169][03976] Updated weights for policy 0, policy_version 60465 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:22,828][03976] Updated weights for policy 0, policy_version 60475 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:24,514][03976] Updated weights for policy 0, policy_version 60485 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:04:26,168][03423] Fps is (10 sec: 48332.8, 60 sec: 48059.7, 300 sec: 47263.7). Total num frames: 475561984. Throughput: 0: 12089.9. Samples: 6369892. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:04:26,168][03423] Avg episode reward: [(0, '52.314')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:26,185][03976] Updated weights for policy 0, policy_version 60495 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:27,888][03976] Updated weights for policy 0, policy_version 60505 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:29,571][03976] Updated weights for policy 0, policy_version 60515 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:04:31,167][03423] Fps is (10 sec: 48333.0, 60 sec: 48332.9, 300 sec: 47291.4). Total num frames: 475807744. Throughput: 0: 12089.0. Samples: 6442232. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:04:31,168][03423] Avg episode reward: [(0, '54.383')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:31,284][03976] Updated weights for policy 0, policy_version 60525 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:32,994][03976] Updated weights for policy 0, policy_version 60535 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:34,705][03976] Updated weights for policy 0, policy_version 60545 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:04:36,167][03423] Fps is (10 sec: 48332.9, 60 sec: 48332.8, 300 sec: 47319.2). Total num frames: 476045312. Throughput: 0: 12080.6. Samples: 6478404. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:04:36,169][03423] Avg episode reward: [(0, '53.288')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:36,411][03976] Updated weights for policy 0, policy_version 60555 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:38,133][03976] Updated weights for policy 0, policy_version 60565 (0.0013)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:39,801][03976] Updated weights for policy 0, policy_version 60575 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:04:41,168][03423] Fps is (10 sec: 48332.5, 60 sec: 48469.3, 300 sec: 47347.0). Total num frames: 476291072. Throughput: 0: 12078.3. Samples: 6550548. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:04:41,169][03423] Avg episode reward: [(0, '55.977')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:41,489][03976] Updated weights for policy 0, policy_version 60585 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:43,198][03976] Updated weights for policy 0, policy_version 60595 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:44,880][03976] Updated weights for policy 0, policy_version 60605 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:04:46,168][03423] Fps is (10 sec: 48332.1, 60 sec: 48332.7, 300 sec: 47347.0). Total num frames: 476528640. Throughput: 0: 12077.6. Samples: 6623036. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:04:46,169][03423] Avg episode reward: [(0, '52.223')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:46,588][03976] Updated weights for policy 0, policy_version 60615 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:48,293][03976] Updated weights for policy 0, policy_version 60625 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:49,976][03976] Updated weights for policy 0, policy_version 60635 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:04:51,168][03423] Fps is (10 sec: 48331.6, 60 sec: 48332.6, 300 sec: 47402.5). Total num frames: 476774400. Throughput: 0: 12072.1. Samples: 6659076. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:04:51,169][03423] Avg episode reward: [(0, '55.454')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:51,682][03976] Updated weights for policy 0, policy_version 60645 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:53,352][03976] Updated weights for policy 0, policy_version 60655 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:55,042][03976] Updated weights for policy 0, policy_version 60665 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:04:56,168][03423] Fps is (10 sec: 48333.2, 60 sec: 48332.8, 300 sec: 47430.3). Total num frames: 477011968. Throughput: 0: 12080.8. Samples: 6731932. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:04:56,169][03423] Avg episode reward: [(0, '52.738')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:56,728][03976] Updated weights for policy 0, policy_version 60675 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:04:58,486][03976] Updated weights for policy 0, policy_version 60685 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:00,106][03976] Updated weights for policy 0, policy_version 60695 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:05:01,168][03423] Fps is (10 sec: 48333.7, 60 sec: 48332.9, 300 sec: 47485.8). Total num frames: 477257728. Throughput: 0: 12089.8. Samples: 6804628. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:05:01,169][03423] Avg episode reward: [(0, '54.927')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:01,781][03976] Updated weights for policy 0, policy_version 60705 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:03,501][03976] Updated weights for policy 0, policy_version 60715 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:05,183][03976] Updated weights for policy 0, policy_version 60725 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:05:06,168][03423] Fps is (10 sec: 48332.7, 60 sec: 48332.7, 300 sec: 47485.8). Total num frames: 477495296. Throughput: 0: 12086.4. Samples: 6840920. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:05:06,169][03423] Avg episode reward: [(0, '54.496')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:06,882][03976] Updated weights for policy 0, policy_version 60735 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:08,638][03976] Updated weights for policy 0, policy_version 60745 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:10,320][03976] Updated weights for policy 0, policy_version 60755 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:05:11,168][03423] Fps is (10 sec: 48332.5, 60 sec: 48332.8, 300 sec: 47513.6). Total num frames: 477741056. Throughput: 0: 12067.1. Samples: 6912912. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:05:11,169][03423] Avg episode reward: [(0, '54.518')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:11,995][03976] Updated weights for policy 0, policy_version 60765 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:13,733][03976] Updated weights for policy 0, policy_version 60775 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:15,464][03976] Updated weights for policy 0, policy_version 60785 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:05:16,168][03423] Fps is (10 sec: 48333.1, 60 sec: 48332.8, 300 sec: 47541.4). Total num frames: 477978624. Throughput: 0: 12072.3. Samples: 6985488. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:05:16,169][03423] Avg episode reward: [(0, '55.066')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:17,137][03976] Updated weights for policy 0, policy_version 60795 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:18,835][03976] Updated weights for policy 0, policy_version 60805 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:20,494][03976] Updated weights for policy 0, policy_version 60815 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:05:21,167][03423] Fps is (10 sec: 47514.4, 60 sec: 48196.3, 300 sec: 47541.4). Total num frames: 478216192. Throughput: 0: 12073.8. Samples: 7021724. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:05:21,169][03423] Avg episode reward: [(0, '52.536')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:22,184][03976] Updated weights for policy 0, policy_version 60825 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:23,872][03976] Updated weights for policy 0, policy_version 60835 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:25,561][03976] Updated weights for policy 0, policy_version 60845 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:05:26,167][03423] Fps is (10 sec: 48333.0, 60 sec: 48332.8, 300 sec: 47569.1). Total num frames: 478461952. Throughput: 0: 12078.5. Samples: 7094080. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:05:26,168][03423] Avg episode reward: [(0, '54.110')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:27,265][03976] Updated weights for policy 0, policy_version 60855 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:28,957][03976] Updated weights for policy 0, policy_version 60865 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:30,662][03976] Updated weights for policy 0, policy_version 60875 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:05:31,167][03423] Fps is (10 sec: 48333.0, 60 sec: 48196.3, 300 sec: 47596.9). Total num frames: 478699520. Throughput: 0: 12073.7. Samples: 7166348. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:05:31,168][03423] Avg episode reward: [(0, '56.118')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:32,368][03976] Updated weights for policy 0, policy_version 60885 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:34,076][03976] Updated weights for policy 0, policy_version 60895 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:35,729][03976] Updated weights for policy 0, policy_version 60905 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:05:36,168][03423] Fps is (10 sec: 48332.5, 60 sec: 48332.8, 300 sec: 47624.7). Total num frames: 478945280. Throughput: 0: 12088.2. Samples: 7203040. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:05:36,177][03423] Avg episode reward: [(0, '56.054')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:37,440][03976] Updated weights for policy 0, policy_version 60915 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:39,135][03976] Updated weights for policy 0, policy_version 60925 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:40,849][03976] Updated weights for policy 0, policy_version 60935 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:05:41,168][03423] Fps is (10 sec: 48332.2, 60 sec: 48196.2, 300 sec: 47624.7). Total num frames: 479182848. Throughput: 0: 12070.5. Samples: 7275104. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:05:41,169][03423] Avg episode reward: [(0, '53.439')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:42,524][03976] Updated weights for policy 0, policy_version 60945 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:44,248][03976] Updated weights for policy 0, policy_version 60955 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:45,937][03976] Updated weights for policy 0, policy_version 60965 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:05:46,167][03423] Fps is (10 sec: 48333.1, 60 sec: 48332.9, 300 sec: 47652.5). Total num frames: 479428608. Throughput: 0: 12070.2. Samples: 7347784. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:05:46,168][03423] Avg episode reward: [(0, '55.201')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:47,660][03976] Updated weights for policy 0, policy_version 60975 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:49,316][03976] Updated weights for policy 0, policy_version 60985 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:50,977][03976] Updated weights for policy 0, policy_version 60995 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:05:51,168][03423] Fps is (10 sec: 48333.0, 60 sec: 48196.5, 300 sec: 47680.2). Total num frames: 479666176. Throughput: 0: 12060.4. Samples: 7383636. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:05:51,169][03423] Avg episode reward: [(0, '52.884')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:52,687][03976] Updated weights for policy 0, policy_version 61005 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:54,399][03976] Updated weights for policy 0, policy_version 61015 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:56,077][03976] Updated weights for policy 0, policy_version 61025 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:05:56,168][03423] Fps is (10 sec: 48331.5, 60 sec: 48332.6, 300 sec: 47707.9). Total num frames: 479911936. Throughput: 0: 12074.9. Samples: 7456284. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:05:56,169][03423] Avg episode reward: [(0, '51.532')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:57,794][03976] Updated weights for policy 0, policy_version 61035 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:05:59,498][03976] Updated weights for policy 0, policy_version 61045 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:06:01,168][03423] Fps is (10 sec: 48332.7, 60 sec: 48196.3, 300 sec: 47735.8). Total num frames: 480149504. Throughput: 0: 12073.7. Samples: 7528804. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:06:01,168][03423] Avg episode reward: [(0, '53.925')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:01,186][03976] Updated weights for policy 0, policy_version 61055 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:02,884][03976] Updated weights for policy 0, policy_version 61065 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:04,581][03976] Updated weights for policy 0, policy_version 61075 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:06:06,168][03423] Fps is (10 sec: 48333.2, 60 sec: 48332.7, 300 sec: 47763.5). Total num frames: 480395264. Throughput: 0: 12077.3. Samples: 7565204. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:06:06,169][03423] Avg episode reward: [(0, '54.948')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:06,173][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000061084_480395264.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:06:06,246][03956] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000059686_468942848.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:06,320][03976] Updated weights for policy 0, policy_version 61085 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:07,970][03976] Updated weights for policy 0, policy_version 61095 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:09,709][03976] Updated weights for policy 0, policy_version 61105 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:06:11,168][03423] Fps is (10 sec: 48332.5, 60 sec: 48196.3, 300 sec: 47791.3). Total num frames: 480632832. Throughput: 0: 12070.3. Samples: 7637244. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:06:11,169][03423] Avg episode reward: [(0, '54.726')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:11,379][03976] Updated weights for policy 0, policy_version 61115 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:13,112][03976] Updated weights for policy 0, policy_version 61125 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:14,795][03976] Updated weights for policy 0, policy_version 61135 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:06:16,168][03423] Fps is (10 sec: 48333.6, 60 sec: 48332.8, 300 sec: 47846.8). Total num frames: 480878592. Throughput: 0: 12063.4. Samples: 7709204. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:06:16,169][03423] Avg episode reward: [(0, '53.004')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:16,513][03976] Updated weights for policy 0, policy_version 61145 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:18,197][03976] Updated weights for policy 0, policy_version 61155 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:19,907][03976] Updated weights for policy 0, policy_version 61165 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:06:21,168][03423] Fps is (10 sec: 48332.3, 60 sec: 48332.6, 300 sec: 47846.8). Total num frames: 481116160. Throughput: 0: 12054.1. Samples: 7745476. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:06:21,169][03423] Avg episode reward: [(0, '55.987')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:21,638][03976] Updated weights for policy 0, policy_version 61175 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:23,337][03976] Updated weights for policy 0, policy_version 61185 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:25,035][03976] Updated weights for policy 0, policy_version 61195 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:06:26,168][03423] Fps is (10 sec: 47513.4, 60 sec: 48196.2, 300 sec: 47819.0). Total num frames: 481353728. Throughput: 0: 12054.9. Samples: 7817576. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:06:26,169][03423] Avg episode reward: [(0, '55.511')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:26,745][03976] Updated weights for policy 0, policy_version 61205 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:28,468][03976] Updated weights for policy 0, policy_version 61215 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:30,129][03976] Updated weights for policy 0, policy_version 61225 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:06:31,167][03423] Fps is (10 sec: 48334.1, 60 sec: 48332.8, 300 sec: 47846.9). Total num frames: 481599488. Throughput: 0: 12044.5. Samples: 7889788. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:06:31,168][03423] Avg episode reward: [(0, '54.394')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:31,841][03976] Updated weights for policy 0, policy_version 61235 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:33,541][03976] Updated weights for policy 0, policy_version 61245 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:35,213][03976] Updated weights for policy 0, policy_version 61255 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:06:36,168][03423] Fps is (10 sec: 48332.3, 60 sec: 48196.2, 300 sec: 47819.0). Total num frames: 481837056. Throughput: 0: 12052.7. Samples: 7926008. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:06:36,169][03423] Avg episode reward: [(0, '52.376')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:36,900][03976] Updated weights for policy 0, policy_version 61265 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:38,637][03976] Updated weights for policy 0, policy_version 61275 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:40,329][03976] Updated weights for policy 0, policy_version 61285 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:06:41,168][03423] Fps is (10 sec: 48332.5, 60 sec: 48332.8, 300 sec: 47819.1). Total num frames: 482082816. Throughput: 0: 12045.2. Samples: 7998316. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:06:41,169][03423] Avg episode reward: [(0, '55.718')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:42,015][03976] Updated weights for policy 0, policy_version 61295 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:43,719][03976] Updated weights for policy 0, policy_version 61305 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:45,431][03976] Updated weights for policy 0, policy_version 61315 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:06:46,167][03423] Fps is (10 sec: 48333.7, 60 sec: 48196.3, 300 sec: 47819.1). Total num frames: 482320384. Throughput: 0: 12040.3. Samples: 8070616. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:06:46,168][03423] Avg episode reward: [(0, '56.383')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:47,145][03976] Updated weights for policy 0, policy_version 61325 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:48,836][03976] Updated weights for policy 0, policy_version 61335 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:50,516][03976] Updated weights for policy 0, policy_version 61345 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:06:51,168][03423] Fps is (10 sec: 47513.4, 60 sec: 48196.2, 300 sec: 47791.3). Total num frames: 482557952. Throughput: 0: 12035.3. Samples: 8106792. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:06:51,169][03423] Avg episode reward: [(0, '55.262')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:52,212][03976] Updated weights for policy 0, policy_version 61355 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:53,925][03976] Updated weights for policy 0, policy_version 61365 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:55,604][03976] Updated weights for policy 0, policy_version 61375 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:06:56,167][03423] Fps is (10 sec: 48332.9, 60 sec: 48196.5, 300 sec: 47819.1). Total num frames: 482803712. Throughput: 0: 12044.4. Samples: 8179240. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:06:56,169][03423] Avg episode reward: [(0, '53.580')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:57,300][03976] Updated weights for policy 0, policy_version 61385 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:06:59,003][03976] Updated weights for policy 0, policy_version 61395 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:00,686][03976] Updated weights for policy 0, policy_version 61405 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:07:01,168][03423] Fps is (10 sec: 48332.3, 60 sec: 48196.2, 300 sec: 47791.3). Total num frames: 483041280. Throughput: 0: 12055.1. Samples: 8251684. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:07:01,169][03423] Avg episode reward: [(0, '54.353')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:02,411][03976] Updated weights for policy 0, policy_version 61415 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:04,098][03976] Updated weights for policy 0, policy_version 61425 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:05,785][03976] Updated weights for policy 0, policy_version 61435 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:07:06,168][03423] Fps is (10 sec: 48332.6, 60 sec: 48196.4, 300 sec: 47791.3). Total num frames: 483287040. Throughput: 0: 12050.8. Samples: 8287760. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:07:06,168][03423] Avg episode reward: [(0, '55.186')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:07,497][03976] Updated weights for policy 0, policy_version 61445 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:09,196][03976] Updated weights for policy 0, policy_version 61455 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:10,899][03976] Updated weights for policy 0, policy_version 61465 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:07:11,168][03423] Fps is (10 sec: 48332.0, 60 sec: 48196.1, 300 sec: 47819.1). Total num frames: 483524608. Throughput: 0: 12059.4. Samples: 8360252. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:07:11,169][03423] Avg episode reward: [(0, '54.922')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:12,598][03976] Updated weights for policy 0, policy_version 61475 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:14,334][03976] Updated weights for policy 0, policy_version 61485 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:16,019][03976] Updated weights for policy 0, policy_version 61495 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:07:16,167][03423] Fps is (10 sec: 47513.8, 60 sec: 48059.8, 300 sec: 47846.8). Total num frames: 483762176. Throughput: 0: 12062.5. Samples: 8432600. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:07:16,168][03423] Avg episode reward: [(0, '56.330')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:17,657][03976] Updated weights for policy 0, policy_version 61505 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:19,373][03976] Updated weights for policy 0, policy_version 61515 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:21,081][03976] Updated weights for policy 0, policy_version 61525 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:07:21,167][03423] Fps is (10 sec: 48334.6, 60 sec: 48196.5, 300 sec: 47874.6). Total num frames: 484007936. Throughput: 0: 12060.0. Samples: 8468704. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:07:21,168][03423] Avg episode reward: [(0, '54.665')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:22,788][03976] Updated weights for policy 0, policy_version 61535 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:24,457][03976] Updated weights for policy 0, policy_version 61545 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:26,163][03976] Updated weights for policy 0, policy_version 61555 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:07:26,168][03423] Fps is (10 sec: 49151.0, 60 sec: 48332.7, 300 sec: 47930.1). Total num frames: 484253696. Throughput: 0: 12063.8. Samples: 8541188. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:07:26,169][03423] Avg episode reward: [(0, '55.677')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:27,869][03976] Updated weights for policy 0, policy_version 61565 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:29,540][03976] Updated weights for policy 0, policy_version 61575 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:07:31,167][03423] Fps is (10 sec: 48332.7, 60 sec: 48196.2, 300 sec: 47957.9). Total num frames: 484491264. Throughput: 0: 12071.6. Samples: 8613840. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:07:31,169][03423] Avg episode reward: [(0, '55.232')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:31,232][03976] Updated weights for policy 0, policy_version 61585 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:32,953][03976] Updated weights for policy 0, policy_version 61595 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:34,650][03976] Updated weights for policy 0, policy_version 61605 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:07:36,167][03423] Fps is (10 sec: 47514.6, 60 sec: 48196.4, 300 sec: 47985.7). Total num frames: 484728832. Throughput: 0: 12065.4. Samples: 8649732. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:07:36,169][03423] Avg episode reward: [(0, '54.226')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:36,343][03976] Updated weights for policy 0, policy_version 61615 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:38,054][03976] Updated weights for policy 0, policy_version 61625 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:39,765][03976] Updated weights for policy 0, policy_version 61635 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:07:41,167][03423] Fps is (10 sec: 48332.8, 60 sec: 48196.3, 300 sec: 48013.5). Total num frames: 484974592. Throughput: 0: 12069.9. Samples: 8722384. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:07:41,168][03423] Avg episode reward: [(0, '52.706')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:41,474][03976] Updated weights for policy 0, policy_version 61645 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:43,141][03976] Updated weights for policy 0, policy_version 61655 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:44,873][03976] Updated weights for policy 0, policy_version 61665 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:07:46,168][03423] Fps is (10 sec: 48331.7, 60 sec: 48196.1, 300 sec: 48013.4). Total num frames: 485212160. Throughput: 0: 12061.1. Samples: 8794436. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:07:46,169][03423] Avg episode reward: [(0, '57.200')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:46,565][03976] Updated weights for policy 0, policy_version 61675 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:48,284][03976] Updated weights for policy 0, policy_version 61685 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:49,972][03976] Updated weights for policy 0, policy_version 61695 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:07:51,167][03423] Fps is (10 sec: 48332.9, 60 sec: 48332.9, 300 sec: 48041.2). Total num frames: 485457920. Throughput: 0: 12061.0. Samples: 8830504. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:07:51,169][03423] Avg episode reward: [(0, '53.850')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:51,676][03976] Updated weights for policy 0, policy_version 61705 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:53,344][03976] Updated weights for policy 0, policy_version 61715 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:55,038][03976] Updated weights for policy 0, policy_version 61725 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:07:56,168][03423] Fps is (10 sec: 48333.7, 60 sec: 48196.2, 300 sec: 48041.2). Total num frames: 485695488. Throughput: 0: 12058.1. Samples: 8902864. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:07:56,169][03423] Avg episode reward: [(0, '56.362')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:56,717][03976] Updated weights for policy 0, policy_version 61735 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:07:58,444][03976] Updated weights for policy 0, policy_version 61745 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:00,161][03976] Updated weights for policy 0, policy_version 61755 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:08:01,167][03423] Fps is (10 sec: 47513.7, 60 sec: 48196.4, 300 sec: 48069.0). Total num frames: 485933056. Throughput: 0: 12050.9. Samples: 8974892. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:08:01,169][03423] Avg episode reward: [(0, '55.171')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:01,867][03976] Updated weights for policy 0, policy_version 61765 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:03,549][03976] Updated weights for policy 0, policy_version 61775 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:05,279][03976] Updated weights for policy 0, policy_version 61785 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:08:06,168][03423] Fps is (10 sec: 48332.3, 60 sec: 48196.2, 300 sec: 48124.5). Total num frames: 486178816. Throughput: 0: 12056.0. Samples: 9011224. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:08:06,169][03423] Avg episode reward: [(0, '55.938')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:06,174][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000061790_486178816.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:08:06,244][03956] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000060377_474603520.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:06,960][03976] Updated weights for policy 0, policy_version 61795 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:08,692][03976] Updated weights for policy 0, policy_version 61805 (0.0014)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:10,411][03976] Updated weights for policy 0, policy_version 61815 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:08:11,167][03423] Fps is (10 sec: 48333.1, 60 sec: 48196.6, 300 sec: 48152.3). Total num frames: 486416384. Throughput: 0: 12049.2. Samples: 9083400. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:08:11,168][03423] Avg episode reward: [(0, '52.826')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:12,087][03976] Updated weights for policy 0, policy_version 61825 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:13,817][03976] Updated weights for policy 0, policy_version 61835 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:15,522][03976] Updated weights for policy 0, policy_version 61845 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:08:16,168][03423] Fps is (10 sec: 47514.0, 60 sec: 48196.2, 300 sec: 48152.3). Total num frames: 486653952. Throughput: 0: 12025.1. Samples: 9154968. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:08:16,169][03423] Avg episode reward: [(0, '49.664')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:17,182][03976] Updated weights for policy 0, policy_version 61855 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:18,881][03976] Updated weights for policy 0, policy_version 61865 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:20,560][03976] Updated weights for policy 0, policy_version 61875 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:08:21,168][03423] Fps is (10 sec: 48332.2, 60 sec: 48196.2, 300 sec: 48207.8). Total num frames: 486899712. Throughput: 0: 12041.9. Samples: 9191620. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:08:21,169][03423] Avg episode reward: [(0, '52.102')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:22,277][03976] Updated weights for policy 0, policy_version 61885 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:23,954][03976] Updated weights for policy 0, policy_version 61895 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:25,662][03976] Updated weights for policy 0, policy_version 61905 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:08:26,168][03423] Fps is (10 sec: 48332.5, 60 sec: 48059.8, 300 sec: 48235.6). Total num frames: 487137280. Throughput: 0: 12032.1. Samples: 9263832. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:08:26,169][03423] Avg episode reward: [(0, '50.935')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:27,378][03976] Updated weights for policy 0, policy_version 61915 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:29,117][03976] Updated weights for policy 0, policy_version 61925 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:30,783][03976] Updated weights for policy 0, policy_version 61935 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:08:31,167][03423] Fps is (10 sec: 48333.0, 60 sec: 48196.3, 300 sec: 48263.4). Total num frames: 487383040. Throughput: 0: 12040.3. Samples: 9336248. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:08:31,168][03423] Avg episode reward: [(0, '53.853')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:32,475][03976] Updated weights for policy 0, policy_version 61945 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:34,171][03976] Updated weights for policy 0, policy_version 61955 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:35,853][03976] Updated weights for policy 0, policy_version 61965 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:08:36,167][03423] Fps is (10 sec: 48333.6, 60 sec: 48196.3, 300 sec: 48263.4). Total num frames: 487620608. Throughput: 0: 12041.5. Samples: 9372372. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:08:36,168][03423] Avg episode reward: [(0, '55.972')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:37,574][03976] Updated weights for policy 0, policy_version 61975 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:39,246][03976] Updated weights for policy 0, policy_version 61985 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:40,962][03976] Updated weights for policy 0, policy_version 61995 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:08:41,167][03423] Fps is (10 sec: 48333.0, 60 sec: 48196.3, 300 sec: 48263.4). Total num frames: 487866368. Throughput: 0: 12045.3. Samples: 9444900. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:08:41,168][03423] Avg episode reward: [(0, '56.147')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:42,653][03976] Updated weights for policy 0, policy_version 62005 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:44,330][03976] Updated weights for policy 0, policy_version 62015 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:46,029][03976] Updated weights for policy 0, policy_version 62025 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:08:46,167][03423] Fps is (10 sec: 48332.7, 60 sec: 48196.5, 300 sec: 48235.6). Total num frames: 488103936. Throughput: 0: 12056.1. Samples: 9517416. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:08:46,168][03423] Avg episode reward: [(0, '54.903')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:47,762][03976] Updated weights for policy 0, policy_version 62035 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:49,456][03976] Updated weights for policy 0, policy_version 62045 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:51,112][03976] Updated weights for policy 0, policy_version 62055 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:08:51,167][03423] Fps is (10 sec: 48332.5, 60 sec: 48196.2, 300 sec: 48263.4). Total num frames: 488349696. Throughput: 0: 12059.1. Samples: 9553880. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:08:51,168][03423] Avg episode reward: [(0, '54.805')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:52,814][03976] Updated weights for policy 0, policy_version 62065 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:54,518][03976] Updated weights for policy 0, policy_version 62075 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:08:56,167][03423] Fps is (10 sec: 48332.6, 60 sec: 48196.3, 300 sec: 48235.6). Total num frames: 488587264. Throughput: 0: 12058.7. Samples: 9626044. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:08:56,168][03423] Avg episode reward: [(0, '55.455')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:56,218][03976] Updated weights for policy 0, policy_version 62085 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:57,918][03976] Updated weights for policy 0, policy_version 62095 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:08:59,604][03976] Updated weights for policy 0, policy_version 62105 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:09:01,167][03423] Fps is (10 sec: 48333.4, 60 sec: 48332.9, 300 sec: 48263.4). Total num frames: 488833024. Throughput: 0: 12083.2. Samples: 9698712. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:09:01,168][03423] Avg episode reward: [(0, '55.733')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:01,282][03976] Updated weights for policy 0, policy_version 62115 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:02,960][03976] Updated weights for policy 0, policy_version 62125 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:04,649][03976] Updated weights for policy 0, policy_version 62135 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:09:06,167][03423] Fps is (10 sec: 48332.9, 60 sec: 48196.4, 300 sec: 48235.6). Total num frames: 489070592. Throughput: 0: 12067.2. Samples: 9734644. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:09:06,168][03423] Avg episode reward: [(0, '54.845')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:06,369][03976] Updated weights for policy 0, policy_version 62145 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:08,036][03976] Updated weights for policy 0, policy_version 62155 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:09,772][03976] Updated weights for policy 0, policy_version 62165 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:09:11,168][03423] Fps is (10 sec: 48331.9, 60 sec: 48332.7, 300 sec: 48263.4). Total num frames: 489316352. Throughput: 0: 12077.3. Samples: 9807312. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:09:11,169][03423] Avg episode reward: [(0, '54.689')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:11,471][03976] Updated weights for policy 0, policy_version 62175 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:13,191][03976] Updated weights for policy 0, policy_version 62185 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:14,868][03976] Updated weights for policy 0, policy_version 62195 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:09:16,167][03423] Fps is (10 sec: 48332.8, 60 sec: 48332.9, 300 sec: 48235.6). Total num frames: 489553920. Throughput: 0: 12076.1. Samples: 9879672. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:09:16,169][03423] Avg episode reward: [(0, '54.605')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:16,624][03976] Updated weights for policy 0, policy_version 62205 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:18,277][03976] Updated weights for policy 0, policy_version 62215 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:20,040][03976] Updated weights for policy 0, policy_version 62225 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:09:21,168][03423] Fps is (10 sec: 48333.0, 60 sec: 48332.8, 300 sec: 48263.4). Total num frames: 489799680. Throughput: 0: 12065.0. Samples: 9915296. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:09:21,169][03423] Avg episode reward: [(0, '56.604')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:21,729][03976] Updated weights for policy 0, policy_version 62235 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:23,427][03976] Updated weights for policy 0, policy_version 62245 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:25,108][03976] Updated weights for policy 0, policy_version 62255 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:09:26,167][03423] Fps is (10 sec: 48332.8, 60 sec: 48332.9, 300 sec: 48235.6). Total num frames: 490037248. Throughput: 0: 12062.4. Samples: 9987708. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:09:26,169][03423] Avg episode reward: [(0, '55.753')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:26,797][03976] Updated weights for policy 0, policy_version 62265 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:28,494][03976] Updated weights for policy 0, policy_version 62275 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:30,186][03976] Updated weights for policy 0, policy_version 62285 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:09:31,168][03423] Fps is (10 sec: 47513.4, 60 sec: 48196.2, 300 sec: 48235.6). Total num frames: 490274816. Throughput: 0: 12058.9. Samples: 10060068. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:09:31,169][03423] Avg episode reward: [(0, '53.403')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:31,898][03976] Updated weights for policy 0, policy_version 62295 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:33,597][03976] Updated weights for policy 0, policy_version 62305 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:35,285][03976] Updated weights for policy 0, policy_version 62315 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:09:36,168][03423] Fps is (10 sec: 48332.3, 60 sec: 48332.7, 300 sec: 48235.6). Total num frames: 490520576. Throughput: 0: 12050.3. Samples: 10096144. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:09:36,168][03423] Avg episode reward: [(0, '56.069')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:36,973][03976] Updated weights for policy 0, policy_version 62325 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:38,696][03976] Updated weights for policy 0, policy_version 62335 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:40,397][03976] Updated weights for policy 0, policy_version 62345 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:09:41,168][03423] Fps is (10 sec: 48332.5, 60 sec: 48196.1, 300 sec: 48235.6). Total num frames: 490758144. Throughput: 0: 12047.6. Samples: 10168188. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:09:41,169][03423] Avg episode reward: [(0, '52.681')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:42,063][03976] Updated weights for policy 0, policy_version 62355 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:43,793][03976] Updated weights for policy 0, policy_version 62365 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:45,506][03976] Updated weights for policy 0, policy_version 62375 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:09:46,168][03423] Fps is (10 sec: 47513.7, 60 sec: 48196.2, 300 sec: 48207.9). Total num frames: 490995712. Throughput: 0: 12040.2. Samples: 10240524. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:09:46,169][03423] Avg episode reward: [(0, '54.373')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:47,191][03976] Updated weights for policy 0, policy_version 62385 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:48,889][03976] Updated weights for policy 0, policy_version 62395 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:50,584][03976] Updated weights for policy 0, policy_version 62405 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:09:51,168][03423] Fps is (10 sec: 48333.4, 60 sec: 48196.3, 300 sec: 48235.6). Total num frames: 491241472. Throughput: 0: 12050.0. Samples: 10276896. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:09:51,168][03423] Avg episode reward: [(0, '55.625')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:52,304][03976] Updated weights for policy 0, policy_version 62415 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:54,005][03976] Updated weights for policy 0, policy_version 62425 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:55,728][03976] Updated weights for policy 0, policy_version 62435 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:09:56,168][03423] Fps is (10 sec: 48332.0, 60 sec: 48196.1, 300 sec: 48207.8). Total num frames: 491479040. Throughput: 0: 12039.8. Samples: 10349104. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:09:56,169][03423] Avg episode reward: [(0, '54.838')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:57,381][03976] Updated weights for policy 0, policy_version 62445 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:09:59,092][03976] Updated weights for policy 0, policy_version 62455 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:00,769][03976] Updated weights for policy 0, policy_version 62465 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:10:01,167][03423] Fps is (10 sec: 48332.9, 60 sec: 48196.2, 300 sec: 48235.6). Total num frames: 491724800. Throughput: 0: 12043.5. Samples: 10421628. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:10:01,169][03423] Avg episode reward: [(0, '55.188')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:02,486][03976] Updated weights for policy 0, policy_version 62475 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:04,180][03976] Updated weights for policy 0, policy_version 62485 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:05,867][03976] Updated weights for policy 0, policy_version 62495 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:10:06,168][03423] Fps is (10 sec: 48333.8, 60 sec: 48196.2, 300 sec: 48207.9). Total num frames: 491962368. Throughput: 0: 12051.6. Samples: 10457620. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:10:06,169][03423] Avg episode reward: [(0, '53.379')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:06,211][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000062497_491970560.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:10:06,276][03956] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000061084_480395264.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:07,580][03976] Updated weights for policy 0, policy_version 62505 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:09,307][03976] Updated weights for policy 0, policy_version 62515 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:11,001][03976] Updated weights for policy 0, policy_version 62525 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:10:11,167][03423] Fps is (10 sec: 48332.7, 60 sec: 48196.3, 300 sec: 48235.6). Total num frames: 492208128. Throughput: 0: 12054.0. Samples: 10530136. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:10:11,168][03423] Avg episode reward: [(0, '54.379')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:12,687][03976] Updated weights for policy 0, policy_version 62535 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:14,380][03976] Updated weights for policy 0, policy_version 62545 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:16,087][03976] Updated weights for policy 0, policy_version 62555 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:10:16,168][03423] Fps is (10 sec: 48332.4, 60 sec: 48196.2, 300 sec: 48235.6). Total num frames: 492445696. Throughput: 0: 12055.1. Samples: 10602548. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:10:16,169][03423] Avg episode reward: [(0, '54.567')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:17,786][03976] Updated weights for policy 0, policy_version 62565 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:19,458][03976] Updated weights for policy 0, policy_version 62575 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:21,127][03976] Updated weights for policy 0, policy_version 62585 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:10:21,168][03423] Fps is (10 sec: 48332.2, 60 sec: 48196.2, 300 sec: 48235.6). Total num frames: 492691456. Throughput: 0: 12058.1. Samples: 10638760. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:10:21,169][03423] Avg episode reward: [(0, '53.577')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:22,881][03976] Updated weights for policy 0, policy_version 62595 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:24,547][03976] Updated weights for policy 0, policy_version 62605 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:10:26,168][03423] Fps is (10 sec: 48332.8, 60 sec: 48196.2, 300 sec: 48235.6). Total num frames: 492929024. Throughput: 0: 12062.3. Samples: 10710992. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:10:26,169][03423] Avg episode reward: [(0, '53.683')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:26,282][03976] Updated weights for policy 0, policy_version 62615 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:27,986][03976] Updated weights for policy 0, policy_version 62625 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:29,704][03976] Updated weights for policy 0, policy_version 62635 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:10:31,167][03423] Fps is (10 sec: 47514.3, 60 sec: 48196.3, 300 sec: 48207.8). Total num frames: 493166592. Throughput: 0: 12057.0. Samples: 10783088. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:10:31,168][03423] Avg episode reward: [(0, '58.179')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:31,186][03956] Saving new best policy, reward=58.179!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:31,364][03976] Updated weights for policy 0, policy_version 62645 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:33,066][03976] Updated weights for policy 0, policy_version 62655 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:34,778][03976] Updated weights for policy 0, policy_version 62665 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:10:36,168][03423] Fps is (10 sec: 48333.1, 60 sec: 48196.3, 300 sec: 48235.6). Total num frames: 493412352. Throughput: 0: 12049.0. Samples: 10819100. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:10:36,169][03423] Avg episode reward: [(0, '52.506')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:36,521][03976] Updated weights for policy 0, policy_version 62675 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:38,208][03976] Updated weights for policy 0, policy_version 62685 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:39,924][03976] Updated weights for policy 0, policy_version 62695 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:10:41,168][03423] Fps is (10 sec: 48332.1, 60 sec: 48196.3, 300 sec: 48207.8). Total num frames: 493649920. Throughput: 0: 12051.8. Samples: 10891436. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:10:41,169][03423] Avg episode reward: [(0, '50.890')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:41,589][03976] Updated weights for policy 0, policy_version 62705 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:43,286][03976] Updated weights for policy 0, policy_version 62715 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:44,967][03976] Updated weights for policy 0, policy_version 62725 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:10:46,168][03423] Fps is (10 sec: 47513.5, 60 sec: 48196.3, 300 sec: 48207.8). Total num frames: 493887488. Throughput: 0: 12041.9. Samples: 10963516. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:10:46,169][03423] Avg episode reward: [(0, '53.749')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:46,676][03976] Updated weights for policy 0, policy_version 62735 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:48,382][03976] Updated weights for policy 0, policy_version 62745 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:50,100][03976] Updated weights for policy 0, policy_version 62755 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:10:51,168][03423] Fps is (10 sec: 48333.4, 60 sec: 48196.3, 300 sec: 48207.9). Total num frames: 494133248. Throughput: 0: 12051.7. Samples: 10999948. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:10:51,169][03423] Avg episode reward: [(0, '54.441')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:51,788][03976] Updated weights for policy 0, policy_version 62765 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:53,492][03976] Updated weights for policy 0, policy_version 62775 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:55,214][03976] Updated weights for policy 0, policy_version 62785 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:10:56,168][03423] Fps is (10 sec: 48333.0, 60 sec: 48196.4, 300 sec: 48207.8). Total num frames: 494370816. Throughput: 0: 12036.2. Samples: 11071764. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:10:56,169][03423] Avg episode reward: [(0, '55.017')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:56,911][03976] Updated weights for policy 0, policy_version 62795 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:10:58,640][03976] Updated weights for policy 0, policy_version 62805 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:00,328][03976] Updated weights for policy 0, policy_version 62815 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:11:01,168][03423] Fps is (10 sec: 48332.7, 60 sec: 48196.2, 300 sec: 48207.9). Total num frames: 494616576. Throughput: 0: 12030.2. Samples: 11143908. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:11:01,169][03423] Avg episode reward: [(0, '54.722')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:02,015][03976] Updated weights for policy 0, policy_version 62825 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:03,697][03976] Updated weights for policy 0, policy_version 62835 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:05,401][03976] Updated weights for policy 0, policy_version 62845 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:11:06,167][03423] Fps is (10 sec: 48332.9, 60 sec: 48196.3, 300 sec: 48207.9). Total num frames: 494854144. Throughput: 0: 12031.9. Samples: 11180192. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:11:06,168][03423] Avg episode reward: [(0, '52.284')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:07,085][03976] Updated weights for policy 0, policy_version 62855 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:08,770][03976] Updated weights for policy 0, policy_version 62865 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:10,498][03976] Updated weights for policy 0, policy_version 62875 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:11:11,168][03423] Fps is (10 sec: 47513.5, 60 sec: 48059.7, 300 sec: 48180.1). Total num frames: 495091712. Throughput: 0: 12034.5. Samples: 11252544. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:11:11,169][03423] Avg episode reward: [(0, '56.108')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:12,184][03976] Updated weights for policy 0, policy_version 62885 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:13,915][03976] Updated weights for policy 0, policy_version 62895 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:15,635][03976] Updated weights for policy 0, policy_version 62905 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:11:16,168][03423] Fps is (10 sec: 48332.7, 60 sec: 48196.3, 300 sec: 48207.9). Total num frames: 495337472. Throughput: 0: 12039.5. Samples: 11324868. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:11:16,169][03423] Avg episode reward: [(0, '54.731')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:17,328][03976] Updated weights for policy 0, policy_version 62915 (0.0013)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:18,987][03976] Updated weights for policy 0, policy_version 62925 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:20,645][03976] Updated weights for policy 0, policy_version 62935 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:11:21,168][03423] Fps is (10 sec: 48332.0, 60 sec: 48059.7, 300 sec: 48207.8). Total num frames: 495575040. Throughput: 0: 12035.8. Samples: 11360712. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:11:21,169][03423] Avg episode reward: [(0, '54.804')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:22,393][03976] Updated weights for policy 0, policy_version 62945 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:24,058][03976] Updated weights for policy 0, policy_version 62955 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:25,742][03976] Updated weights for policy 0, policy_version 62965 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:11:26,168][03423] Fps is (10 sec: 48332.1, 60 sec: 48196.2, 300 sec: 48207.8). Total num frames: 495820800. Throughput: 0: 12048.8. Samples: 11433632. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:11:26,169][03423] Avg episode reward: [(0, '53.748')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:27,435][03976] Updated weights for policy 0, policy_version 62975 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:29,135][03976] Updated weights for policy 0, policy_version 62985 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:30,856][03976] Updated weights for policy 0, policy_version 62995 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:11:31,168][03423] Fps is (10 sec: 48333.6, 60 sec: 48196.2, 300 sec: 48207.9). Total num frames: 496058368. Throughput: 0: 12057.5. Samples: 11506104. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:11:31,169][03423] Avg episode reward: [(0, '54.722')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:32,557][03976] Updated weights for policy 0, policy_version 63005 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:34,285][03976] Updated weights for policy 0, policy_version 63015 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:35,959][03976] Updated weights for policy 0, policy_version 63025 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:11:36,168][03423] Fps is (10 sec: 48333.2, 60 sec: 48196.2, 300 sec: 48207.8). Total num frames: 496304128. Throughput: 0: 12056.7. Samples: 11542500. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:11:36,169][03423] Avg episode reward: [(0, '52.422')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:37,686][03976] Updated weights for policy 0, policy_version 63035 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:39,347][03976] Updated weights for policy 0, policy_version 63045 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:41,033][03976] Updated weights for policy 0, policy_version 63055 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:11:41,168][03423] Fps is (10 sec: 48333.0, 60 sec: 48196.4, 300 sec: 48207.8). Total num frames: 496541696. Throughput: 0: 12054.7. Samples: 11614224. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:11:41,169][03423] Avg episode reward: [(0, '54.314')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:42,735][03976] Updated weights for policy 0, policy_version 63065 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:44,430][03976] Updated weights for policy 0, policy_version 63075 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:46,144][03976] Updated weights for policy 0, policy_version 63085 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:11:46,167][03423] Fps is (10 sec: 48333.5, 60 sec: 48332.9, 300 sec: 48235.6). Total num frames: 496787456. Throughput: 0: 12062.6. Samples: 11686724. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:11:46,169][03423] Avg episode reward: [(0, '55.428')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:47,833][03976] Updated weights for policy 0, policy_version 63095 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:49,519][03976] Updated weights for policy 0, policy_version 63105 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:11:51,167][03423] Fps is (10 sec: 48332.9, 60 sec: 48196.3, 300 sec: 48207.8). Total num frames: 497025024. Throughput: 0: 12066.1. Samples: 11723168. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:11:51,168][03423] Avg episode reward: [(0, '56.730')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:51,241][03976] Updated weights for policy 0, policy_version 63115 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:52,940][03976] Updated weights for policy 0, policy_version 63125 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:54,680][03976] Updated weights for policy 0, policy_version 63135 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:11:56,167][03423] Fps is (10 sec: 48332.6, 60 sec: 48332.8, 300 sec: 48235.6). Total num frames: 497270784. Throughput: 0: 12068.3. Samples: 11795616. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:11:56,168][03423] Avg episode reward: [(0, '55.142')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:56,335][03976] Updated weights for policy 0, policy_version 63145 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:58,041][03976] Updated weights for policy 0, policy_version 63155 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:11:59,690][03976] Updated weights for policy 0, policy_version 63165 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:12:01,168][03423] Fps is (10 sec: 48332.1, 60 sec: 48196.2, 300 sec: 48207.8). Total num frames: 497508352. Throughput: 0: 12069.4. Samples: 11867992. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:12:01,169][03423] Avg episode reward: [(0, '53.514')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:01,385][03976] Updated weights for policy 0, policy_version 63175 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:03,080][03976] Updated weights for policy 0, policy_version 63185 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:04,793][03976] Updated weights for policy 0, policy_version 63195 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:12:06,167][03423] Fps is (10 sec: 47513.7, 60 sec: 48196.3, 300 sec: 48207.9). Total num frames: 497745920. Throughput: 0: 12078.8. Samples: 11904256. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:12:06,168][03423] Avg episode reward: [(0, '55.977')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:06,187][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000063203_497754112.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:12:06,256][03956] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000061790_486178816.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:06,535][03976] Updated weights for policy 0, policy_version 63205 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:08,221][03976] Updated weights for policy 0, policy_version 63215 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:09,934][03976] Updated weights for policy 0, policy_version 63225 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:12:11,168][03423] Fps is (10 sec: 48333.2, 60 sec: 48332.8, 300 sec: 48235.6). Total num frames: 497991680. Throughput: 0: 12066.8. Samples: 11976636. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:12:11,169][03423] Avg episode reward: [(0, '56.336')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:11,627][03976] Updated weights for policy 0, policy_version 63235 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:13,325][03976] Updated weights for policy 0, policy_version 63245 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:14,995][03976] Updated weights for policy 0, policy_version 63255 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:12:16,168][03423] Fps is (10 sec: 48332.3, 60 sec: 48196.2, 300 sec: 48207.8). Total num frames: 498229248. Throughput: 0: 12049.3. Samples: 12048324. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:12:16,169][03423] Avg episode reward: [(0, '54.090')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:16,717][03976] Updated weights for policy 0, policy_version 63265 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:18,405][03976] Updated weights for policy 0, policy_version 63275 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:20,114][03976] Updated weights for policy 0, policy_version 63285 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:12:21,168][03423] Fps is (10 sec: 48332.7, 60 sec: 48332.9, 300 sec: 48207.9). Total num frames: 498475008. Throughput: 0: 12057.6. Samples: 12085092. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:12:21,169][03423] Avg episode reward: [(0, '55.891')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:21,806][03976] Updated weights for policy 0, policy_version 63295 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:23,507][03976] Updated weights for policy 0, policy_version 63305 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:25,187][03976] Updated weights for policy 0, policy_version 63315 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:12:26,168][03423] Fps is (10 sec: 48332.8, 60 sec: 48196.4, 300 sec: 48207.8). Total num frames: 498712576. Throughput: 0: 12069.9. Samples: 12157368. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:12:26,169][03423] Avg episode reward: [(0, '55.394')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:26,860][03976] Updated weights for policy 0, policy_version 63325 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:28,582][03976] Updated weights for policy 0, policy_version 63335 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:30,297][03976] Updated weights for policy 0, policy_version 63345 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:12:31,168][03423] Fps is (10 sec: 47513.6, 60 sec: 48196.2, 300 sec: 48207.8). Total num frames: 498950144. Throughput: 0: 12059.2. Samples: 12229388. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:12:31,169][03423] Avg episode reward: [(0, '55.423')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:31,974][03976] Updated weights for policy 0, policy_version 63355 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:33,710][03976] Updated weights for policy 0, policy_version 63365 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:35,413][03976] Updated weights for policy 0, policy_version 63375 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:12:36,168][03423] Fps is (10 sec: 48332.6, 60 sec: 48196.3, 300 sec: 48207.8). Total num frames: 499195904. Throughput: 0: 12050.8. Samples: 12265456. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:12:36,170][03423] Avg episode reward: [(0, '54.992')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:37,099][03976] Updated weights for policy 0, policy_version 63385 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:38,791][03976] Updated weights for policy 0, policy_version 63395 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:40,503][03976] Updated weights for policy 0, policy_version 63405 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:12:41,168][03423] Fps is (10 sec: 48333.0, 60 sec: 48196.3, 300 sec: 48207.9). Total num frames: 499433472. Throughput: 0: 12057.1. Samples: 12338188. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:12:41,169][03423] Avg episode reward: [(0, '54.861')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:42,195][03976] Updated weights for policy 0, policy_version 63415 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:43,896][03976] Updated weights for policy 0, policy_version 63425 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:45,582][03976] Updated weights for policy 0, policy_version 63435 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:12:46,168][03423] Fps is (10 sec: 48332.7, 60 sec: 48196.1, 300 sec: 48207.8). Total num frames: 499679232. Throughput: 0: 12052.4. Samples: 12410352. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:12:46,169][03423] Avg episode reward: [(0, '56.215')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:47,308][03976] Updated weights for policy 0, policy_version 63445 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:49,051][03976] Updated weights for policy 0, policy_version 63455 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:50,779][03976] Updated weights for policy 0, policy_version 63465 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:12:51,168][03423] Fps is (10 sec: 48332.3, 60 sec: 48196.2, 300 sec: 48207.8). Total num frames: 499916800. Throughput: 0: 12048.0. Samples: 12446420. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:12:51,169][03423] Avg episode reward: [(0, '55.390')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:52,460][03976] Updated weights for policy 0, policy_version 63475 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:54,132][03976] Updated weights for policy 0, policy_version 63485 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:55,776][03976] Updated weights for policy 0, policy_version 63495 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:12:56,168][03423] Fps is (10 sec: 48333.1, 60 sec: 48196.2, 300 sec: 48235.6). Total num frames: 500162560. Throughput: 0: 12044.0. Samples: 12518616. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:12:56,169][03423] Avg episode reward: [(0, '55.728')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:57,512][03976] Updated weights for policy 0, policy_version 63505 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:12:59,183][03976] Updated weights for policy 0, policy_version 63515 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:00,893][03976] Updated weights for policy 0, policy_version 63525 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:13:01,167][03423] Fps is (10 sec: 48333.5, 60 sec: 48196.4, 300 sec: 48207.9). Total num frames: 500400128. Throughput: 0: 12061.8. Samples: 12591104. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:13:01,169][03423] Avg episode reward: [(0, '54.415')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:02,590][03976] Updated weights for policy 0, policy_version 63535 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:04,304][03976] Updated weights for policy 0, policy_version 63545 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:05,978][03976] Updated weights for policy 0, policy_version 63555 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:13:06,167][03423] Fps is (10 sec: 48333.2, 60 sec: 48332.8, 300 sec: 48235.6). Total num frames: 500645888. Throughput: 0: 12053.1. Samples: 12627480. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:13:06,169][03423] Avg episode reward: [(0, '52.832')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:07,674][03976] Updated weights for policy 0, policy_version 63565 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:09,420][03976] Updated weights for policy 0, policy_version 63575 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:11,142][03976] Updated weights for policy 0, policy_version 63585 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:13:11,167][03423] Fps is (10 sec: 48332.8, 60 sec: 48196.3, 300 sec: 48235.6). Total num frames: 500883456. Throughput: 0: 12053.2. Samples: 12699760. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:13:11,168][03423] Avg episode reward: [(0, '55.619')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:12,802][03976] Updated weights for policy 0, policy_version 63595 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:14,503][03976] Updated weights for policy 0, policy_version 63605 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:13:16,168][03423] Fps is (10 sec: 47513.2, 60 sec: 48196.3, 300 sec: 48207.8). Total num frames: 501121024. Throughput: 0: 12049.3. Samples: 12771608. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:13:16,168][03423] Avg episode reward: [(0, '54.183')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:16,170][03976] Updated weights for policy 0, policy_version 63615 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:17,898][03976] Updated weights for policy 0, policy_version 63625 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:19,586][03976] Updated weights for policy 0, policy_version 63635 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:13:21,168][03423] Fps is (10 sec: 48332.7, 60 sec: 48196.3, 300 sec: 48235.6). Total num frames: 501366784. Throughput: 0: 12056.5. Samples: 12807996. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:13:21,169][03423] Avg episode reward: [(0, '55.388')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:21,310][03976] Updated weights for policy 0, policy_version 63645 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:22,967][03976] Updated weights for policy 0, policy_version 63655 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:24,695][03976] Updated weights for policy 0, policy_version 63665 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:13:26,168][03423] Fps is (10 sec: 48332.9, 60 sec: 48196.3, 300 sec: 48207.8). Total num frames: 501604352. Throughput: 0: 12047.0. Samples: 12880304. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:13:26,169][03423] Avg episode reward: [(0, '56.470')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:26,364][03976] Updated weights for policy 0, policy_version 63675 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:28,077][03976] Updated weights for policy 0, policy_version 63685 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:29,789][03976] Updated weights for policy 0, policy_version 63695 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:13:31,168][03423] Fps is (10 sec: 48332.9, 60 sec: 48332.9, 300 sec: 48235.6). Total num frames: 501850112. Throughput: 0: 12045.7. Samples: 12952408. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:13:31,169][03423] Avg episode reward: [(0, '55.626')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:31,477][03976] Updated weights for policy 0, policy_version 63705 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:33,168][03976] Updated weights for policy 0, policy_version 63715 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:34,905][03976] Updated weights for policy 0, policy_version 63725 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:13:36,168][03423] Fps is (10 sec: 48332.6, 60 sec: 48196.3, 300 sec: 48207.8). Total num frames: 502087680. Throughput: 0: 12046.0. Samples: 12988488. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:13:36,169][03423] Avg episode reward: [(0, '53.110')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:36,622][03976] Updated weights for policy 0, policy_version 63735 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:38,345][03976] Updated weights for policy 0, policy_version 63745 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:40,050][03976] Updated weights for policy 0, policy_version 63755 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:13:41,168][03423] Fps is (10 sec: 47513.3, 60 sec: 48196.2, 300 sec: 48207.8). Total num frames: 502325248. Throughput: 0: 12043.8. Samples: 13060588. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:13:41,169][03423] Avg episode reward: [(0, '53.767')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:41,742][03976] Updated weights for policy 0, policy_version 63765 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:43,421][03976] Updated weights for policy 0, policy_version 63775 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:45,154][03976] Updated weights for policy 0, policy_version 63785 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:13:46,168][03423] Fps is (10 sec: 47513.9, 60 sec: 48059.8, 300 sec: 48180.1). Total num frames: 502562816. Throughput: 0: 12029.3. Samples: 13132424. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:13:46,168][03423] Avg episode reward: [(0, '54.821')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:46,837][03976] Updated weights for policy 0, policy_version 63795 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:48,540][03976] Updated weights for policy 0, policy_version 63805 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:50,255][03976] Updated weights for policy 0, policy_version 63815 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:13:51,168][03423] Fps is (10 sec: 48332.8, 60 sec: 48196.3, 300 sec: 48207.8). Total num frames: 502808576. Throughput: 0: 12030.5. Samples: 13168852. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:13:51,169][03423] Avg episode reward: [(0, '55.139')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:51,932][03976] Updated weights for policy 0, policy_version 63825 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:53,657][03976] Updated weights for policy 0, policy_version 63835 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:55,369][03976] Updated weights for policy 0, policy_version 63845 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:13:56,168][03423] Fps is (10 sec: 48332.5, 60 sec: 48059.7, 300 sec: 48180.0). Total num frames: 503046144. Throughput: 0: 12023.7. Samples: 13240828. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:13:56,169][03423] Avg episode reward: [(0, '52.723')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:57,072][03976] Updated weights for policy 0, policy_version 63855 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:13:58,764][03976] Updated weights for policy 0, policy_version 63865 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:00,444][03976] Updated weights for policy 0, policy_version 63875 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:14:01,167][03423] Fps is (10 sec: 48333.1, 60 sec: 48196.3, 300 sec: 48207.8). Total num frames: 503291904. Throughput: 0: 12033.5. Samples: 13313116. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:14:01,169][03423] Avg episode reward: [(0, '54.511')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:02,159][03976] Updated weights for policy 0, policy_version 63885 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:03,868][03976] Updated weights for policy 0, policy_version 63895 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:05,549][03976] Updated weights for policy 0, policy_version 63905 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:14:06,168][03423] Fps is (10 sec: 48333.0, 60 sec: 48059.7, 300 sec: 48180.1). Total num frames: 503529472. Throughput: 0: 12025.1. Samples: 13349124. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:14:06,169][03423] Avg episode reward: [(0, '55.466')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:06,172][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000063908_503529472.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:14:06,244][03956] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000062497_491970560.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:07,330][03976] Updated weights for policy 0, policy_version 63915 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:09,006][03976] Updated weights for policy 0, policy_version 63925 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:10,708][03976] Updated weights for policy 0, policy_version 63935 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:14:11,168][03423] Fps is (10 sec: 47513.5, 60 sec: 48059.7, 300 sec: 48180.1). Total num frames: 503767040. Throughput: 0: 12014.9. Samples: 13420972. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:14:11,169][03423] Avg episode reward: [(0, '54.395')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:12,408][03976] Updated weights for policy 0, policy_version 63945 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:14,127][03976] Updated weights for policy 0, policy_version 63955 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:15,831][03976] Updated weights for policy 0, policy_version 63965 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:14:16,168][03423] Fps is (10 sec: 47513.5, 60 sec: 48059.7, 300 sec: 48152.3). Total num frames: 504004608. Throughput: 0: 12020.3. Samples: 13493324. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:14:16,168][03423] Avg episode reward: [(0, '52.897')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:17,507][03976] Updated weights for policy 0, policy_version 63975 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:19,204][03976] Updated weights for policy 0, policy_version 63985 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:20,866][03976] Updated weights for policy 0, policy_version 63995 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:14:21,168][03423] Fps is (10 sec: 48332.7, 60 sec: 48059.7, 300 sec: 48180.1). Total num frames: 504250368. Throughput: 0: 12020.0. Samples: 13529388. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:14:21,169][03423] Avg episode reward: [(0, '55.078')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:22,574][03976] Updated weights for policy 0, policy_version 64005 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:24,307][03976] Updated weights for policy 0, policy_version 64015 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:25,983][03976] Updated weights for policy 0, policy_version 64025 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:14:26,168][03423] Fps is (10 sec: 49151.3, 60 sec: 48196.1, 300 sec: 48207.8). Total num frames: 504496128. Throughput: 0: 12028.2. Samples: 13601860. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:14:26,169][03423] Avg episode reward: [(0, '53.699')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:27,667][03976] Updated weights for policy 0, policy_version 64035 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:29,377][03976] Updated weights for policy 0, policy_version 64045 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:31,070][03976] Updated weights for policy 0, policy_version 64055 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:14:31,168][03423] Fps is (10 sec: 48332.4, 60 sec: 48059.6, 300 sec: 48180.1). Total num frames: 504733696. Throughput: 0: 12042.2. Samples: 13674324. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:14:31,169][03423] Avg episode reward: [(0, '55.690')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:32,785][03976] Updated weights for policy 0, policy_version 64065 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:34,478][03976] Updated weights for policy 0, policy_version 64075 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:14:36,167][03423] Fps is (10 sec: 47514.8, 60 sec: 48059.8, 300 sec: 48180.1). Total num frames: 504971264. Throughput: 0: 12036.2. Samples: 13710480. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:14:36,168][03423] Avg episode reward: [(0, '55.837')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:36,176][03976] Updated weights for policy 0, policy_version 64085 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:37,856][03976] Updated weights for policy 0, policy_version 64095 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:39,591][03976] Updated weights for policy 0, policy_version 64105 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:14:41,168][03423] Fps is (10 sec: 48332.9, 60 sec: 48196.3, 300 sec: 48207.8). Total num frames: 505217024. Throughput: 0: 12042.3. Samples: 13782732. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:14:41,169][03423] Avg episode reward: [(0, '55.355')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:41,278][03976] Updated weights for policy 0, policy_version 64115 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:42,984][03976] Updated weights for policy 0, policy_version 64125 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:44,673][03976] Updated weights for policy 0, policy_version 64135 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:14:46,168][03423] Fps is (10 sec: 48332.4, 60 sec: 48196.3, 300 sec: 48180.1). Total num frames: 505454592. Throughput: 0: 12037.6. Samples: 13854808. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:14:46,168][03423] Avg episode reward: [(0, '55.232')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:46,364][03976] Updated weights for policy 0, policy_version 64145 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:48,063][03976] Updated weights for policy 0, policy_version 64155 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:49,766][03976] Updated weights for policy 0, policy_version 64165 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:14:51,168][03423] Fps is (10 sec: 48332.8, 60 sec: 48196.3, 300 sec: 48207.9). Total num frames: 505700352. Throughput: 0: 12040.3. Samples: 13890940. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:14:51,168][03423] Avg episode reward: [(0, '54.660')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:51,518][03976] Updated weights for policy 0, policy_version 64175 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:53,218][03976] Updated weights for policy 0, policy_version 64185 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:54,926][03976] Updated weights for policy 0, policy_version 64195 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:14:56,167][03423] Fps is (10 sec: 48333.0, 60 sec: 48196.3, 300 sec: 48180.1). Total num frames: 505937920. Throughput: 0: 12049.4. Samples: 13963196. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:14:56,169][03423] Avg episode reward: [(0, '55.059')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:56,605][03976] Updated weights for policy 0, policy_version 64205 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:14:58,321][03976] Updated weights for policy 0, policy_version 64215 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:00,032][03976] Updated weights for policy 0, policy_version 64225 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:15:01,167][03423] Fps is (10 sec: 47514.2, 60 sec: 48059.8, 300 sec: 48180.1). Total num frames: 506175488. Throughput: 0: 12043.1. Samples: 14035260. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:15:01,168][03423] Avg episode reward: [(0, '54.999')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:01,700][03976] Updated weights for policy 0, policy_version 64235 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:03,413][03976] Updated weights for policy 0, policy_version 64245 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:05,124][03976] Updated weights for policy 0, policy_version 64255 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:15:06,168][03423] Fps is (10 sec: 48332.7, 60 sec: 48196.3, 300 sec: 48180.1). Total num frames: 506421248. Throughput: 0: 12053.1. Samples: 14071776. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:15:06,169][03423] Avg episode reward: [(0, '52.713')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:06,795][03976] Updated weights for policy 0, policy_version 64265 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:08,485][03976] Updated weights for policy 0, policy_version 64275 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:10,185][03976] Updated weights for policy 0, policy_version 64285 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:15:11,168][03423] Fps is (10 sec: 48332.4, 60 sec: 48196.2, 300 sec: 48180.1). Total num frames: 506658816. Throughput: 0: 12045.6. Samples: 14143912. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:15:11,169][03423] Avg episode reward: [(0, '55.383')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:11,956][03976] Updated weights for policy 0, policy_version 64295 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:13,626][03976] Updated weights for policy 0, policy_version 64305 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:15,309][03976] Updated weights for policy 0, policy_version 64315 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:15:16,168][03423] Fps is (10 sec: 48332.9, 60 sec: 48332.9, 300 sec: 48180.1). Total num frames: 506904576. Throughput: 0: 12039.1. Samples: 14216080. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:15:16,169][03423] Avg episode reward: [(0, '54.133')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:16,943][03976] Updated weights for policy 0, policy_version 64325 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:18,661][03976] Updated weights for policy 0, policy_version 64335 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:20,375][03976] Updated weights for policy 0, policy_version 64345 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:15:21,168][03423] Fps is (10 sec: 48332.9, 60 sec: 48196.3, 300 sec: 48180.1). Total num frames: 507142144. Throughput: 0: 12041.8. Samples: 14252360. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:15:21,169][03423] Avg episode reward: [(0, '55.022')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:22,080][03976] Updated weights for policy 0, policy_version 64355 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:23,771][03976] Updated weights for policy 0, policy_version 64365 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:25,478][03976] Updated weights for policy 0, policy_version 64375 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:15:26,167][03423] Fps is (10 sec: 48333.2, 60 sec: 48196.5, 300 sec: 48207.8). Total num frames: 507387904. Throughput: 0: 12041.6. Samples: 14324604. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:15:26,168][03423] Avg episode reward: [(0, '53.836')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:27,188][03976] Updated weights for policy 0, policy_version 64385 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:28,905][03976] Updated weights for policy 0, policy_version 64395 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:30,596][03976] Updated weights for policy 0, policy_version 64405 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:15:31,168][03423] Fps is (10 sec: 48332.9, 60 sec: 48196.4, 300 sec: 48180.1). Total num frames: 507625472. Throughput: 0: 12048.9. Samples: 14397008. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:15:31,169][03423] Avg episode reward: [(0, '55.105')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:32,286][03976] Updated weights for policy 0, policy_version 64415 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:34,029][03976] Updated weights for policy 0, policy_version 64425 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:35,733][03976] Updated weights for policy 0, policy_version 64435 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:15:36,167][03423] Fps is (10 sec: 47513.5, 60 sec: 48196.3, 300 sec: 48180.1). Total num frames: 507863040. Throughput: 0: 12047.6. Samples: 14433080. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:15:36,168][03423] Avg episode reward: [(0, '53.302')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:37,408][03976] Updated weights for policy 0, policy_version 64445 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:39,103][03976] Updated weights for policy 0, policy_version 64455 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:40,794][03976] Updated weights for policy 0, policy_version 64465 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:15:41,167][03423] Fps is (10 sec: 48332.8, 60 sec: 48196.3, 300 sec: 48207.8). Total num frames: 508108800. Throughput: 0: 12050.7. Samples: 14505476. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:15:41,168][03423] Avg episode reward: [(0, '52.056')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:42,475][03976] Updated weights for policy 0, policy_version 64475 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:44,234][03976] Updated weights for policy 0, policy_version 64485 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:45,918][03976] Updated weights for policy 0, policy_version 64495 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:15:46,168][03423] Fps is (10 sec: 48331.8, 60 sec: 48196.2, 300 sec: 48180.0). Total num frames: 508346368. Throughput: 0: 12051.2. Samples: 14577568. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:15:46,169][03423] Avg episode reward: [(0, '53.947')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:47,642][03976] Updated weights for policy 0, policy_version 64505 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:49,315][03976] Updated weights for policy 0, policy_version 64515 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:51,038][03976] Updated weights for policy 0, policy_version 64525 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:15:51,167][03423] Fps is (10 sec: 47513.7, 60 sec: 48059.8, 300 sec: 48180.1). Total num frames: 508583936. Throughput: 0: 12036.5. Samples: 14613416. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:15:51,168][03423] Avg episode reward: [(0, '55.426')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:52,737][03976] Updated weights for policy 0, policy_version 64535 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:54,447][03976] Updated weights for policy 0, policy_version 64545 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:56,103][03976] Updated weights for policy 0, policy_version 64555 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:15:56,168][03423] Fps is (10 sec: 48333.6, 60 sec: 48196.3, 300 sec: 48180.1). Total num frames: 508829696. Throughput: 0: 12040.5. Samples: 14685736. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:15:56,169][03423] Avg episode reward: [(0, '54.055')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:57,819][03976] Updated weights for policy 0, policy_version 64565 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:15:59,473][03976] Updated weights for policy 0, policy_version 64575 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:01,167][03976] Updated weights for policy 0, policy_version 64585 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:16:01,168][03423] Fps is (10 sec: 49151.5, 60 sec: 48332.7, 300 sec: 48207.8). Total num frames: 509075456. Throughput: 0: 12048.9. Samples: 14758280. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:16:01,176][03423] Avg episode reward: [(0, '55.899')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:02,882][03976] Updated weights for policy 0, policy_version 64595 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:04,587][03976] Updated weights for policy 0, policy_version 64605 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:16:06,168][03423] Fps is (10 sec: 48332.3, 60 sec: 48196.2, 300 sec: 48207.8). Total num frames: 509313024. Throughput: 0: 12046.9. Samples: 14794472. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:16:06,169][03423] Avg episode reward: [(0, '53.629')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:06,172][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000064614_509313024.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:16:06,243][03956] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000063203_497754112.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:06,309][03976] Updated weights for policy 0, policy_version 64615 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:07,983][03976] Updated weights for policy 0, policy_version 64625 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:09,668][03976] Updated weights for policy 0, policy_version 64635 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:16:11,167][03423] Fps is (10 sec: 47514.1, 60 sec: 48196.3, 300 sec: 48180.1). Total num frames: 509550592. Throughput: 0: 12040.3. Samples: 14866416. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:16:11,168][03423] Avg episode reward: [(0, '53.446')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:11,395][03976] Updated weights for policy 0, policy_version 64645 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:13,082][03976] Updated weights for policy 0, policy_version 64655 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:14,813][03976] Updated weights for policy 0, policy_version 64665 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:16:16,167][03423] Fps is (10 sec: 48333.3, 60 sec: 48196.3, 300 sec: 48207.9). Total num frames: 509796352. Throughput: 0: 12043.4. Samples: 14938960. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:16:16,169][03423] Avg episode reward: [(0, '54.381')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:16,478][03976] Updated weights for policy 0, policy_version 64675 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:18,192][03976] Updated weights for policy 0, policy_version 64685 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:19,933][03976] Updated weights for policy 0, policy_version 64695 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:16:21,168][03423] Fps is (10 sec: 48332.5, 60 sec: 48196.3, 300 sec: 48180.1). Total num frames: 510033920. Throughput: 0: 12047.2. Samples: 14975204. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:16:21,169][03423] Avg episode reward: [(0, '53.142')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:21,646][03976] Updated weights for policy 0, policy_version 64705 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:23,298][03976] Updated weights for policy 0, policy_version 64715 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:25,001][03976] Updated weights for policy 0, policy_version 64725 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:16:26,168][03423] Fps is (10 sec: 47513.5, 60 sec: 48059.7, 300 sec: 48180.1). Total num frames: 510271488. Throughput: 0: 12046.9. Samples: 15047588. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:16:26,169][03423] Avg episode reward: [(0, '56.163')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:26,680][03976] Updated weights for policy 0, policy_version 64735 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:28,411][03976] Updated weights for policy 0, policy_version 64745 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:30,121][03976] Updated weights for policy 0, policy_version 64755 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:16:31,168][03423] Fps is (10 sec: 48332.9, 60 sec: 48196.3, 300 sec: 48180.1). Total num frames: 510517248. Throughput: 0: 12043.9. Samples: 15119544. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:16:31,168][03423] Avg episode reward: [(0, '56.777')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:31,816][03976] Updated weights for policy 0, policy_version 64765 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:33,520][03976] Updated weights for policy 0, policy_version 64775 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:35,216][03976] Updated weights for policy 0, policy_version 64785 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:16:36,168][03423] Fps is (10 sec: 48332.3, 60 sec: 48196.1, 300 sec: 48180.1). Total num frames: 510754816. Throughput: 0: 12054.9. Samples: 15155888. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:16:36,169][03423] Avg episode reward: [(0, '56.172')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:36,902][03976] Updated weights for policy 0, policy_version 64795 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:38,585][03976] Updated weights for policy 0, policy_version 64805 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:40,255][03976] Updated weights for policy 0, policy_version 64815 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:16:41,168][03423] Fps is (10 sec: 48332.6, 60 sec: 48196.2, 300 sec: 48180.0). Total num frames: 511000576. Throughput: 0: 12056.0. Samples: 15228256. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:16:41,169][03423] Avg episode reward: [(0, '56.989')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:41,986][03976] Updated weights for policy 0, policy_version 64825 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:43,718][03976] Updated weights for policy 0, policy_version 64835 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:45,429][03976] Updated weights for policy 0, policy_version 64845 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:16:46,167][03423] Fps is (10 sec: 48333.3, 60 sec: 48196.4, 300 sec: 48180.1). Total num frames: 511238144. Throughput: 0: 12049.6. Samples: 15300512. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:16:46,168][03423] Avg episode reward: [(0, '54.426')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:47,131][03976] Updated weights for policy 0, policy_version 64855 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:48,799][03976] Updated weights for policy 0, policy_version 64865 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:50,500][03976] Updated weights for policy 0, policy_version 64875 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:16:51,168][03423] Fps is (10 sec: 47513.4, 60 sec: 48196.2, 300 sec: 48152.3). Total num frames: 511475712. Throughput: 0: 12045.9. Samples: 15336536. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:16:51,169][03423] Avg episode reward: [(0, '54.223')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:52,223][03976] Updated weights for policy 0, policy_version 64885 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:53,935][03976] Updated weights for policy 0, policy_version 64895 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:55,608][03976] Updated weights for policy 0, policy_version 64905 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:16:56,167][03423] Fps is (10 sec: 48332.9, 60 sec: 48196.3, 300 sec: 48180.1). Total num frames: 511721472. Throughput: 0: 12048.5. Samples: 15408600. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:16:56,169][03423] Avg episode reward: [(0, '57.584')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:57,315][03976] Updated weights for policy 0, policy_version 64915 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:16:58,997][03976] Updated weights for policy 0, policy_version 64925 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:00,688][03976] Updated weights for policy 0, policy_version 64935 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:17:01,168][03423] Fps is (10 sec: 48332.7, 60 sec: 48059.7, 300 sec: 48180.0). Total num frames: 511959040. Throughput: 0: 12038.3. Samples: 15480684. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:17:01,168][03423] Avg episode reward: [(0, '56.877')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:02,435][03976] Updated weights for policy 0, policy_version 64945 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:04,134][03976] Updated weights for policy 0, policy_version 64955 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:05,857][03976] Updated weights for policy 0, policy_version 64965 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:17:06,168][03423] Fps is (10 sec: 47513.5, 60 sec: 48059.8, 300 sec: 48152.3). Total num frames: 512196608. Throughput: 0: 12033.0. Samples: 15516688. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:17:06,169][03423] Avg episode reward: [(0, '53.323')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:07,537][03976] Updated weights for policy 0, policy_version 64975 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:09,225][03976] Updated weights for policy 0, policy_version 64985 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:10,930][03976] Updated weights for policy 0, policy_version 64995 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:17:11,168][03423] Fps is (10 sec: 48333.3, 60 sec: 48196.2, 300 sec: 48180.1). Total num frames: 512442368. Throughput: 0: 12040.9. Samples: 15589428. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:17:11,169][03423] Avg episode reward: [(0, '55.966')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:12,626][03976] Updated weights for policy 0, policy_version 65005 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:14,322][03976] Updated weights for policy 0, policy_version 65015 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:15,981][03976] Updated weights for policy 0, policy_version 65025 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:17:16,167][03423] Fps is (10 sec: 48332.9, 60 sec: 48059.7, 300 sec: 48152.3). Total num frames: 512679936. Throughput: 0: 12036.8. Samples: 15661200. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:17:16,168][03423] Avg episode reward: [(0, '55.903')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:17,702][03976] Updated weights for policy 0, policy_version 65035 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:19,395][03976] Updated weights for policy 0, policy_version 65045 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:21,118][03976] Updated weights for policy 0, policy_version 65055 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:17:21,168][03423] Fps is (10 sec: 48332.9, 60 sec: 48196.3, 300 sec: 48180.1). Total num frames: 512925696. Throughput: 0: 12044.2. Samples: 15697876. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:17:21,169][03423] Avg episode reward: [(0, '55.084')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:22,824][03976] Updated weights for policy 0, policy_version 65065 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:24,504][03976] Updated weights for policy 0, policy_version 65075 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:17:26,167][03423] Fps is (10 sec: 48332.8, 60 sec: 48196.3, 300 sec: 48180.1). Total num frames: 513163264. Throughput: 0: 12039.3. Samples: 15770024. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:17:26,168][03423] Avg episode reward: [(0, '53.838')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:26,188][03976] Updated weights for policy 0, policy_version 65085 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:27,902][03976] Updated weights for policy 0, policy_version 65095 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:29,592][03976] Updated weights for policy 0, policy_version 65105 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:17:31,167][03423] Fps is (10 sec: 48333.0, 60 sec: 48196.3, 300 sec: 48180.1). Total num frames: 513409024. Throughput: 0: 12046.7. Samples: 15842612. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:17:31,169][03423] Avg episode reward: [(0, '54.803')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:31,296][03976] Updated weights for policy 0, policy_version 65115 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:32,964][03976] Updated weights for policy 0, policy_version 65125 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:34,664][03976] Updated weights for policy 0, policy_version 65135 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:17:36,168][03423] Fps is (10 sec: 48331.4, 60 sec: 48196.1, 300 sec: 48180.0). Total num frames: 513646592. Throughput: 0: 12048.6. Samples: 15878724. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:17:36,169][03423] Avg episode reward: [(0, '54.701')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:36,386][03976] Updated weights for policy 0, policy_version 65145 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:38,084][03976] Updated weights for policy 0, policy_version 65155 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:39,811][03976] Updated weights for policy 0, policy_version 65165 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:17:41,167][03423] Fps is (10 sec: 48333.0, 60 sec: 48196.4, 300 sec: 48180.1). Total num frames: 513892352. Throughput: 0: 12052.6. Samples: 15950968. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:17:41,168][03423] Avg episode reward: [(0, '54.368')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:41,494][03976] Updated weights for policy 0, policy_version 65175 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:43,178][03976] Updated weights for policy 0, policy_version 65185 (0.0013)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:44,873][03976] Updated weights for policy 0, policy_version 65195 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:17:46,168][03423] Fps is (10 sec: 48333.7, 60 sec: 48196.2, 300 sec: 48180.1). Total num frames: 514129920. Throughput: 0: 12059.6. Samples: 16023364. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:17:46,169][03423] Avg episode reward: [(0, '56.338')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:46,581][03976] Updated weights for policy 0, policy_version 65205 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:48,280][03976] Updated weights for policy 0, policy_version 65215 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:49,993][03976] Updated weights for policy 0, policy_version 65225 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:17:51,168][03423] Fps is (10 sec: 47512.9, 60 sec: 48196.3, 300 sec: 48152.3). Total num frames: 514367488. Throughput: 0: 12064.3. Samples: 16059584. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:17:51,169][03423] Avg episode reward: [(0, '56.085')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:51,663][03976] Updated weights for policy 0, policy_version 65235 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:53,340][03976] Updated weights for policy 0, policy_version 65245 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:55,041][03976] Updated weights for policy 0, policy_version 65255 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:17:56,168][03423] Fps is (10 sec: 48333.0, 60 sec: 48196.2, 300 sec: 48180.1). Total num frames: 514613248. Throughput: 0: 12058.9. Samples: 16132080. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:17:56,168][03423] Avg episode reward: [(0, '54.156')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:56,755][03976] Updated weights for policy 0, policy_version 65265 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:17:58,486][03976] Updated weights for policy 0, policy_version 65275 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:00,163][03976] Updated weights for policy 0, policy_version 65285 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:18:01,168][03423] Fps is (10 sec: 48333.2, 60 sec: 48196.4, 300 sec: 48152.3). Total num frames: 514850816. Throughput: 0: 12070.7. Samples: 16204380. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:18:01,170][03423] Avg episode reward: [(0, '54.161')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:01,925][03976] Updated weights for policy 0, policy_version 65295 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:03,585][03976] Updated weights for policy 0, policy_version 65305 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:05,266][03976] Updated weights for policy 0, policy_version 65315 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:18:06,168][03423] Fps is (10 sec: 48333.1, 60 sec: 48332.8, 300 sec: 48180.1). Total num frames: 515096576. Throughput: 0: 12062.8. Samples: 16240704. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:18:06,169][03423] Avg episode reward: [(0, '53.099')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:06,173][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000065320_515096576.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:18:06,242][03956] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000063908_503529472.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:07,000][03976] Updated weights for policy 0, policy_version 65325 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:08,687][03976] Updated weights for policy 0, policy_version 65335 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:10,385][03976] Updated weights for policy 0, policy_version 65345 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:18:11,167][03423] Fps is (10 sec: 48332.9, 60 sec: 48196.3, 300 sec: 48180.1). Total num frames: 515334144. Throughput: 0: 12045.8. Samples: 16312084. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:18:11,169][03423] Avg episode reward: [(0, '55.695')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:12,097][03976] Updated weights for policy 0, policy_version 65355 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:13,797][03976] Updated weights for policy 0, policy_version 65365 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:15,511][03976] Updated weights for policy 0, policy_version 65375 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:18:16,167][03423] Fps is (10 sec: 47513.7, 60 sec: 48196.3, 300 sec: 48152.3). Total num frames: 515571712. Throughput: 0: 12040.8. Samples: 16384448. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:18:16,168][03423] Avg episode reward: [(0, '54.472')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:17,222][03976] Updated weights for policy 0, policy_version 65385 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:18,923][03976] Updated weights for policy 0, policy_version 65395 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:20,656][03976] Updated weights for policy 0, policy_version 65405 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:18:21,168][03423] Fps is (10 sec: 48332.7, 60 sec: 48196.3, 300 sec: 48180.1). Total num frames: 515817472. Throughput: 0: 12041.5. Samples: 16420588. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:18:21,169][03423] Avg episode reward: [(0, '55.227')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:22,401][03976] Updated weights for policy 0, policy_version 65415 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:24,085][03976] Updated weights for policy 0, policy_version 65425 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:25,778][03976] Updated weights for policy 0, policy_version 65435 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:18:26,168][03423] Fps is (10 sec: 48332.5, 60 sec: 48196.2, 300 sec: 48152.3). Total num frames: 516055040. Throughput: 0: 12033.6. Samples: 16492480. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:18:26,169][03423] Avg episode reward: [(0, '53.248')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:27,455][03976] Updated weights for policy 0, policy_version 65445 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:29,132][03976] Updated weights for policy 0, policy_version 65455 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:30,818][03976] Updated weights for policy 0, policy_version 65465 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:18:31,168][03423] Fps is (10 sec: 48332.9, 60 sec: 48196.2, 300 sec: 48180.1). Total num frames: 516300800. Throughput: 0: 12032.6. Samples: 16564828. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:18:31,169][03423] Avg episode reward: [(0, '54.144')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:32,527][03976] Updated weights for policy 0, policy_version 65475 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:34,203][03976] Updated weights for policy 0, policy_version 65485 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:35,955][03976] Updated weights for policy 0, policy_version 65495 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:18:36,168][03423] Fps is (10 sec: 48332.5, 60 sec: 48196.4, 300 sec: 48180.1). Total num frames: 516538368. Throughput: 0: 12033.0. Samples: 16601068. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:18:36,169][03423] Avg episode reward: [(0, '54.059')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:37,593][03976] Updated weights for policy 0, policy_version 65505 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:39,328][03976] Updated weights for policy 0, policy_version 65515 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:41,008][03976] Updated weights for policy 0, policy_version 65525 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:18:41,168][03423] Fps is (10 sec: 47513.6, 60 sec: 48059.7, 300 sec: 48180.1). Total num frames: 516775936. Throughput: 0: 12023.9. Samples: 16673156. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:18:41,168][03423] Avg episode reward: [(0, '55.517')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:42,701][03976] Updated weights for policy 0, policy_version 65535 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:44,429][03976] Updated weights for policy 0, policy_version 65545 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:46,133][03976] Updated weights for policy 0, policy_version 65555 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:18:46,168][03423] Fps is (10 sec: 48332.8, 60 sec: 48196.3, 300 sec: 48180.1). Total num frames: 517021696. Throughput: 0: 12035.2. Samples: 16745964. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:18:46,169][03423] Avg episode reward: [(0, '54.841')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:47,829][03976] Updated weights for policy 0, policy_version 65565 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:49,546][03976] Updated weights for policy 0, policy_version 65575 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:18:51,168][03423] Fps is (10 sec: 48332.5, 60 sec: 48196.3, 300 sec: 48180.1). Total num frames: 517259264. Throughput: 0: 12023.9. Samples: 16781780. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:18:51,169][03423] Avg episode reward: [(0, '54.776')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:51,219][03976] Updated weights for policy 0, policy_version 65585 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:52,947][03976] Updated weights for policy 0, policy_version 65595 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:54,635][03976] Updated weights for policy 0, policy_version 65605 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:18:56,168][03423] Fps is (10 sec: 47514.0, 60 sec: 48059.8, 300 sec: 48152.3). Total num frames: 517496832. Throughput: 0: 12040.7. Samples: 16853916. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:18:56,168][03423] Avg episode reward: [(0, '54.445')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:56,320][03976] Updated weights for policy 0, policy_version 65615 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:57,962][03976] Updated weights for policy 0, policy_version 65625 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:18:59,635][03976] Updated weights for policy 0, policy_version 65635 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:19:01,168][03423] Fps is (10 sec: 49151.1, 60 sec: 48332.6, 300 sec: 48207.8). Total num frames: 517750784. Throughput: 0: 12069.8. Samples: 16927592. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:19:01,169][03423] Avg episode reward: [(0, '53.356')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:01,313][03976] Updated weights for policy 0, policy_version 65645 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:03,001][03976] Updated weights for policy 0, policy_version 65655 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:04,641][03976] Updated weights for policy 0, policy_version 65665 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:19:06,168][03423] Fps is (10 sec: 49971.3, 60 sec: 48332.8, 300 sec: 48235.6). Total num frames: 517996544. Throughput: 0: 12093.6. Samples: 16964800. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:19:06,169][03423] Avg episode reward: [(0, '53.766')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:06,275][03976] Updated weights for policy 0, policy_version 65675 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:07,898][03976] Updated weights for policy 0, policy_version 65685 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:09,567][03976] Updated weights for policy 0, policy_version 65695 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:19:11,168][03423] Fps is (10 sec: 49153.3, 60 sec: 48469.3, 300 sec: 48263.4). Total num frames: 518242304. Throughput: 0: 12150.7. Samples: 17039260. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:19:11,168][03423] Avg episode reward: [(0, '55.258')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:11,246][03976] Updated weights for policy 0, policy_version 65705 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:12,888][03976] Updated weights for policy 0, policy_version 65715 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:14,542][03976] Updated weights for policy 0, policy_version 65725 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:19:16,168][03423] Fps is (10 sec: 49151.8, 60 sec: 48605.8, 300 sec: 48263.4). Total num frames: 518488064. Throughput: 0: 12188.8. Samples: 17113324. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:19:16,168][03423] Avg episode reward: [(0, '56.703')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:16,184][03976] Updated weights for policy 0, policy_version 65735 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:17,835][03976] Updated weights for policy 0, policy_version 65745 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:19,498][03976] Updated weights for policy 0, policy_version 65755 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:21,158][03976] Updated weights for policy 0, policy_version 65765 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:19:21,168][03423] Fps is (10 sec: 49970.5, 60 sec: 48742.3, 300 sec: 48291.2). Total num frames: 518742016. Throughput: 0: 12206.7. Samples: 17150368. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:19:21,169][03423] Avg episode reward: [(0, '54.372')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:22,791][03976] Updated weights for policy 0, policy_version 65775 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:24,463][03976] Updated weights for policy 0, policy_version 65785 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:26,109][03976] Updated weights for policy 0, policy_version 65795 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:19:26,168][03423] Fps is (10 sec: 49971.1, 60 sec: 48878.9, 300 sec: 48318.9). Total num frames: 518987776. Throughput: 0: 12250.4. Samples: 17224424. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:19:26,169][03423] Avg episode reward: [(0, '53.429')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:27,782][03976] Updated weights for policy 0, policy_version 65805 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:29,468][03976] Updated weights for policy 0, policy_version 65815 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:31,117][03976] Updated weights for policy 0, policy_version 65825 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:19:31,168][03423] Fps is (10 sec: 49152.4, 60 sec: 48878.9, 300 sec: 48346.7). Total num frames: 519233536. Throughput: 0: 12281.1. Samples: 17298612. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:19:31,169][03423] Avg episode reward: [(0, '52.997')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:32,767][03976] Updated weights for policy 0, policy_version 65835 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:34,395][03976] Updated weights for policy 0, policy_version 65845 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:36,056][03976] Updated weights for policy 0, policy_version 65855 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:19:36,168][03423] Fps is (10 sec: 49151.7, 60 sec: 49015.4, 300 sec: 48346.7). Total num frames: 519479296. Throughput: 0: 12313.4. Samples: 17335884. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:19:36,169][03423] Avg episode reward: [(0, '52.956')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:37,699][03976] Updated weights for policy 0, policy_version 65865 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:39,362][03976] Updated weights for policy 0, policy_version 65875 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:40,993][03976] Updated weights for policy 0, policy_version 65885 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:19:41,167][03423] Fps is (10 sec: 49152.5, 60 sec: 49152.0, 300 sec: 48374.5). Total num frames: 519725056. Throughput: 0: 12357.4. Samples: 17409996. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:19:41,168][03423] Avg episode reward: [(0, '56.015')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:42,634][03976] Updated weights for policy 0, policy_version 65895 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:44,334][03976] Updated weights for policy 0, policy_version 65905 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:46,014][03976] Updated weights for policy 0, policy_version 65915 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:19:46,167][03423] Fps is (10 sec: 49152.7, 60 sec: 49152.1, 300 sec: 48374.5). Total num frames: 519970816. Throughput: 0: 12376.3. Samples: 17484524. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:19:46,168][03423] Avg episode reward: [(0, '57.343')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:47,670][03976] Updated weights for policy 0, policy_version 65925 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:49,315][03976] Updated weights for policy 0, policy_version 65935 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:50,953][03976] Updated weights for policy 0, policy_version 65945 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:19:51,168][03423] Fps is (10 sec: 49967.6, 60 sec: 49424.6, 300 sec: 48429.9). Total num frames: 520224768. Throughput: 0: 12366.8. Samples: 17521316. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:19:51,169][03423] Avg episode reward: [(0, '55.643')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:52,657][03976] Updated weights for policy 0, policy_version 65955 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:54,291][03976] Updated weights for policy 0, policy_version 65965 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:55,914][03976] Updated weights for policy 0, policy_version 65975 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:19:56,167][03423] Fps is (10 sec: 49971.3, 60 sec: 49561.6, 300 sec: 48457.8). Total num frames: 520470528. Throughput: 0: 12369.1. Samples: 17595868. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:19:56,169][03423] Avg episode reward: [(0, '54.853')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:57,589][03976] Updated weights for policy 0, policy_version 65985 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:19:59,241][03976] Updated weights for policy 0, policy_version 65995 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:00,895][03976] Updated weights for policy 0, policy_version 66005 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:20:01,168][03423] Fps is (10 sec: 49154.9, 60 sec: 49425.2, 300 sec: 48457.8). Total num frames: 520716288. Throughput: 0: 12371.8. Samples: 17670056. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:20:01,169][03423] Avg episode reward: [(0, '52.959')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:02,555][03976] Updated weights for policy 0, policy_version 66015 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:04,182][03976] Updated weights for policy 0, policy_version 66025 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:05,818][03976] Updated weights for policy 0, policy_version 66035 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:20:06,168][03423] Fps is (10 sec: 49970.9, 60 sec: 49561.6, 300 sec: 48513.3). Total num frames: 520970240. Throughput: 0: 12370.8. Samples: 17707052. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:20:06,169][03423] Avg episode reward: [(0, '55.758')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:06,172][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000066037_520970240.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:20:06,234][03956] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000064614_509313024.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:07,491][03976] Updated weights for policy 0, policy_version 66045 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:09,161][03976] Updated weights for policy 0, policy_version 66055 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:10,844][03976] Updated weights for policy 0, policy_version 66065 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:20:11,167][03423] Fps is (10 sec: 49971.7, 60 sec: 49561.6, 300 sec: 48513.3). Total num frames: 521216000. Throughput: 0: 12371.1. Samples: 17781124. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:20:11,168][03423] Avg episode reward: [(0, '57.140')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:12,527][03976] Updated weights for policy 0, policy_version 66075 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:14,119][03976] Updated weights for policy 0, policy_version 66085 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:15,769][03976] Updated weights for policy 0, policy_version 66095 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:20:16,168][03423] Fps is (10 sec: 49152.2, 60 sec: 49561.6, 300 sec: 48541.1). Total num frames: 521461760. Throughput: 0: 12381.6. Samples: 17855784. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:20:16,169][03423] Avg episode reward: [(0, '55.583')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:17,405][03976] Updated weights for policy 0, policy_version 66105 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:19,064][03976] Updated weights for policy 0, policy_version 66115 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:20,763][03976] Updated weights for policy 0, policy_version 66125 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:20:21,168][03423] Fps is (10 sec: 49151.9, 60 sec: 49425.2, 300 sec: 48541.1). Total num frames: 521707520. Throughput: 0: 12385.1. Samples: 17893212. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:20:21,168][03423] Avg episode reward: [(0, '54.655')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:22,379][03976] Updated weights for policy 0, policy_version 66135 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:24,031][03976] Updated weights for policy 0, policy_version 66145 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:25,693][03976] Updated weights for policy 0, policy_version 66155 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:20:26,168][03423] Fps is (10 sec: 49152.0, 60 sec: 49425.1, 300 sec: 48568.8). Total num frames: 521953280. Throughput: 0: 12381.0. Samples: 17967140. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:20:26,169][03423] Avg episode reward: [(0, '55.066')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:27,409][03976] Updated weights for policy 0, policy_version 66165 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:29,046][03976] Updated weights for policy 0, policy_version 66175 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:30,689][03976] Updated weights for policy 0, policy_version 66185 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:20:31,167][03423] Fps is (10 sec: 49152.1, 60 sec: 49425.1, 300 sec: 48596.6). Total num frames: 522199040. Throughput: 0: 12370.8. Samples: 18041208. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:20:31,169][03423] Avg episode reward: [(0, '53.915')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:32,330][03976] Updated weights for policy 0, policy_version 66195 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:34,006][03976] Updated weights for policy 0, policy_version 66205 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:35,662][03976] Updated weights for policy 0, policy_version 66215 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:20:36,167][03423] Fps is (10 sec: 49971.4, 60 sec: 49561.7, 300 sec: 48624.4). Total num frames: 522452992. Throughput: 0: 12370.1. Samples: 18077960. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:20:36,169][03423] Avg episode reward: [(0, '55.431')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:37,293][03976] Updated weights for policy 0, policy_version 66225 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:38,985][03976] Updated weights for policy 0, policy_version 66235 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:40,635][03976] Updated weights for policy 0, policy_version 66245 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:20:41,167][03423] Fps is (10 sec: 49971.3, 60 sec: 49561.6, 300 sec: 48652.2). Total num frames: 522698752. Throughput: 0: 12360.2. Samples: 18152076. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:20:41,169][03423] Avg episode reward: [(0, '54.580')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:42,298][03976] Updated weights for policy 0, policy_version 66255 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:43,978][03976] Updated weights for policy 0, policy_version 66265 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:45,639][03976] Updated weights for policy 0, policy_version 66275 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:20:46,168][03423] Fps is (10 sec: 49151.7, 60 sec: 49561.6, 300 sec: 48679.9). Total num frames: 522944512. Throughput: 0: 12357.8. Samples: 18226156. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:20:46,168][03423] Avg episode reward: [(0, '54.137')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:47,315][03976] Updated weights for policy 0, policy_version 66285 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:48,976][03976] Updated weights for policy 0, policy_version 66295 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:50,643][03976] Updated weights for policy 0, policy_version 66305 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:20:51,168][03423] Fps is (10 sec: 49151.8, 60 sec: 49425.6, 300 sec: 48679.9). Total num frames: 523190272. Throughput: 0: 12352.3. Samples: 18262904. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:20:51,169][03423] Avg episode reward: [(0, '53.688')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:52,313][03976] Updated weights for policy 0, policy_version 66315 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:53,958][03976] Updated weights for policy 0, policy_version 66325 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:55,576][03976] Updated weights for policy 0, policy_version 66335 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:20:56,168][03423] Fps is (10 sec: 49151.8, 60 sec: 49425.0, 300 sec: 48679.9). Total num frames: 523436032. Throughput: 0: 12355.9. Samples: 18337140. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:20:56,169][03423] Avg episode reward: [(0, '55.801')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:57,202][03976] Updated weights for policy 0, policy_version 66345 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:20:58,860][03976] Updated weights for policy 0, policy_version 66355 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:00,523][03976] Updated weights for policy 0, policy_version 66365 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:21:01,167][03423] Fps is (10 sec: 49152.5, 60 sec: 49425.2, 300 sec: 48707.7). Total num frames: 523681792. Throughput: 0: 12354.7. Samples: 18411744. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:21:01,169][03423] Avg episode reward: [(0, '53.766')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:02,181][03976] Updated weights for policy 0, policy_version 66375 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:03,829][03976] Updated weights for policy 0, policy_version 66385 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:05,472][03976] Updated weights for policy 0, policy_version 66395 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:21:06,168][03423] Fps is (10 sec: 49971.5, 60 sec: 49425.1, 300 sec: 48763.2). Total num frames: 523935744. Throughput: 0: 12351.8. Samples: 18449044. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:21:06,168][03423] Avg episode reward: [(0, '55.144')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:07,178][03976] Updated weights for policy 0, policy_version 66405 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:08,798][03976] Updated weights for policy 0, policy_version 66415 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:10,420][03976] Updated weights for policy 0, policy_version 66425 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:21:11,168][03423] Fps is (10 sec: 49970.6, 60 sec: 49425.0, 300 sec: 48763.2). Total num frames: 524181504. Throughput: 0: 12359.6. Samples: 18523324. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:21:11,169][03423] Avg episode reward: [(0, '56.755')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:12,111][03976] Updated weights for policy 0, policy_version 66435 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:13,777][03976] Updated weights for policy 0, policy_version 66445 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:15,391][03976] Updated weights for policy 0, policy_version 66455 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:21:16,168][03423] Fps is (10 sec: 49151.6, 60 sec: 49425.0, 300 sec: 48791.0). Total num frames: 524427264. Throughput: 0: 12369.8. Samples: 18597852. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:21:16,169][03423] Avg episode reward: [(0, '55.113')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:17,087][03976] Updated weights for policy 0, policy_version 66465 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:18,730][03976] Updated weights for policy 0, policy_version 66475 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:20,387][03976] Updated weights for policy 0, policy_version 66485 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:21:21,167][03423] Fps is (10 sec: 49152.2, 60 sec: 49425.1, 300 sec: 48818.8). Total num frames: 524673024. Throughput: 0: 12376.3. Samples: 18634892. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:21:21,169][03423] Avg episode reward: [(0, '54.405')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:22,027][03976] Updated weights for policy 0, policy_version 66495 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:23,722][03976] Updated weights for policy 0, policy_version 66505 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:25,360][03976] Updated weights for policy 0, policy_version 66515 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:21:26,168][03423] Fps is (10 sec: 49971.2, 60 sec: 49561.5, 300 sec: 48846.5). Total num frames: 524926976. Throughput: 0: 12378.0. Samples: 18709088. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:21:26,169][03423] Avg episode reward: [(0, '54.791')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:27,015][03976] Updated weights for policy 0, policy_version 66525 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:28,645][03976] Updated weights for policy 0, policy_version 66535 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:30,307][03976] Updated weights for policy 0, policy_version 66545 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:21:31,167][03423] Fps is (10 sec: 49971.4, 60 sec: 49561.6, 300 sec: 48874.3). Total num frames: 525172736. Throughput: 0: 12376.6. Samples: 18783100. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:21:31,168][03423] Avg episode reward: [(0, '55.830')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:31,955][03976] Updated weights for policy 0, policy_version 66555 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:33,615][03976] Updated weights for policy 0, policy_version 66565 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:35,260][03976] Updated weights for policy 0, policy_version 66575 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:21:36,168][03423] Fps is (10 sec: 49152.5, 60 sec: 49425.0, 300 sec: 48874.3). Total num frames: 525418496. Throughput: 0: 12382.1. Samples: 18820100. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:21:36,169][03423] Avg episode reward: [(0, '53.003')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:36,902][03976] Updated weights for policy 0, policy_version 66585 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:38,579][03976] Updated weights for policy 0, policy_version 66595 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:40,234][03976] Updated weights for policy 0, policy_version 66605 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:21:41,168][03423] Fps is (10 sec: 49151.6, 60 sec: 49425.0, 300 sec: 48902.1). Total num frames: 525664256. Throughput: 0: 12389.6. Samples: 18894672. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:21:41,169][03423] Avg episode reward: [(0, '55.632')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:41,876][03976] Updated weights for policy 0, policy_version 66615 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:43,561][03976] Updated weights for policy 0, policy_version 66625 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:45,225][03976] Updated weights for policy 0, policy_version 66635 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:21:46,167][03423] Fps is (10 sec: 49152.3, 60 sec: 49425.1, 300 sec: 48929.9). Total num frames: 525910016. Throughput: 0: 12372.4. Samples: 18968504. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:21:46,168][03423] Avg episode reward: [(0, '55.274')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:46,867][03976] Updated weights for policy 0, policy_version 66645 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:48,512][03976] Updated weights for policy 0, policy_version 66655 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:50,178][03976] Updated weights for policy 0, policy_version 66665 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:21:51,168][03423] Fps is (10 sec: 49152.0, 60 sec: 49425.1, 300 sec: 48929.8). Total num frames: 526155776. Throughput: 0: 12373.2. Samples: 19005840. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:21:51,169][03423] Avg episode reward: [(0, '53.773')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:51,842][03976] Updated weights for policy 0, policy_version 66675 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:53,485][03976] Updated weights for policy 0, policy_version 66685 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:55,113][03976] Updated weights for policy 0, policy_version 66695 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:21:56,167][03423] Fps is (10 sec: 49971.1, 60 sec: 49561.7, 300 sec: 48985.4). Total num frames: 526409728. Throughput: 0: 12369.8. Samples: 19079964. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:21:56,168][03423] Avg episode reward: [(0, '53.770')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:56,737][03976] Updated weights for policy 0, policy_version 66705 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:21:58,429][03976] Updated weights for policy 0, policy_version 66715 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:00,101][03976] Updated weights for policy 0, policy_version 66725 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:22:01,168][03423] Fps is (10 sec: 49971.2, 60 sec: 49561.5, 300 sec: 49013.1). Total num frames: 526655488. Throughput: 0: 12368.5. Samples: 19154432. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:22:01,169][03423] Avg episode reward: [(0, '57.060')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:01,755][03976] Updated weights for policy 0, policy_version 66735 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:03,400][03976] Updated weights for policy 0, policy_version 66745 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:05,024][03976] Updated weights for policy 0, policy_version 66755 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:22:06,168][03423] Fps is (10 sec: 49151.8, 60 sec: 49425.1, 300 sec: 49013.2). Total num frames: 526901248. Throughput: 0: 12377.0. Samples: 19191856. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:22:06,168][03423] Avg episode reward: [(0, '54.472')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:06,212][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000066762_526909440.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:22:06,278][03956] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000065320_515096576.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:06,683][03976] Updated weights for policy 0, policy_version 66765 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:08,336][03976] Updated weights for policy 0, policy_version 66775 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:09,994][03976] Updated weights for policy 0, policy_version 66785 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:22:11,167][03423] Fps is (10 sec: 49971.6, 60 sec: 49561.7, 300 sec: 49068.7). Total num frames: 527155200. Throughput: 0: 12373.1. Samples: 19265876. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:22:11,168][03423] Avg episode reward: [(0, '55.776')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:11,655][03976] Updated weights for policy 0, policy_version 66795 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:13,362][03976] Updated weights for policy 0, policy_version 66805 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:15,014][03976] Updated weights for policy 0, policy_version 66815 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:22:16,168][03423] Fps is (10 sec: 49971.0, 60 sec: 49561.7, 300 sec: 49068.7). Total num frames: 527400960. Throughput: 0: 12377.5. Samples: 19340088. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:22:16,169][03423] Avg episode reward: [(0, '55.700')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:16,653][03976] Updated weights for policy 0, policy_version 66825 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:18,274][03976] Updated weights for policy 0, policy_version 66835 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:19,948][03976] Updated weights for policy 0, policy_version 66845 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:22:21,168][03423] Fps is (10 sec: 49151.6, 60 sec: 49561.6, 300 sec: 49096.5). Total num frames: 527646720. Throughput: 0: 12383.5. Samples: 19377356. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:22:21,169][03423] Avg episode reward: [(0, '56.970')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:21,575][03976] Updated weights for policy 0, policy_version 66855 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:23,223][03976] Updated weights for policy 0, policy_version 66865 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:24,899][03976] Updated weights for policy 0, policy_version 66875 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:22:26,168][03423] Fps is (10 sec: 49151.5, 60 sec: 49425.0, 300 sec: 49096.4). Total num frames: 527892480. Throughput: 0: 12380.1. Samples: 19451780. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:22:26,169][03423] Avg episode reward: [(0, '54.279')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:26,549][03976] Updated weights for policy 0, policy_version 66885 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:28,224][03976] Updated weights for policy 0, policy_version 66895 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:29,853][03976] Updated weights for policy 0, policy_version 66905 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:22:31,168][03423] Fps is (10 sec: 49152.0, 60 sec: 49425.0, 300 sec: 49124.3). Total num frames: 528138240. Throughput: 0: 12390.0. Samples: 19526056. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:22:31,168][03423] Avg episode reward: [(0, '56.708')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:31,528][03976] Updated weights for policy 0, policy_version 66915 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:33,157][03976] Updated weights for policy 0, policy_version 66925 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:34,840][03976] Updated weights for policy 0, policy_version 66935 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:22:36,168][03423] Fps is (10 sec: 49152.5, 60 sec: 49425.0, 300 sec: 49124.2). Total num frames: 528384000. Throughput: 0: 12383.8. Samples: 19563112. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:22:36,169][03423] Avg episode reward: [(0, '56.961')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:36,525][03976] Updated weights for policy 0, policy_version 66945 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:38,183][03976] Updated weights for policy 0, policy_version 66955 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:39,815][03976] Updated weights for policy 0, policy_version 66965 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:22:41,168][03423] Fps is (10 sec: 49971.3, 60 sec: 49561.6, 300 sec: 49179.8). Total num frames: 528637952. Throughput: 0: 12379.5. Samples: 19637040. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:22:41,168][03423] Avg episode reward: [(0, '56.673')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:41,481][03976] Updated weights for policy 0, policy_version 66975 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:43,132][03976] Updated weights for policy 0, policy_version 66985 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:44,783][03976] Updated weights for policy 0, policy_version 66995 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:22:46,168][03423] Fps is (10 sec: 49971.2, 60 sec: 49561.5, 300 sec: 49207.5). Total num frames: 528883712. Throughput: 0: 12365.6. Samples: 19710884. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:22:46,168][03423] Avg episode reward: [(0, '54.285')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:46,451][03976] Updated weights for policy 0, policy_version 67005 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:48,182][03976] Updated weights for policy 0, policy_version 67015 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:49,828][03976] Updated weights for policy 0, policy_version 67025 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:22:51,168][03423] Fps is (10 sec: 49151.6, 60 sec: 49561.6, 300 sec: 49207.5). Total num frames: 529129472. Throughput: 0: 12344.5. Samples: 19747360. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:22:51,169][03423] Avg episode reward: [(0, '54.995')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:51,502][03976] Updated weights for policy 0, policy_version 67035 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:53,178][03976] Updated weights for policy 0, policy_version 67045 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:54,773][03976] Updated weights for policy 0, policy_version 67055 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:22:56,168][03423] Fps is (10 sec: 49151.8, 60 sec: 49425.0, 300 sec: 49235.3). Total num frames: 529375232. Throughput: 0: 12341.2. Samples: 19821232. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:22:56,169][03423] Avg episode reward: [(0, '55.664')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:56,525][03976] Updated weights for policy 0, policy_version 67065 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:58,258][03976] Updated weights for policy 0, policy_version 67075 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:22:59,965][03976] Updated weights for policy 0, policy_version 67085 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:23:01,168][03423] Fps is (10 sec: 47513.9, 60 sec: 49152.0, 300 sec: 49179.8). Total num frames: 529604608. Throughput: 0: 12279.7. Samples: 19892676. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:23:01,168][03423] Avg episode reward: [(0, '55.793')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:01,693][03976] Updated weights for policy 0, policy_version 67095 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:03,410][03976] Updated weights for policy 0, policy_version 67105 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:05,136][03976] Updated weights for policy 0, policy_version 67115 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:23:06,168][03423] Fps is (10 sec: 47513.5, 60 sec: 49151.9, 300 sec: 49207.5). Total num frames: 529850368. Throughput: 0: 12258.9. Samples: 19929008. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:23:06,169][03423] Avg episode reward: [(0, '52.719')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:06,854][03976] Updated weights for policy 0, policy_version 67125 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:08,601][03976] Updated weights for policy 0, policy_version 67135 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:10,309][03976] Updated weights for policy 0, policy_version 67145 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:23:11,167][03423] Fps is (10 sec: 48333.2, 60 sec: 48878.9, 300 sec: 49207.5). Total num frames: 530087936. Throughput: 0: 12177.7. Samples: 19999776. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:23:11,168][03423] Avg episode reward: [(0, '54.391')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:12,019][03976] Updated weights for policy 0, policy_version 67155 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:13,761][03976] Updated weights for policy 0, policy_version 67165 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:15,468][03976] Updated weights for policy 0, policy_version 67175 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:23:16,168][03423] Fps is (10 sec: 47514.0, 60 sec: 48742.4, 300 sec: 49179.8). Total num frames: 530325504. Throughput: 0: 12118.8. Samples: 20071400. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:23:16,169][03423] Avg episode reward: [(0, '54.320')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:17,185][03976] Updated weights for policy 0, policy_version 67185 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:18,869][03976] Updated weights for policy 0, policy_version 67195 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:20,592][03976] Updated weights for policy 0, policy_version 67205 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:23:21,168][03423] Fps is (10 sec: 47513.2, 60 sec: 48605.9, 300 sec: 49179.8). Total num frames: 530563072. Throughput: 0: 12096.5. Samples: 20107456. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:23:21,169][03423] Avg episode reward: [(0, '52.604')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:22,288][03976] Updated weights for policy 0, policy_version 67215 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:24,021][03976] Updated weights for policy 0, policy_version 67225 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:25,728][03976] Updated weights for policy 0, policy_version 67235 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:23:26,168][03423] Fps is (10 sec: 47513.6, 60 sec: 48469.5, 300 sec: 49152.0). Total num frames: 530800640. Throughput: 0: 12043.4. Samples: 20178992. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:23:26,168][03423] Avg episode reward: [(0, '53.274')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:27,429][03976] Updated weights for policy 0, policy_version 67245 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:29,185][03976] Updated weights for policy 0, policy_version 67255 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:30,916][03976] Updated weights for policy 0, policy_version 67265 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:23:31,167][03423] Fps is (10 sec: 47513.8, 60 sec: 48332.8, 300 sec: 49152.0). Total num frames: 531038208. Throughput: 0: 11993.6. Samples: 20250596. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:23:31,168][03423] Avg episode reward: [(0, '54.034')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:32,612][03976] Updated weights for policy 0, policy_version 67275 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:34,308][03976] Updated weights for policy 0, policy_version 67285 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:36,009][03976] Updated weights for policy 0, policy_version 67295 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:23:36,168][03423] Fps is (10 sec: 48332.8, 60 sec: 48332.8, 300 sec: 49179.8). Total num frames: 531283968. Throughput: 0: 11991.8. Samples: 20286992. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:23:36,168][03423] Avg episode reward: [(0, '54.614')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:37,720][03976] Updated weights for policy 0, policy_version 67305 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:39,407][03976] Updated weights for policy 0, policy_version 67315 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:41,128][03976] Updated weights for policy 0, policy_version 67325 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:23:41,168][03423] Fps is (10 sec: 48332.5, 60 sec: 48059.7, 300 sec: 49152.0). Total num frames: 531521536. Throughput: 0: 11946.0. Samples: 20358800. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:23:41,169][03423] Avg episode reward: [(0, '54.001')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:42,886][03976] Updated weights for policy 0, policy_version 67335 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:44,678][03976] Updated weights for policy 0, policy_version 67345 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:23:46,167][03423] Fps is (10 sec: 46694.5, 60 sec: 47786.7, 300 sec: 49124.2). Total num frames: 531750912. Throughput: 0: 11913.8. Samples: 20428796. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:23:46,168][03423] Avg episode reward: [(0, '55.031')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:46,417][03976] Updated weights for policy 0, policy_version 67355 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:48,116][03976] Updated weights for policy 0, policy_version 67365 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:49,909][03976] Updated weights for policy 0, policy_version 67375 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:23:51,167][03423] Fps is (10 sec: 46694.8, 60 sec: 47650.2, 300 sec: 49124.2). Total num frames: 531988480. Throughput: 0: 11903.9. Samples: 20464680. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:23:51,168][03423] Avg episode reward: [(0, '53.823')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:51,597][03976] Updated weights for policy 0, policy_version 67385 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:53,325][03976] Updated weights for policy 0, policy_version 67395 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:55,030][03976] Updated weights for policy 0, policy_version 67405 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:23:56,168][03423] Fps is (10 sec: 47513.3, 60 sec: 47513.6, 300 sec: 49068.7). Total num frames: 532226048. Throughput: 0: 11913.7. Samples: 20535892. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:23:56,168][03423] Avg episode reward: [(0, '53.954')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:56,710][03976] Updated weights for policy 0, policy_version 67415 (0.0013)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:23:58,434][03976] Updated weights for policy 0, policy_version 67425 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:00,163][03976] Updated weights for policy 0, policy_version 67435 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:24:01,168][03423] Fps is (10 sec: 48332.5, 60 sec: 47786.7, 300 sec: 49068.7). Total num frames: 532471808. Throughput: 0: 11916.4. Samples: 20607636. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:24:01,169][03423] Avg episode reward: [(0, '54.498')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:01,841][03976] Updated weights for policy 0, policy_version 67445 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:03,591][03976] Updated weights for policy 0, policy_version 67455 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:05,321][03976] Updated weights for policy 0, policy_version 67465 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:24:06,168][03423] Fps is (10 sec: 48333.0, 60 sec: 47650.2, 300 sec: 49040.9). Total num frames: 532709376. Throughput: 0: 11914.8. Samples: 20643620. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:24:06,169][03423] Avg episode reward: [(0, '55.733')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:06,173][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000067470_532709376.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:24:06,241][03956] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000066037_520970240.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:07,018][03976] Updated weights for policy 0, policy_version 67475 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:08,747][03976] Updated weights for policy 0, policy_version 67485 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:10,477][03976] Updated weights for policy 0, policy_version 67495 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:24:11,168][03423] Fps is (10 sec: 47513.7, 60 sec: 47650.1, 300 sec: 49013.2). Total num frames: 532946944. Throughput: 0: 11915.4. Samples: 20715184. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:24:11,169][03423] Avg episode reward: [(0, '54.761')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:12,138][03976] Updated weights for policy 0, policy_version 67505 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:13,932][03976] Updated weights for policy 0, policy_version 67515 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:15,721][03976] Updated weights for policy 0, policy_version 67525 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:24:16,168][03423] Fps is (10 sec: 46694.1, 60 sec: 47513.6, 300 sec: 48929.9). Total num frames: 533176320. Throughput: 0: 11881.6. Samples: 20785268. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:24:16,168][03423] Avg episode reward: [(0, '55.234')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:17,463][03976] Updated weights for policy 0, policy_version 67535 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:19,219][03976] Updated weights for policy 0, policy_version 67545 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:20,917][03976] Updated weights for policy 0, policy_version 67555 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:24:21,168][03423] Fps is (10 sec: 46694.5, 60 sec: 47513.6, 300 sec: 48902.1). Total num frames: 533413888. Throughput: 0: 11862.5. Samples: 20820804. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:24:21,169][03423] Avg episode reward: [(0, '55.338')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:22,540][03976] Updated weights for policy 0, policy_version 67565 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:24,210][03976] Updated weights for policy 0, policy_version 67575 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:25,883][03976] Updated weights for policy 0, policy_version 67585 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:24:26,168][03423] Fps is (10 sec: 48332.8, 60 sec: 47650.1, 300 sec: 48902.1). Total num frames: 533659648. Throughput: 0: 11885.0. Samples: 20893624. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:24:26,169][03423] Avg episode reward: [(0, '56.732')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:27,510][03976] Updated weights for policy 0, policy_version 67595 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:29,187][03976] Updated weights for policy 0, policy_version 67605 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:30,858][03976] Updated weights for policy 0, policy_version 67615 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:24:31,167][03423] Fps is (10 sec: 49152.3, 60 sec: 47786.7, 300 sec: 48902.1). Total num frames: 533905408. Throughput: 0: 11972.5. Samples: 20967556. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:24:31,168][03423] Avg episode reward: [(0, '56.516')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:32,548][03976] Updated weights for policy 0, policy_version 67625 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:34,195][03976] Updated weights for policy 0, policy_version 67635 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:35,875][03976] Updated weights for policy 0, policy_version 67645 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:24:36,168][03423] Fps is (10 sec: 49152.3, 60 sec: 47786.7, 300 sec: 48902.1). Total num frames: 534151168. Throughput: 0: 11997.6. Samples: 21004572. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:24:36,168][03423] Avg episode reward: [(0, '55.632')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:37,520][03976] Updated weights for policy 0, policy_version 67655 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:39,219][03976] Updated weights for policy 0, policy_version 67665 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:40,929][03976] Updated weights for policy 0, policy_version 67675 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:24:41,168][03423] Fps is (10 sec: 49151.7, 60 sec: 47923.2, 300 sec: 48902.1). Total num frames: 534396928. Throughput: 0: 12047.0. Samples: 21078008. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:24:41,169][03423] Avg episode reward: [(0, '54.661')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:42,626][03976] Updated weights for policy 0, policy_version 67685 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:44,375][03976] Updated weights for policy 0, policy_version 67695 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:46,078][03976] Updated weights for policy 0, policy_version 67705 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:24:46,168][03423] Fps is (10 sec: 48332.8, 60 sec: 48059.7, 300 sec: 48846.6). Total num frames: 534634496. Throughput: 0: 12038.5. Samples: 21149368. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:24:46,169][03423] Avg episode reward: [(0, '55.673')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:47,764][03976] Updated weights for policy 0, policy_version 67715 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:49,561][03976] Updated weights for policy 0, policy_version 67725 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:24:51,168][03423] Fps is (10 sec: 47513.6, 60 sec: 48059.7, 300 sec: 48818.8). Total num frames: 534872064. Throughput: 0: 12037.0. Samples: 21185284. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:24:51,169][03423] Avg episode reward: [(0, '54.066')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:51,307][03976] Updated weights for policy 0, policy_version 67735 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:53,012][03976] Updated weights for policy 0, policy_version 67745 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:54,746][03976] Updated weights for policy 0, policy_version 67755 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:24:56,168][03423] Fps is (10 sec: 47513.6, 60 sec: 48059.8, 300 sec: 48791.0). Total num frames: 535109632. Throughput: 0: 12023.1. Samples: 21256224. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:24:56,168][03423] Avg episode reward: [(0, '54.368')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:56,421][03976] Updated weights for policy 0, policy_version 67765 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:58,157][03976] Updated weights for policy 0, policy_version 67775 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:24:59,871][03976] Updated weights for policy 0, policy_version 67785 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:25:01,168][03423] Fps is (10 sec: 47513.6, 60 sec: 47923.2, 300 sec: 48735.5). Total num frames: 535347200. Throughput: 0: 12059.7. Samples: 21327956. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:25:01,169][03423] Avg episode reward: [(0, '55.496')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:01,567][03976] Updated weights for policy 0, policy_version 67795 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:03,291][03976] Updated weights for policy 0, policy_version 67805 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:04,990][03976] Updated weights for policy 0, policy_version 67815 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:25:06,168][03423] Fps is (10 sec: 47513.3, 60 sec: 47923.1, 300 sec: 48707.7). Total num frames: 535584768. Throughput: 0: 12066.0. Samples: 21363776. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:25:06,169][03423] Avg episode reward: [(0, '55.863')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:06,674][03976] Updated weights for policy 0, policy_version 67825 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:08,372][03976] Updated weights for policy 0, policy_version 67835 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:10,094][03976] Updated weights for policy 0, policy_version 67845 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:25:11,167][03423] Fps is (10 sec: 48333.0, 60 sec: 48059.8, 300 sec: 48707.7). Total num frames: 535830528. Throughput: 0: 12051.4. Samples: 21435936. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:25:11,169][03423] Avg episode reward: [(0, '56.822')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:11,854][03976] Updated weights for policy 0, policy_version 67855 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:13,563][03976] Updated weights for policy 0, policy_version 67865 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:15,275][03976] Updated weights for policy 0, policy_version 67875 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:25:16,168][03423] Fps is (10 sec: 48333.0, 60 sec: 48196.3, 300 sec: 48679.9). Total num frames: 536068096. Throughput: 0: 12000.9. Samples: 21507596. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:25:16,168][03423] Avg episode reward: [(0, '54.833')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:17,005][03976] Updated weights for policy 0, policy_version 67885 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:18,708][03976] Updated weights for policy 0, policy_version 67895 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:20,418][03976] Updated weights for policy 0, policy_version 67905 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:25:21,168][03423] Fps is (10 sec: 47513.2, 60 sec: 48196.2, 300 sec: 48652.1). Total num frames: 536305664. Throughput: 0: 11974.6. Samples: 21543428. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:25:21,169][03423] Avg episode reward: [(0, '56.318')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:22,092][03976] Updated weights for policy 0, policy_version 67915 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:23,791][03976] Updated weights for policy 0, policy_version 67925 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:25,519][03976] Updated weights for policy 0, policy_version 67935 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:25:26,168][03423] Fps is (10 sec: 47513.5, 60 sec: 48059.8, 300 sec: 48624.4). Total num frames: 536543232. Throughput: 0: 11938.0. Samples: 21615220. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:25:26,168][03423] Avg episode reward: [(0, '54.006')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:27,220][03976] Updated weights for policy 0, policy_version 67945 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:28,947][03976] Updated weights for policy 0, policy_version 67955 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:30,678][03976] Updated weights for policy 0, policy_version 67965 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:25:31,167][03423] Fps is (10 sec: 47514.2, 60 sec: 47923.2, 300 sec: 48568.9). Total num frames: 536780800. Throughput: 0: 11944.2. Samples: 21686856. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:25:31,168][03423] Avg episode reward: [(0, '56.192')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:32,367][03976] Updated weights for policy 0, policy_version 67975 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:34,078][03976] Updated weights for policy 0, policy_version 67985 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:35,780][03976] Updated weights for policy 0, policy_version 67995 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:25:36,168][03423] Fps is (10 sec: 48332.8, 60 sec: 47923.2, 300 sec: 48568.8). Total num frames: 537026560. Throughput: 0: 11951.3. Samples: 21723092. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:25:36,169][03423] Avg episode reward: [(0, '55.047')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:37,465][03976] Updated weights for policy 0, policy_version 68005 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:39,181][03976] Updated weights for policy 0, policy_version 68015 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:40,913][03976] Updated weights for policy 0, policy_version 68025 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:25:41,167][03423] Fps is (10 sec: 48332.6, 60 sec: 47786.7, 300 sec: 48541.1). Total num frames: 537264128. Throughput: 0: 11976.9. Samples: 21795184. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:25:41,168][03423] Avg episode reward: [(0, '55.353')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:42,616][03976] Updated weights for policy 0, policy_version 68035 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:44,302][03976] Updated weights for policy 0, policy_version 68045 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:46,038][03976] Updated weights for policy 0, policy_version 68055 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:25:46,167][03423] Fps is (10 sec: 47514.0, 60 sec: 47786.7, 300 sec: 48513.3). Total num frames: 537501696. Throughput: 0: 11974.1. Samples: 21866792. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:25:46,168][03423] Avg episode reward: [(0, '56.248')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:47,717][03976] Updated weights for policy 0, policy_version 68065 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:49,412][03976] Updated weights for policy 0, policy_version 68075 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:51,137][03976] Updated weights for policy 0, policy_version 68085 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:25:51,168][03423] Fps is (10 sec: 48332.5, 60 sec: 47923.2, 300 sec: 48513.3). Total num frames: 537747456. Throughput: 0: 11983.8. Samples: 21903048. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:25:51,168][03423] Avg episode reward: [(0, '54.817')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:52,861][03976] Updated weights for policy 0, policy_version 68095 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:54,554][03976] Updated weights for policy 0, policy_version 68105 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:25:56,167][03423] Fps is (10 sec: 48332.7, 60 sec: 47923.2, 300 sec: 48485.5). Total num frames: 537985024. Throughput: 0: 11982.9. Samples: 21975168. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:25:56,169][03423] Avg episode reward: [(0, '55.256')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:56,276][03976] Updated weights for policy 0, policy_version 68115 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:57,997][03976] Updated weights for policy 0, policy_version 68125 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:25:59,668][03976] Updated weights for policy 0, policy_version 68135 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:26:01,168][03423] Fps is (10 sec: 47513.4, 60 sec: 47923.1, 300 sec: 48430.0). Total num frames: 538222592. Throughput: 0: 11989.7. Samples: 22047132. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:26:01,169][03423] Avg episode reward: [(0, '52.506')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:01,380][03976] Updated weights for policy 0, policy_version 68145 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:03,084][03976] Updated weights for policy 0, policy_version 68155 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:04,906][03976] Updated weights for policy 0, policy_version 68165 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:26:06,167][03423] Fps is (10 sec: 47513.6, 60 sec: 47923.3, 300 sec: 48402.2). Total num frames: 538460160. Throughput: 0: 11981.7. Samples: 22082604. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:26:06,169][03423] Avg episode reward: [(0, '54.164')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:06,173][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000068172_538460160.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:26:06,249][03956] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000066762_526909440.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:06,781][03976] Updated weights for policy 0, policy_version 68175 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:08,537][03976] Updated weights for policy 0, policy_version 68185 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:10,262][03976] Updated weights for policy 0, policy_version 68195 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:26:11,168][03423] Fps is (10 sec: 46694.6, 60 sec: 47650.1, 300 sec: 48346.7). Total num frames: 538689536. Throughput: 0: 11914.4. Samples: 22151368. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:26:11,169][03423] Avg episode reward: [(0, '53.707')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:11,969][03976] Updated weights for policy 0, policy_version 68205 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:13,760][03976] Updated weights for policy 0, policy_version 68215 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:15,845][03976] Updated weights for policy 0, policy_version 68225 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:26:16,168][03423] Fps is (10 sec: 44236.6, 60 sec: 47240.5, 300 sec: 48235.6). Total num frames: 538902528. Throughput: 0: 11796.5. Samples: 22217700. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:26:16,169][03423] Avg episode reward: [(0, '56.379')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:17,697][03976] Updated weights for policy 0, policy_version 68235 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:19,630][03976] Updated weights for policy 0, policy_version 68245 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:26:21,168][03423] Fps is (10 sec: 43417.5, 60 sec: 46967.5, 300 sec: 48124.5). Total num frames: 539123712. Throughput: 0: 11724.1. Samples: 22250676. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:26:21,169][03423] Avg episode reward: [(0, '55.464')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:21,521][03976] Updated weights for policy 0, policy_version 68255 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:23,493][03976] Updated weights for policy 0, policy_version 68265 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:25,412][03976] Updated weights for policy 0, policy_version 68275 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:26:26,167][03423] Fps is (10 sec: 43418.3, 60 sec: 46558.0, 300 sec: 48013.5). Total num frames: 539336704. Throughput: 0: 11533.0. Samples: 22314168. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:26:26,168][03423] Avg episode reward: [(0, '54.309')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:27,172][03976] Updated weights for policy 0, policy_version 68285 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:28,889][03976] Updated weights for policy 0, policy_version 68295 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:30,573][03976] Updated weights for policy 0, policy_version 68305 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:26:31,167][03423] Fps is (10 sec: 45056.4, 60 sec: 46557.8, 300 sec: 47985.7). Total num frames: 539574272. Throughput: 0: 11492.5. Samples: 22383956. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:26:31,168][03423] Avg episode reward: [(0, '54.048')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:32,292][03976] Updated weights for policy 0, policy_version 68315 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:34,020][03976] Updated weights for policy 0, policy_version 68325 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:35,755][03976] Updated weights for policy 0, policy_version 68335 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:26:36,168][03423] Fps is (10 sec: 47512.9, 60 sec: 46421.3, 300 sec: 47957.9). Total num frames: 539811840. Throughput: 0: 11481.0. Samples: 22419692. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:26:36,169][03423] Avg episode reward: [(0, '56.287')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:37,512][03976] Updated weights for policy 0, policy_version 68345 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:39,330][03976] Updated weights for policy 0, policy_version 68355 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:41,088][03976] Updated weights for policy 0, policy_version 68365 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:26:41,167][03423] Fps is (10 sec: 46694.3, 60 sec: 46284.8, 300 sec: 47902.4). Total num frames: 540041216. Throughput: 0: 11424.0. Samples: 22489248. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:26:41,168][03423] Avg episode reward: [(0, '57.197')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:42,983][03976] Updated weights for policy 0, policy_version 68375 (0.0014)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:44,757][03976] Updated weights for policy 0, policy_version 68385 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:26:46,167][03423] Fps is (10 sec: 45875.4, 60 sec: 46148.3, 300 sec: 47846.8). Total num frames: 540270592. Throughput: 0: 11354.3. Samples: 22558076. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:26:46,168][03423] Avg episode reward: [(0, '56.295')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:46,475][03976] Updated weights for policy 0, policy_version 68395 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:48,327][03976] Updated weights for policy 0, policy_version 68405 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:50,130][03976] Updated weights for policy 0, policy_version 68415 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:26:51,168][03423] Fps is (10 sec: 45055.9, 60 sec: 45738.7, 300 sec: 47735.8). Total num frames: 540491776. Throughput: 0: 11316.5. Samples: 22591848. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:26:51,168][03423] Avg episode reward: [(0, '53.432')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:52,007][03976] Updated weights for policy 0, policy_version 68425 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:53,912][03976] Updated weights for policy 0, policy_version 68435 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:55,869][03976] Updated weights for policy 0, policy_version 68445 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:26:56,168][03423] Fps is (10 sec: 43417.2, 60 sec: 45329.0, 300 sec: 47624.7). Total num frames: 540704768. Throughput: 0: 11256.6. Samples: 22657916. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:26:56,169][03423] Avg episode reward: [(0, '54.352')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:57,599][03976] Updated weights for policy 0, policy_version 68455 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:26:59,484][03976] Updated weights for policy 0, policy_version 68465 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:27:01,167][03423] Fps is (10 sec: 44237.3, 60 sec: 45192.7, 300 sec: 47569.2). Total num frames: 540934144. Throughput: 0: 11267.9. Samples: 22724752. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:27:01,168][03423] Avg episode reward: [(0, '56.262')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:01,335][03976] Updated weights for policy 0, policy_version 68475 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:03,053][03976] Updated weights for policy 0, policy_version 68485 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:05,125][03976] Updated weights for policy 0, policy_version 68495 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:27:06,168][03423] Fps is (10 sec: 45055.9, 60 sec: 44919.4, 300 sec: 47458.0). Total num frames: 541155328. Throughput: 0: 11286.1. Samples: 22758552. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:27:06,169][03423] Avg episode reward: [(0, '55.896')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:06,849][03976] Updated weights for policy 0, policy_version 68505 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:08,800][03976] Updated weights for policy 0, policy_version 68515 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:10,871][03976] Updated weights for policy 0, policy_version 68525 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:27:11,167][03423] Fps is (10 sec: 42598.1, 60 sec: 44509.9, 300 sec: 47319.2). Total num frames: 541360128. Throughput: 0: 11303.2. Samples: 22822812. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:27:11,169][03423] Avg episode reward: [(0, '55.856')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:12,724][03976] Updated weights for policy 0, policy_version 68535 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:14,633][03976] Updated weights for policy 0, policy_version 68545 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:27:16,167][03423] Fps is (10 sec: 41779.6, 60 sec: 44509.9, 300 sec: 47208.1). Total num frames: 541573120. Throughput: 0: 11153.1. Samples: 22885844. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:27:16,168][03423] Avg episode reward: [(0, '55.563')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:16,549][03976] Updated weights for policy 0, policy_version 68555 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:18,292][03976] Updated weights for policy 0, policy_version 68565 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:20,415][03976] Updated weights for policy 0, policy_version 68575 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:27:21,168][03423] Fps is (10 sec: 43417.4, 60 sec: 44509.9, 300 sec: 47124.8). Total num frames: 541794304. Throughput: 0: 11116.5. Samples: 22919936. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:27:21,168][03423] Avg episode reward: [(0, '54.945')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:22,351][03976] Updated weights for policy 0, policy_version 68585 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:24,062][03976] Updated weights for policy 0, policy_version 68595 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:25,817][03976] Updated weights for policy 0, policy_version 68605 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:27:26,168][03423] Fps is (10 sec: 45055.9, 60 sec: 44782.8, 300 sec: 47069.3). Total num frames: 542023680. Throughput: 0: 11012.3. Samples: 22984800. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:27:26,169][03423] Avg episode reward: [(0, '56.474')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:27,590][03976] Updated weights for policy 0, policy_version 68615 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:29,387][03976] Updated weights for policy 0, policy_version 68625 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:27:31,168][03423] Fps is (10 sec: 45055.7, 60 sec: 44509.8, 300 sec: 46986.0). Total num frames: 542244864. Throughput: 0: 11003.3. Samples: 23053228. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:27:31,169][03423] Avg episode reward: [(0, '54.863')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:31,304][03976] Updated weights for policy 0, policy_version 68635 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:33,375][03976] Updated weights for policy 0, policy_version 68645 (0.0013)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:35,859][03976] Updated weights for policy 0, policy_version 68655 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:27:36,168][03423] Fps is (10 sec: 40140.5, 60 sec: 43554.1, 300 sec: 46736.0). Total num frames: 542425088. Throughput: 0: 10923.0. Samples: 23083384. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:27:36,169][03423] Avg episode reward: [(0, '54.023')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:37,640][03976] Updated weights for policy 0, policy_version 68665 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:39,347][03976] Updated weights for policy 0, policy_version 68675 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:41,051][03976] Updated weights for policy 0, policy_version 68685 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:27:41,167][03423] Fps is (10 sec: 41779.7, 60 sec: 43690.7, 300 sec: 46708.3). Total num frames: 542662656. Throughput: 0: 10816.1. Samples: 23144640. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:27:41,168][03423] Avg episode reward: [(0, '53.049')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:42,979][03976] Updated weights for policy 0, policy_version 68695 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:44,732][03976] Updated weights for policy 0, policy_version 68705 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:27:46,170][03423] Fps is (10 sec: 46681.5, 60 sec: 43688.6, 300 sec: 46652.3). Total num frames: 542892032. Throughput: 0: 10849.5. Samples: 23213012. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:27:46,171][03423] Avg episode reward: [(0, '56.057')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:46,528][03976] Updated weights for policy 0, policy_version 68715 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:48,262][03976] Updated weights for policy 0, policy_version 68725 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:50,066][03976] Updated weights for policy 0, policy_version 68735 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:27:51,168][03423] Fps is (10 sec: 45874.4, 60 sec: 43827.1, 300 sec: 46597.2). Total num frames: 543121408. Throughput: 0: 10885.0. Samples: 23248376. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:27:51,169][03423] Avg episode reward: [(0, '54.837')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:51,744][03976] Updated weights for policy 0, policy_version 68745 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:53,502][03976] Updated weights for policy 0, policy_version 68755 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:55,185][03976] Updated weights for policy 0, policy_version 68765 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:27:56,168][03423] Fps is (10 sec: 46707.5, 60 sec: 44236.8, 300 sec: 46625.0). Total num frames: 543358976. Throughput: 0: 11028.0. Samples: 23319072. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:27:56,169][03423] Avg episode reward: [(0, '55.424')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:57,053][03976] Updated weights for policy 0, policy_version 68775 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:27:58,984][03976] Updated weights for policy 0, policy_version 68785 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:00,838][03976] Updated weights for policy 0, policy_version 68795 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:28:01,168][03423] Fps is (10 sec: 45056.4, 60 sec: 43963.6, 300 sec: 46513.9). Total num frames: 543571968. Throughput: 0: 11104.3. Samples: 23385540. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:28:01,169][03423] Avg episode reward: [(0, '57.057')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:02,654][03976] Updated weights for policy 0, policy_version 68805 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:04,630][03976] Updated weights for policy 0, policy_version 68815 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:28:06,168][03423] Fps is (10 sec: 43417.6, 60 sec: 43963.8, 300 sec: 46458.3). Total num frames: 543793152. Throughput: 0: 11088.4. Samples: 23418912. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:28:06,169][03423] Avg episode reward: [(0, '54.419')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:06,173][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000068823_543793152.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:28:06,245][03956] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000067470_532709376.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:06,510][03976] Updated weights for policy 0, policy_version 68825 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:08,359][03976] Updated weights for policy 0, policy_version 68835 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:10,210][03976] Updated weights for policy 0, policy_version 68845 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:28:11,168][03423] Fps is (10 sec: 44236.3, 60 sec: 44236.7, 300 sec: 46402.8). Total num frames: 544014336. Throughput: 0: 11070.5. Samples: 23482976. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:28:11,169][03423] Avg episode reward: [(0, '53.533')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:11,945][03976] Updated weights for policy 0, policy_version 68855 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:13,721][03976] Updated weights for policy 0, policy_version 68865 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:15,436][03976] Updated weights for policy 0, policy_version 68875 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:28:16,168][03423] Fps is (10 sec: 45874.9, 60 sec: 44646.3, 300 sec: 46402.8). Total num frames: 544251904. Throughput: 0: 11114.8. Samples: 23553396. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:28:16,169][03423] Avg episode reward: [(0, '55.332')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:17,163][03976] Updated weights for policy 0, policy_version 68885 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:18,957][03976] Updated weights for policy 0, policy_version 68895 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:20,657][03976] Updated weights for policy 0, policy_version 68905 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:28:21,167][03423] Fps is (10 sec: 46695.3, 60 sec: 44783.0, 300 sec: 46375.1). Total num frames: 544481280. Throughput: 0: 11218.2. Samples: 23588204. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:28:21,168][03423] Avg episode reward: [(0, '54.872')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:22,418][03976] Updated weights for policy 0, policy_version 68915 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:24,197][03976] Updated weights for policy 0, policy_version 68925 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:25,953][03976] Updated weights for policy 0, policy_version 68935 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:28:26,168][03423] Fps is (10 sec: 46694.6, 60 sec: 44919.4, 300 sec: 46375.0). Total num frames: 544718848. Throughput: 0: 11424.1. Samples: 23658724. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:28:26,169][03423] Avg episode reward: [(0, '55.391')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:27,675][03976] Updated weights for policy 0, policy_version 68945 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:29,401][03976] Updated weights for policy 0, policy_version 68955 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:28:31,167][03423] Fps is (10 sec: 47513.9, 60 sec: 45192.7, 300 sec: 46347.3). Total num frames: 544956416. Throughput: 0: 11471.7. Samples: 23729204. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:31,168][03976] Updated weights for policy 0, policy_version 68965 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:28:31,169][03423] Avg episode reward: [(0, '55.822')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:32,882][03976] Updated weights for policy 0, policy_version 68975 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:34,604][03976] Updated weights for policy 0, policy_version 68985 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:28:36,167][03423] Fps is (10 sec: 46694.7, 60 sec: 46011.8, 300 sec: 46319.5). Total num frames: 545185792. Throughput: 0: 11475.4. Samples: 23764768. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:28:36,168][03423] Avg episode reward: [(0, '56.523')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:36,335][03976] Updated weights for policy 0, policy_version 68995 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:38,119][03976] Updated weights for policy 0, policy_version 69005 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:39,837][03976] Updated weights for policy 0, policy_version 69015 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:28:41,167][03423] Fps is (10 sec: 46694.6, 60 sec: 46011.8, 300 sec: 46347.3). Total num frames: 545423360. Throughput: 0: 11475.9. Samples: 23835484. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:28:41,168][03423] Avg episode reward: [(0, '54.881')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:41,525][03976] Updated weights for policy 0, policy_version 69025 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:43,302][03976] Updated weights for policy 0, policy_version 69035 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:45,038][03976] Updated weights for policy 0, policy_version 69045 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:28:46,168][03423] Fps is (10 sec: 47512.8, 60 sec: 46150.3, 300 sec: 46347.3). Total num frames: 545660928. Throughput: 0: 11582.9. Samples: 23906772. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:28:46,169][03423] Avg episode reward: [(0, '55.764')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:46,758][03976] Updated weights for policy 0, policy_version 69055 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:48,460][03976] Updated weights for policy 0, policy_version 69065 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:50,135][03976] Updated weights for policy 0, policy_version 69075 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:28:51,167][03423] Fps is (10 sec: 48332.2, 60 sec: 46421.5, 300 sec: 46375.1). Total num frames: 545906688. Throughput: 0: 11638.6. Samples: 23942648. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:28:51,168][03423] Avg episode reward: [(0, '52.821')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:51,841][03976] Updated weights for policy 0, policy_version 69085 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:53,604][03976] Updated weights for policy 0, policy_version 69095 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:55,333][03976] Updated weights for policy 0, policy_version 69105 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:28:56,167][03423] Fps is (10 sec: 48333.5, 60 sec: 46421.4, 300 sec: 46347.3). Total num frames: 546144256. Throughput: 0: 11802.4. Samples: 24014084. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:28:56,169][03423] Avg episode reward: [(0, '53.889')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:57,110][03976] Updated weights for policy 0, policy_version 69115 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:28:58,850][03976] Updated weights for policy 0, policy_version 69125 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:00,644][03976] Updated weights for policy 0, policy_version 69135 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:29:01,168][03423] Fps is (10 sec: 46694.3, 60 sec: 46694.4, 300 sec: 46319.5). Total num frames: 546373632. Throughput: 0: 11799.0. Samples: 24084352. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:29:01,169][03423] Avg episode reward: [(0, '54.327')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:02,362][03976] Updated weights for policy 0, policy_version 69145 (0.0015)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:04,090][03976] Updated weights for policy 0, policy_version 69155 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:05,870][03976] Updated weights for policy 0, policy_version 69165 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:29:06,167][03423] Fps is (10 sec: 45875.3, 60 sec: 46831.0, 300 sec: 46291.7). Total num frames: 546603008. Throughput: 0: 11800.8. Samples: 24119240. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:29:06,169][03423] Avg episode reward: [(0, '55.924')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:07,583][03976] Updated weights for policy 0, policy_version 69175 (0.0013)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:09,354][03976] Updated weights for policy 0, policy_version 69185 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:11,010][03976] Updated weights for policy 0, policy_version 69195 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:29:11,168][03423] Fps is (10 sec: 46694.3, 60 sec: 47104.1, 300 sec: 46319.5). Total num frames: 546840576. Throughput: 0: 11795.6. Samples: 24189524. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:29:11,168][03423] Avg episode reward: [(0, '56.601')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:12,793][03976] Updated weights for policy 0, policy_version 69205 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:14,741][03976] Updated weights for policy 0, policy_version 69215 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:29:16,168][03423] Fps is (10 sec: 46694.3, 60 sec: 46967.5, 300 sec: 46291.7). Total num frames: 547069952. Throughput: 0: 11745.0. Samples: 24257732. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:29:16,169][03423] Avg episode reward: [(0, '56.641')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:16,505][03976] Updated weights for policy 0, policy_version 69225 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:18,249][03976] Updated weights for policy 0, policy_version 69235 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:19,944][03976] Updated weights for policy 0, policy_version 69245 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:29:21,167][03423] Fps is (10 sec: 46694.8, 60 sec: 47104.0, 300 sec: 46264.0). Total num frames: 547307520. Throughput: 0: 11745.3. Samples: 24293304. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:29:21,168][03423] Avg episode reward: [(0, '54.081')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:21,688][03976] Updated weights for policy 0, policy_version 69255 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:23,424][03976] Updated weights for policy 0, policy_version 69265 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:25,176][03976] Updated weights for policy 0, policy_version 69275 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:29:26,168][03423] Fps is (10 sec: 46694.2, 60 sec: 46967.5, 300 sec: 46208.4). Total num frames: 547536896. Throughput: 0: 11755.0. Samples: 24364460. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:29:26,169][03423] Avg episode reward: [(0, '55.838')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:26,930][03976] Updated weights for policy 0, policy_version 69285 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:28,667][03976] Updated weights for policy 0, policy_version 69295 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:30,362][03976] Updated weights for policy 0, policy_version 69305 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:29:31,168][03423] Fps is (10 sec: 46693.9, 60 sec: 46967.4, 300 sec: 46180.7). Total num frames: 547774464. Throughput: 0: 11741.3. Samples: 24435128. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:29:31,168][03423] Avg episode reward: [(0, '53.265')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:32,074][03976] Updated weights for policy 0, policy_version 69315 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:33,779][03976] Updated weights for policy 0, policy_version 69325 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:35,491][03976] Updated weights for policy 0, policy_version 69335 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:29:36,168][03423] Fps is (10 sec: 47513.8, 60 sec: 47104.0, 300 sec: 46152.9). Total num frames: 548012032. Throughput: 0: 11738.4. Samples: 24470876. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:29:36,168][03423] Avg episode reward: [(0, '55.666')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:37,252][03976] Updated weights for policy 0, policy_version 69345 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:38,982][03976] Updated weights for policy 0, policy_version 69355 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:40,656][03976] Updated weights for policy 0, policy_version 69365 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:29:41,168][03423] Fps is (10 sec: 47513.5, 60 sec: 47103.8, 300 sec: 46152.9). Total num frames: 548249600. Throughput: 0: 11718.6. Samples: 24541424. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:29:41,169][03423] Avg episode reward: [(0, '55.380')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:42,413][03976] Updated weights for policy 0, policy_version 69375 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:44,137][03976] Updated weights for policy 0, policy_version 69385 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:45,852][03976] Updated weights for policy 0, policy_version 69395 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:29:46,168][03423] Fps is (10 sec: 47513.6, 60 sec: 47104.1, 300 sec: 46152.9). Total num frames: 548487168. Throughput: 0: 11750.0. Samples: 24613104. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:29:46,169][03423] Avg episode reward: [(0, '57.042')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:47,610][03976] Updated weights for policy 0, policy_version 69405 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:49,346][03976] Updated weights for policy 0, policy_version 69415 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:51,100][03976] Updated weights for policy 0, policy_version 69425 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:29:51,168][03423] Fps is (10 sec: 47512.7, 60 sec: 46967.3, 300 sec: 46152.9). Total num frames: 548724736. Throughput: 0: 11764.0. Samples: 24648624. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:29:51,172][03423] Avg episode reward: [(0, '53.635')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:52,846][03976] Updated weights for policy 0, policy_version 69435 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:54,589][03976] Updated weights for policy 0, policy_version 69445 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:29:56,168][03423] Fps is (10 sec: 47513.6, 60 sec: 46967.5, 300 sec: 46152.9). Total num frames: 548962304. Throughput: 0: 11772.2. Samples: 24719272. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:29:56,169][03423] Avg episode reward: [(0, '54.925')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:56,313][03976] Updated weights for policy 0, policy_version 69455 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:58,027][03976] Updated weights for policy 0, policy_version 69465 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:29:59,752][03976] Updated weights for policy 0, policy_version 69475 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:30:01,167][03423] Fps is (10 sec: 46695.9, 60 sec: 46967.5, 300 sec: 46125.1). Total num frames: 549191680. Throughput: 0: 11805.5. Samples: 24788980. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:30:01,168][03423] Avg episode reward: [(0, '55.405')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:01,553][03976] Updated weights for policy 0, policy_version 69485 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:03,353][03976] Updated weights for policy 0, policy_version 69495 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:05,117][03976] Updated weights for policy 0, policy_version 69505 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:30:06,168][03423] Fps is (10 sec: 45874.6, 60 sec: 46967.4, 300 sec: 46069.6). Total num frames: 549421056. Throughput: 0: 11783.2. Samples: 24823548. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:30:06,169][03423] Avg episode reward: [(0, '56.285')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:06,220][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000069511_549429248.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:30:06,292][03956] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000068172_538460160.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:06,918][03976] Updated weights for policy 0, policy_version 69515 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:08,651][03976] Updated weights for policy 0, policy_version 69525 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:10,352][03976] Updated weights for policy 0, policy_version 69535 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:30:11,167][03423] Fps is (10 sec: 46694.2, 60 sec: 46967.5, 300 sec: 46069.6). Total num frames: 549658624. Throughput: 0: 11771.3. Samples: 24894168. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:30:11,168][03423] Avg episode reward: [(0, '55.550')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:12,057][03976] Updated weights for policy 0, policy_version 69545 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:13,782][03976] Updated weights for policy 0, policy_version 69555 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:15,439][03976] Updated weights for policy 0, policy_version 69565 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:30:16,167][03423] Fps is (10 sec: 48333.4, 60 sec: 47240.5, 300 sec: 46097.4). Total num frames: 549904384. Throughput: 0: 11798.7. Samples: 24966068. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:30:16,169][03423] Avg episode reward: [(0, '54.870')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:17,170][03976] Updated weights for policy 0, policy_version 69575 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:18,929][03976] Updated weights for policy 0, policy_version 69585 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:20,629][03976] Updated weights for policy 0, policy_version 69595 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:30:21,168][03423] Fps is (10 sec: 48332.8, 60 sec: 47240.5, 300 sec: 46097.4). Total num frames: 550141952. Throughput: 0: 11807.2. Samples: 25002200. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:30:21,169][03423] Avg episode reward: [(0, '54.771')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:22,339][03976] Updated weights for policy 0, policy_version 69605 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:24,046][03976] Updated weights for policy 0, policy_version 69615 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:25,719][03976] Updated weights for policy 0, policy_version 69625 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:30:26,168][03423] Fps is (10 sec: 47513.3, 60 sec: 47377.1, 300 sec: 46097.3). Total num frames: 550379520. Throughput: 0: 11833.1. Samples: 25073912. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:30:26,169][03423] Avg episode reward: [(0, '54.838')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:27,440][03976] Updated weights for policy 0, policy_version 69635 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:29,149][03976] Updated weights for policy 0, policy_version 69645 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:30,886][03976] Updated weights for policy 0, policy_version 69655 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:30:31,168][03423] Fps is (10 sec: 47513.2, 60 sec: 47377.0, 300 sec: 46069.6). Total num frames: 550617088. Throughput: 0: 11835.6. Samples: 25145708. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:30:31,169][03423] Avg episode reward: [(0, '56.047')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:32,617][03976] Updated weights for policy 0, policy_version 69665 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:34,324][03976] Updated weights for policy 0, policy_version 69675 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:36,058][03976] Updated weights for policy 0, policy_version 69685 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:30:36,168][03423] Fps is (10 sec: 47513.3, 60 sec: 47377.0, 300 sec: 46069.6). Total num frames: 550854656. Throughput: 0: 11831.4. Samples: 25181036. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:30:36,169][03423] Avg episode reward: [(0, '55.709')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:37,777][03976] Updated weights for policy 0, policy_version 69695 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:39,499][03976] Updated weights for policy 0, policy_version 69705 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:30:41,168][03423] Fps is (10 sec: 47513.5, 60 sec: 47377.0, 300 sec: 46069.6). Total num frames: 551092224. Throughput: 0: 11850.7. Samples: 25252556. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:30:41,177][03423] Avg episode reward: [(0, '54.682')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:41,236][03976] Updated weights for policy 0, policy_version 69715 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:42,936][03976] Updated weights for policy 0, policy_version 69725 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:44,604][03976] Updated weights for policy 0, policy_version 69735 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:30:46,168][03423] Fps is (10 sec: 48333.3, 60 sec: 47513.6, 300 sec: 46069.6). Total num frames: 551337984. Throughput: 0: 11905.2. Samples: 25324716. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:30:46,168][03423] Avg episode reward: [(0, '56.456')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:46,315][03976] Updated weights for policy 0, policy_version 69745 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:48,052][03976] Updated weights for policy 0, policy_version 69755 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:49,761][03976] Updated weights for policy 0, policy_version 69765 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:30:51,168][03423] Fps is (10 sec: 48333.2, 60 sec: 47513.8, 300 sec: 46069.6). Total num frames: 551575552. Throughput: 0: 11935.7. Samples: 25360652. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:30:51,168][03423] Avg episode reward: [(0, '54.104')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:51,482][03976] Updated weights for policy 0, policy_version 69775 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:53,180][03976] Updated weights for policy 0, policy_version 69785 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:54,887][03976] Updated weights for policy 0, policy_version 69795 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:30:56,168][03423] Fps is (10 sec: 47513.6, 60 sec: 47513.6, 300 sec: 46069.6). Total num frames: 551813120. Throughput: 0: 11959.4. Samples: 25432340. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:30:56,169][03423] Avg episode reward: [(0, '56.044')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:56,617][03976] Updated weights for policy 0, policy_version 69805 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:30:58,308][03976] Updated weights for policy 0, policy_version 69815 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:00,051][03976] Updated weights for policy 0, policy_version 69825 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:31:01,168][03423] Fps is (10 sec: 47513.6, 60 sec: 47650.1, 300 sec: 46069.6). Total num frames: 552050688. Throughput: 0: 11947.9. Samples: 25503724. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:31:01,168][03423] Avg episode reward: [(0, '55.153')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:01,745][03976] Updated weights for policy 0, policy_version 69835 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:03,453][03976] Updated weights for policy 0, policy_version 69845 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:05,167][03976] Updated weights for policy 0, policy_version 69855 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:31:06,168][03423] Fps is (10 sec: 48332.0, 60 sec: 47923.2, 300 sec: 46125.1). Total num frames: 552296448. Throughput: 0: 11942.2. Samples: 25539600. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:31:06,169][03423] Avg episode reward: [(0, '54.799')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:06,904][03976] Updated weights for policy 0, policy_version 69865 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:08,626][03976] Updated weights for policy 0, policy_version 69875 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:10,300][03976] Updated weights for policy 0, policy_version 69885 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:31:11,168][03423] Fps is (10 sec: 47513.4, 60 sec: 47786.6, 300 sec: 46180.7). Total num frames: 552525824. Throughput: 0: 11944.2. Samples: 25611400. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:31:11,177][03423] Avg episode reward: [(0, '56.256')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:11,963][03976] Updated weights for policy 0, policy_version 69895 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:13,679][03976] Updated weights for policy 0, policy_version 69905 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:15,400][03976] Updated weights for policy 0, policy_version 69915 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:31:16,167][03423] Fps is (10 sec: 47514.7, 60 sec: 47786.7, 300 sec: 46264.0). Total num frames: 552771584. Throughput: 0: 11950.8. Samples: 25683492. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:31:16,169][03423] Avg episode reward: [(0, '53.371')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:17,152][03976] Updated weights for policy 0, policy_version 69925 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:18,855][03976] Updated weights for policy 0, policy_version 69935 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:20,550][03976] Updated weights for policy 0, policy_version 69945 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:31:21,168][03423] Fps is (10 sec: 48332.8, 60 sec: 47786.6, 300 sec: 46347.3). Total num frames: 553009152. Throughput: 0: 11962.2. Samples: 25719332. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:31:21,169][03423] Avg episode reward: [(0, '52.658')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:22,302][03976] Updated weights for policy 0, policy_version 69955 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:24,003][03976] Updated weights for policy 0, policy_version 69965 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:25,702][03976] Updated weights for policy 0, policy_version 69975 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:31:26,168][03423] Fps is (10 sec: 47513.1, 60 sec: 47786.7, 300 sec: 46347.3). Total num frames: 553246720. Throughput: 0: 11967.0. Samples: 25791072. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:31:26,169][03423] Avg episode reward: [(0, '55.008')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:27,381][03976] Updated weights for policy 0, policy_version 69985 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:29,077][03976] Updated weights for policy 0, policy_version 69995 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:30,784][03976] Updated weights for policy 0, policy_version 70005 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:31:31,168][03423] Fps is (10 sec: 48332.9, 60 sec: 47923.2, 300 sec: 46375.1). Total num frames: 553492480. Throughput: 0: 11972.5. Samples: 25863480. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:31:31,169][03423] Avg episode reward: [(0, '56.528')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:32,482][03976] Updated weights for policy 0, policy_version 70015 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:34,192][03976] Updated weights for policy 0, policy_version 70025 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:35,914][03976] Updated weights for policy 0, policy_version 70035 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:31:36,168][03423] Fps is (10 sec: 48333.0, 60 sec: 47923.3, 300 sec: 46402.8). Total num frames: 553730048. Throughput: 0: 11975.2. Samples: 25899536. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:31:36,169][03423] Avg episode reward: [(0, '55.881')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:37,590][03976] Updated weights for policy 0, policy_version 70045 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:39,326][03976] Updated weights for policy 0, policy_version 70055 (0.0014)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:41,034][03976] Updated weights for policy 0, policy_version 70065 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:31:41,167][03423] Fps is (10 sec: 48333.0, 60 sec: 48059.8, 300 sec: 46458.4). Total num frames: 553975808. Throughput: 0: 11980.9. Samples: 25971480. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:31:41,169][03423] Avg episode reward: [(0, '52.700')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:42,760][03976] Updated weights for policy 0, policy_version 70075 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:44,455][03976] Updated weights for policy 0, policy_version 70085 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:46,137][03976] Updated weights for policy 0, policy_version 70095 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:31:46,168][03423] Fps is (10 sec: 48332.9, 60 sec: 47923.2, 300 sec: 46513.9). Total num frames: 554213376. Throughput: 0: 11989.1. Samples: 26043232. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:31:46,169][03423] Avg episode reward: [(0, '55.701')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:47,878][03976] Updated weights for policy 0, policy_version 70105 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:49,575][03976] Updated weights for policy 0, policy_version 70115 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:31:51,168][03423] Fps is (10 sec: 47513.4, 60 sec: 47923.2, 300 sec: 46597.2). Total num frames: 554450944. Throughput: 0: 11994.3. Samples: 26079340. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:31:51,168][03423] Avg episode reward: [(0, '54.751')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:51,273][03976] Updated weights for policy 0, policy_version 70125 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:53,040][03976] Updated weights for policy 0, policy_version 70135 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:54,740][03976] Updated weights for policy 0, policy_version 70145 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:31:56,168][03423] Fps is (10 sec: 47513.2, 60 sec: 47923.1, 300 sec: 46624.9). Total num frames: 554688512. Throughput: 0: 11984.9. Samples: 26150720. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:31:56,169][03423] Avg episode reward: [(0, '54.932')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:56,465][03976] Updated weights for policy 0, policy_version 70155 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:58,156][03976] Updated weights for policy 0, policy_version 70165 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:31:59,890][03976] Updated weights for policy 0, policy_version 70175 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:32:01,168][03423] Fps is (10 sec: 47513.7, 60 sec: 47923.2, 300 sec: 46680.5). Total num frames: 554926080. Throughput: 0: 11972.8. Samples: 26222268. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:32:01,169][03423] Avg episode reward: [(0, '56.965')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:01,630][03976] Updated weights for policy 0, policy_version 70185 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:03,356][03976] Updated weights for policy 0, policy_version 70195 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:05,024][03976] Updated weights for policy 0, policy_version 70205 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:32:06,167][03423] Fps is (10 sec: 47514.1, 60 sec: 47786.8, 300 sec: 46791.6). Total num frames: 555163648. Throughput: 0: 11972.7. Samples: 26258104. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:32:06,168][03423] Avg episode reward: [(0, '55.513')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:06,172][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000070211_555163648.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:32:06,242][03956] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000068823_543793152.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:06,809][03976] Updated weights for policy 0, policy_version 70215 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:08,479][03976] Updated weights for policy 0, policy_version 70225 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:10,193][03976] Updated weights for policy 0, policy_version 70235 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:32:11,168][03423] Fps is (10 sec: 47513.4, 60 sec: 47923.2, 300 sec: 46874.9). Total num frames: 555401216. Throughput: 0: 11963.7. Samples: 26329440. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:32:11,169][03423] Avg episode reward: [(0, '53.737')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:11,905][03976] Updated weights for policy 0, policy_version 70245 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:13,619][03976] Updated weights for policy 0, policy_version 70255 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:15,328][03976] Updated weights for policy 0, policy_version 70265 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:32:16,167][03423] Fps is (10 sec: 48332.8, 60 sec: 47923.2, 300 sec: 46958.2). Total num frames: 555646976. Throughput: 0: 11956.5. Samples: 26401524. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:32:16,168][03423] Avg episode reward: [(0, '52.701')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:17,017][03976] Updated weights for policy 0, policy_version 70275 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:18,752][03976] Updated weights for policy 0, policy_version 70285 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:20,450][03976] Updated weights for policy 0, policy_version 70295 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:32:21,167][03423] Fps is (10 sec: 48333.0, 60 sec: 47923.2, 300 sec: 46986.0). Total num frames: 555884544. Throughput: 0: 11955.4. Samples: 26437528. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:32:21,169][03423] Avg episode reward: [(0, '54.390')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:22,134][03976] Updated weights for policy 0, policy_version 70305 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:23,844][03976] Updated weights for policy 0, policy_version 70315 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:25,555][03976] Updated weights for policy 0, policy_version 70325 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:32:26,168][03423] Fps is (10 sec: 47513.4, 60 sec: 47923.2, 300 sec: 47041.5). Total num frames: 556122112. Throughput: 0: 11955.9. Samples: 26509496. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:32:26,169][03423] Avg episode reward: [(0, '56.005')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:27,319][03976] Updated weights for policy 0, policy_version 70335 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:28,993][03976] Updated weights for policy 0, policy_version 70345 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:30,733][03976] Updated weights for policy 0, policy_version 70355 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:32:31,168][03423] Fps is (10 sec: 47513.5, 60 sec: 47786.7, 300 sec: 47235.9). Total num frames: 556359680. Throughput: 0: 11956.0. Samples: 26581252. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:32:31,169][03423] Avg episode reward: [(0, '53.848')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:32,434][03976] Updated weights for policy 0, policy_version 70365 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:34,099][03976] Updated weights for policy 0, policy_version 70375 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:35,835][03976] Updated weights for policy 0, policy_version 70385 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:32:36,168][03423] Fps is (10 sec: 48332.5, 60 sec: 47923.2, 300 sec: 47263.7). Total num frames: 556605440. Throughput: 0: 11952.1. Samples: 26617184. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:32:36,169][03423] Avg episode reward: [(0, '56.502')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:37,520][03976] Updated weights for policy 0, policy_version 70395 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:39,282][03976] Updated weights for policy 0, policy_version 70405 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:41,019][03976] Updated weights for policy 0, policy_version 70415 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:32:41,168][03423] Fps is (10 sec: 48332.4, 60 sec: 47786.6, 300 sec: 47291.9). Total num frames: 556843008. Throughput: 0: 11960.9. Samples: 26688960. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:32:41,169][03423] Avg episode reward: [(0, '56.257')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:42,714][03976] Updated weights for policy 0, policy_version 70425 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:44,418][03976] Updated weights for policy 0, policy_version 70435 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:46,118][03976] Updated weights for policy 0, policy_version 70445 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:32:46,168][03423] Fps is (10 sec: 47513.8, 60 sec: 47786.6, 300 sec: 47319.2). Total num frames: 557080576. Throughput: 0: 11966.3. Samples: 26760752. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:32:46,168][03423] Avg episode reward: [(0, '52.241')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:47,864][03976] Updated weights for policy 0, policy_version 70455 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:49,562][03976] Updated weights for policy 0, policy_version 70465 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:32:51,168][03423] Fps is (10 sec: 47513.6, 60 sec: 47786.6, 300 sec: 47319.2). Total num frames: 557318144. Throughput: 0: 11960.4. Samples: 26796324. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:32:51,169][03423] Avg episode reward: [(0, '56.953')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:51,270][03976] Updated weights for policy 0, policy_version 70475 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:52,972][03976] Updated weights for policy 0, policy_version 70485 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:54,629][03976] Updated weights for policy 0, policy_version 70495 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:32:56,168][03423] Fps is (10 sec: 47513.6, 60 sec: 47786.7, 300 sec: 47402.5). Total num frames: 557555712. Throughput: 0: 11971.8. Samples: 26868172. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:32:56,169][03423] Avg episode reward: [(0, '55.075')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:56,373][03976] Updated weights for policy 0, policy_version 70505 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:58,120][03976] Updated weights for policy 0, policy_version 70515 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:32:59,851][03976] Updated weights for policy 0, policy_version 70525 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:33:01,167][03423] Fps is (10 sec: 47514.1, 60 sec: 47786.7, 300 sec: 47458.1). Total num frames: 557793280. Throughput: 0: 11962.1. Samples: 26939820. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:33:01,168][03423] Avg episode reward: [(0, '56.120')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:01,545][03976] Updated weights for policy 0, policy_version 70535 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:03,257][03976] Updated weights for policy 0, policy_version 70545 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:04,965][03976] Updated weights for policy 0, policy_version 70555 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:33:06,168][03423] Fps is (10 sec: 48332.9, 60 sec: 47923.2, 300 sec: 47541.4). Total num frames: 558039040. Throughput: 0: 11962.1. Samples: 26975824. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:33:06,169][03423] Avg episode reward: [(0, '55.913')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:06,686][03976] Updated weights for policy 0, policy_version 70565 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:08,407][03976] Updated weights for policy 0, policy_version 70575 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:10,120][03976] Updated weights for policy 0, policy_version 70585 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:33:11,167][03423] Fps is (10 sec: 48332.8, 60 sec: 47923.2, 300 sec: 47541.4). Total num frames: 558276608. Throughput: 0: 11961.3. Samples: 27047756. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:33:11,169][03423] Avg episode reward: [(0, '54.648')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:11,765][03976] Updated weights for policy 0, policy_version 70595 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:13,511][03976] Updated weights for policy 0, policy_version 70605 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:15,215][03976] Updated weights for policy 0, policy_version 70615 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:33:16,168][03423] Fps is (10 sec: 47513.4, 60 sec: 47786.6, 300 sec: 47569.1). Total num frames: 558514176. Throughput: 0: 11964.1. Samples: 27119636. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:33:16,169][03423] Avg episode reward: [(0, '54.929')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:16,975][03976] Updated weights for policy 0, policy_version 70625 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:18,949][03976] Updated weights for policy 0, policy_version 70635 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:20,847][03976] Updated weights for policy 0, policy_version 70645 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:33:21,167][03423] Fps is (10 sec: 45056.0, 60 sec: 47377.1, 300 sec: 47485.8). Total num frames: 558727168. Throughput: 0: 11894.8. Samples: 27152448. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:33:21,168][03423] Avg episode reward: [(0, '54.321')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:22,598][03976] Updated weights for policy 0, policy_version 70655 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:24,265][03976] Updated weights for policy 0, policy_version 70665 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:25,957][03976] Updated weights for policy 0, policy_version 70675 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:33:26,167][03423] Fps is (10 sec: 45875.5, 60 sec: 47513.6, 300 sec: 47513.6). Total num frames: 558972928. Throughput: 0: 11837.8. Samples: 27221660. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:33:26,169][03423] Avg episode reward: [(0, '51.532')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:27,831][03976] Updated weights for policy 0, policy_version 70685 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:29,541][03976] Updated weights for policy 0, policy_version 70695 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:33:31,167][03423] Fps is (10 sec: 47513.6, 60 sec: 47377.1, 300 sec: 47513.6). Total num frames: 559202304. Throughput: 0: 11800.5. Samples: 27291776. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:33:31,168][03423] Avg episode reward: [(0, '55.684')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:31,186][03976] Updated weights for policy 0, policy_version 70705 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:32,969][03976] Updated weights for policy 0, policy_version 70715 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:34,656][03976] Updated weights for policy 0, policy_version 70725 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:33:36,167][03423] Fps is (10 sec: 47513.8, 60 sec: 47377.2, 300 sec: 47541.4). Total num frames: 559448064. Throughput: 0: 11808.3. Samples: 27327696. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:33:36,168][03423] Avg episode reward: [(0, '55.616')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:36,363][03976] Updated weights for policy 0, policy_version 70735 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:38,091][03976] Updated weights for policy 0, policy_version 70745 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:39,786][03976] Updated weights for policy 0, policy_version 70755 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:33:41,168][03423] Fps is (10 sec: 48332.5, 60 sec: 47377.1, 300 sec: 47541.4). Total num frames: 559685632. Throughput: 0: 11799.8. Samples: 27399164. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:33:41,169][03423] Avg episode reward: [(0, '54.948')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:41,457][03976] Updated weights for policy 0, policy_version 70765 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:43,107][03976] Updated weights for policy 0, policy_version 70775 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:44,810][03976] Updated weights for policy 0, policy_version 70785 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:33:46,168][03423] Fps is (10 sec: 48331.8, 60 sec: 47513.5, 300 sec: 47541.3). Total num frames: 559931392. Throughput: 0: 11836.3. Samples: 27472456. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:33:46,169][03423] Avg episode reward: [(0, '53.606')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:46,488][03976] Updated weights for policy 0, policy_version 70795 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:48,157][03976] Updated weights for policy 0, policy_version 70805 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:49,839][03976] Updated weights for policy 0, policy_version 70815 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:33:51,168][03423] Fps is (10 sec: 49152.1, 60 sec: 47650.2, 300 sec: 47569.1). Total num frames: 560177152. Throughput: 0: 11851.6. Samples: 27509144. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:33:51,169][03423] Avg episode reward: [(0, '56.558')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:51,487][03976] Updated weights for policy 0, policy_version 70825 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:53,176][03976] Updated weights for policy 0, policy_version 70835 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:54,872][03976] Updated weights for policy 0, policy_version 70845 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:33:56,168][03423] Fps is (10 sec: 48333.6, 60 sec: 47650.2, 300 sec: 47596.9). Total num frames: 560414720. Throughput: 0: 11888.0. Samples: 27582716. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:33:56,169][03423] Avg episode reward: [(0, '53.361')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:56,558][03976] Updated weights for policy 0, policy_version 70855 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:33:58,337][03976] Updated weights for policy 0, policy_version 70865 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:00,056][03976] Updated weights for policy 0, policy_version 70875 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:34:01,167][03423] Fps is (10 sec: 47514.0, 60 sec: 47650.2, 300 sec: 47624.7). Total num frames: 560652288. Throughput: 0: 11883.1. Samples: 27654372. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:34:01,168][03423] Avg episode reward: [(0, '56.799')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:01,732][03976] Updated weights for policy 0, policy_version 70885 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:03,409][03976] Updated weights for policy 0, policy_version 70895 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:05,111][03976] Updated weights for policy 0, policy_version 70905 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:34:06,167][03423] Fps is (10 sec: 48332.8, 60 sec: 47650.2, 300 sec: 47652.5). Total num frames: 560898048. Throughput: 0: 11962.1. Samples: 27690744. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:34:06,168][03423] Avg episode reward: [(0, '53.825')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:06,173][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000070911_560898048.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:34:06,238][03956] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000069511_549429248.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:06,837][03976] Updated weights for policy 0, policy_version 70915 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:08,502][03976] Updated weights for policy 0, policy_version 70925 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:10,198][03976] Updated weights for policy 0, policy_version 70935 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:34:11,168][03423] Fps is (10 sec: 48332.1, 60 sec: 47650.1, 300 sec: 47680.2). Total num frames: 561135616. Throughput: 0: 12029.8. Samples: 27763004. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:34:11,169][03423] Avg episode reward: [(0, '57.814')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:11,877][03976] Updated weights for policy 0, policy_version 70945 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:13,568][03976] Updated weights for policy 0, policy_version 70955 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:15,328][03976] Updated weights for policy 0, policy_version 70965 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:34:16,167][03423] Fps is (10 sec: 47513.7, 60 sec: 47650.2, 300 sec: 47680.2). Total num frames: 561373184. Throughput: 0: 12069.2. Samples: 27834888. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:34:16,168][03423] Avg episode reward: [(0, '55.814')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:17,005][03976] Updated weights for policy 0, policy_version 70975 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:18,673][03976] Updated weights for policy 0, policy_version 70985 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:20,398][03976] Updated weights for policy 0, policy_version 70995 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:34:21,167][03423] Fps is (10 sec: 48333.4, 60 sec: 48196.3, 300 sec: 47735.8). Total num frames: 561618944. Throughput: 0: 12088.9. Samples: 27871696. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:34:21,169][03423] Avg episode reward: [(0, '57.007')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:22,112][03976] Updated weights for policy 0, policy_version 71005 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:23,802][03976] Updated weights for policy 0, policy_version 71015 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:25,494][03976] Updated weights for policy 0, policy_version 71025 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:34:26,168][03423] Fps is (10 sec: 48332.5, 60 sec: 48059.7, 300 sec: 47735.8). Total num frames: 561856512. Throughput: 0: 12106.4. Samples: 27943952. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:34:26,168][03423] Avg episode reward: [(0, '54.505')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:27,189][03976] Updated weights for policy 0, policy_version 71035 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:28,870][03976] Updated weights for policy 0, policy_version 71045 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:30,561][03976] Updated weights for policy 0, policy_version 71055 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:34:31,168][03423] Fps is (10 sec: 48332.4, 60 sec: 48332.8, 300 sec: 47763.5). Total num frames: 562102272. Throughput: 0: 12091.1. Samples: 28016552. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:34:31,168][03423] Avg episode reward: [(0, '52.948')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:32,263][03976] Updated weights for policy 0, policy_version 71065 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:33,956][03976] Updated weights for policy 0, policy_version 71075 (0.0013)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:35,642][03976] Updated weights for policy 0, policy_version 71085 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:34:36,168][03423] Fps is (10 sec: 48332.9, 60 sec: 48196.2, 300 sec: 47763.5). Total num frames: 562339840. Throughput: 0: 12087.2. Samples: 28053068. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:34:36,169][03423] Avg episode reward: [(0, '55.590')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:37,335][03976] Updated weights for policy 0, policy_version 71095 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:39,128][03976] Updated weights for policy 0, policy_version 71105 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:40,918][03976] Updated weights for policy 0, policy_version 71115 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:34:41,168][03423] Fps is (10 sec: 47513.5, 60 sec: 48196.3, 300 sec: 47763.5). Total num frames: 562577408. Throughput: 0: 12021.8. Samples: 28123696. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:34:41,169][03423] Avg episode reward: [(0, '55.365')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:42,556][03976] Updated weights for policy 0, policy_version 71125 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:44,245][03976] Updated weights for policy 0, policy_version 71135 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:45,943][03976] Updated weights for policy 0, policy_version 71145 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:34:46,168][03423] Fps is (10 sec: 48332.8, 60 sec: 48196.4, 300 sec: 47791.3). Total num frames: 562823168. Throughput: 0: 12050.4. Samples: 28196640. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:34:46,169][03423] Avg episode reward: [(0, '57.498')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:47,715][03976] Updated weights for policy 0, policy_version 71155 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:49,365][03976] Updated weights for policy 0, policy_version 71165 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:51,068][03976] Updated weights for policy 0, policy_version 71175 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:34:51,167][03423] Fps is (10 sec: 48333.2, 60 sec: 48059.8, 300 sec: 47791.3). Total num frames: 563060736. Throughput: 0: 12039.6. Samples: 28232524. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:34:51,169][03423] Avg episode reward: [(0, '54.979')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:52,741][03976] Updated weights for policy 0, policy_version 71185 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:54,405][03976] Updated weights for policy 0, policy_version 71195 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:56,158][03976] Updated weights for policy 0, policy_version 71205 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:34:56,168][03423] Fps is (10 sec: 48332.1, 60 sec: 48196.1, 300 sec: 47846.8). Total num frames: 563306496. Throughput: 0: 12045.8. Samples: 28305064. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:34:56,169][03423] Avg episode reward: [(0, '56.899')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:57,843][03976] Updated weights for policy 0, policy_version 71215 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:34:59,589][03976] Updated weights for policy 0, policy_version 71225 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:35:01,168][03423] Fps is (10 sec: 48332.4, 60 sec: 48196.2, 300 sec: 47874.6). Total num frames: 563544064. Throughput: 0: 12021.8. Samples: 28375872. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:35:01,169][03423] Avg episode reward: [(0, '57.198')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:01,291][03976] Updated weights for policy 0, policy_version 71235 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:03,011][03976] Updated weights for policy 0, policy_version 71245 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:04,709][03976] Updated weights for policy 0, policy_version 71255 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:35:06,167][03423] Fps is (10 sec: 47514.5, 60 sec: 48059.7, 300 sec: 47874.6). Total num frames: 563781632. Throughput: 0: 12011.4. Samples: 28412208. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:35:06,168][03423] Avg episode reward: [(0, '55.400')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:06,411][03976] Updated weights for policy 0, policy_version 71265 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:08,261][03976] Updated weights for policy 0, policy_version 71275 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:09,976][03976] Updated weights for policy 0, policy_version 71285 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:35:11,167][03423] Fps is (10 sec: 47513.9, 60 sec: 48059.8, 300 sec: 47846.8). Total num frames: 564019200. Throughput: 0: 11973.6. Samples: 28482764. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:35:11,168][03423] Avg episode reward: [(0, '57.843')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:11,647][03976] Updated weights for policy 0, policy_version 71295 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:13,358][03976] Updated weights for policy 0, policy_version 71305 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:15,058][03976] Updated weights for policy 0, policy_version 71315 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:35:16,168][03423] Fps is (10 sec: 47512.9, 60 sec: 48059.6, 300 sec: 47846.8). Total num frames: 564256768. Throughput: 0: 11956.8. Samples: 28554608. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:35:16,169][03423] Avg episode reward: [(0, '54.354')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:16,756][03976] Updated weights for policy 0, policy_version 71325 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:18,486][03976] Updated weights for policy 0, policy_version 71335 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:20,155][03976] Updated weights for policy 0, policy_version 71345 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:35:21,168][03423] Fps is (10 sec: 47513.4, 60 sec: 47923.1, 300 sec: 47846.8). Total num frames: 564494336. Throughput: 0: 11957.0. Samples: 28591132. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:35:21,169][03423] Avg episode reward: [(0, '54.686')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:21,854][03976] Updated weights for policy 0, policy_version 71355 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:23,561][03976] Updated weights for policy 0, policy_version 71365 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:25,190][03976] Updated weights for policy 0, policy_version 71375 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:35:26,167][03423] Fps is (10 sec: 48333.8, 60 sec: 48059.8, 300 sec: 47874.6). Total num frames: 564740096. Throughput: 0: 12002.2. Samples: 28663792. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:35:26,169][03423] Avg episode reward: [(0, '56.864')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:26,867][03976] Updated weights for policy 0, policy_version 71385 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:28,513][03976] Updated weights for policy 0, policy_version 71395 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:30,235][03976] Updated weights for policy 0, policy_version 71405 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:35:31,168][03423] Fps is (10 sec: 49152.1, 60 sec: 48059.8, 300 sec: 47902.4). Total num frames: 564985856. Throughput: 0: 12023.2. Samples: 28737684. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:35:31,169][03423] Avg episode reward: [(0, '55.798')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:31,914][03976] Updated weights for policy 0, policy_version 71415 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:33,585][03976] Updated weights for policy 0, policy_version 71425 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:35,253][03976] Updated weights for policy 0, policy_version 71435 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:35:36,167][03423] Fps is (10 sec: 49151.6, 60 sec: 48196.3, 300 sec: 47930.2). Total num frames: 565231616. Throughput: 0: 12030.8. Samples: 28773912. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:35:36,168][03423] Avg episode reward: [(0, '54.055')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:37,039][03976] Updated weights for policy 0, policy_version 71445 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:38,737][03976] Updated weights for policy 0, policy_version 71455 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:40,450][03976] Updated weights for policy 0, policy_version 71465 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:35:41,168][03423] Fps is (10 sec: 48332.8, 60 sec: 48196.3, 300 sec: 47902.4). Total num frames: 565469184. Throughput: 0: 12022.4. Samples: 28846072. Policy #0 lag: (min: 0.0, avg: 1.1, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:35:41,169][03423] Avg episode reward: [(0, '55.582')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:42,197][03976] Updated weights for policy 0, policy_version 71475 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:43,887][03976] Updated weights for policy 0, policy_version 71485 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:45,597][03976] Updated weights for policy 0, policy_version 71495 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:35:46,168][03423] Fps is (10 sec: 47513.4, 60 sec: 48059.7, 300 sec: 47902.4). Total num frames: 565706752. Throughput: 0: 12026.5. Samples: 28917064. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:35:46,169][03423] Avg episode reward: [(0, '54.824')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:47,309][03976] Updated weights for policy 0, policy_version 71505 (0.0013)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:49,033][03976] Updated weights for policy 0, policy_version 71515 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:50,742][03976] Updated weights for policy 0, policy_version 71525 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:35:51,167][03423] Fps is (10 sec: 47513.8, 60 sec: 48059.7, 300 sec: 47902.4). Total num frames: 565944320. Throughput: 0: 12020.4. Samples: 28953124. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:35:51,168][03423] Avg episode reward: [(0, '54.377')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:52,448][03976] Updated weights for policy 0, policy_version 71535 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:54,111][03976] Updated weights for policy 0, policy_version 71545 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:55,840][03976] Updated weights for policy 0, policy_version 71555 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:35:56,167][03423] Fps is (10 sec: 48333.1, 60 sec: 48059.9, 300 sec: 47930.1). Total num frames: 566190080. Throughput: 0: 12066.9. Samples: 29025776. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:35:56,168][03423] Avg episode reward: [(0, '54.436')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:57,466][03976] Updated weights for policy 0, policy_version 71565 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:35:59,112][03976] Updated weights for policy 0, policy_version 71575 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:00,847][03976] Updated weights for policy 0, policy_version 71585 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:36:01,168][03423] Fps is (10 sec: 48332.5, 60 sec: 48059.7, 300 sec: 47902.4). Total num frames: 566427648. Throughput: 0: 12086.7. Samples: 29098508. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:36:01,169][03423] Avg episode reward: [(0, '56.687')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:02,507][03976] Updated weights for policy 0, policy_version 71595 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:04,187][03976] Updated weights for policy 0, policy_version 71605 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:05,857][03976] Updated weights for policy 0, policy_version 71615 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:36:06,167][03423] Fps is (10 sec: 48332.8, 60 sec: 48196.3, 300 sec: 47957.9). Total num frames: 566673408. Throughput: 0: 12087.1. Samples: 29135052. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:36:06,168][03423] Avg episode reward: [(0, '52.671')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:06,210][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000071617_566681600.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:36:06,275][03956] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000070211_555163648.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:07,608][03976] Updated weights for policy 0, policy_version 71625 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:09,266][03976] Updated weights for policy 0, policy_version 71635 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:10,964][03976] Updated weights for policy 0, policy_version 71645 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:36:11,171][03423] Fps is (10 sec: 49134.1, 60 sec: 48329.8, 300 sec: 47957.3). Total num frames: 566919168. Throughput: 0: 12090.7. Samples: 29207920. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:36:11,172][03423] Avg episode reward: [(0, '54.841')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:12,652][03976] Updated weights for policy 0, policy_version 71655 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:14,357][03976] Updated weights for policy 0, policy_version 71665 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:16,046][03976] Updated weights for policy 0, policy_version 71675 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:36:16,167][03423] Fps is (10 sec: 48332.8, 60 sec: 48332.9, 300 sec: 47957.9). Total num frames: 567156736. Throughput: 0: 12060.8. Samples: 29280420. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:36:16,168][03423] Avg episode reward: [(0, '55.416')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:17,726][03976] Updated weights for policy 0, policy_version 71685 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:19,415][03976] Updated weights for policy 0, policy_version 71695 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:21,071][03976] Updated weights for policy 0, policy_version 71705 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:36:21,168][03423] Fps is (10 sec: 48350.3, 60 sec: 48469.3, 300 sec: 47985.7). Total num frames: 567402496. Throughput: 0: 12066.0. Samples: 29316884. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:36:21,169][03423] Avg episode reward: [(0, '50.943')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:22,717][03976] Updated weights for policy 0, policy_version 71715 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:24,371][03976] Updated weights for policy 0, policy_version 71725 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:26,113][03976] Updated weights for policy 0, policy_version 71735 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:36:26,168][03423] Fps is (10 sec: 49151.7, 60 sec: 48469.2, 300 sec: 47985.7). Total num frames: 567648256. Throughput: 0: 12101.1. Samples: 29390624. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:36:26,168][03423] Avg episode reward: [(0, '53.337')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:27,867][03976] Updated weights for policy 0, policy_version 71745 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:29,553][03976] Updated weights for policy 0, policy_version 71755 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:36:31,168][03423] Fps is (10 sec: 48333.0, 60 sec: 48332.8, 300 sec: 47985.7). Total num frames: 567885824. Throughput: 0: 12121.2. Samples: 29462520. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:36:31,168][03423] Avg episode reward: [(0, '54.935')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:31,213][03976] Updated weights for policy 0, policy_version 71765 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:32,866][03976] Updated weights for policy 0, policy_version 71775 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:34,543][03976] Updated weights for policy 0, policy_version 71785 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:36:36,167][03423] Fps is (10 sec: 48333.0, 60 sec: 48332.8, 300 sec: 47985.7). Total num frames: 568131584. Throughput: 0: 12142.5. Samples: 29499536. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:36:36,169][03423] Avg episode reward: [(0, '54.192')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:36,222][03976] Updated weights for policy 0, policy_version 71795 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:37,892][03976] Updated weights for policy 0, policy_version 71805 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:39,613][03976] Updated weights for policy 0, policy_version 71815 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:36:41,168][03423] Fps is (10 sec: 49151.7, 60 sec: 48469.3, 300 sec: 48013.4). Total num frames: 568377344. Throughput: 0: 12144.9. Samples: 29572296. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:36:41,169][03423] Avg episode reward: [(0, '55.498')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:41,257][03976] Updated weights for policy 0, policy_version 71825 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:42,968][03976] Updated weights for policy 0, policy_version 71835 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:44,672][03976] Updated weights for policy 0, policy_version 71845 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:36:46,168][03423] Fps is (10 sec: 49151.4, 60 sec: 48605.8, 300 sec: 48041.2). Total num frames: 568623104. Throughput: 0: 12148.9. Samples: 29645208. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:36:46,169][03423] Avg episode reward: [(0, '55.229')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:46,334][03976] Updated weights for policy 0, policy_version 71855 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:48,021][03976] Updated weights for policy 0, policy_version 71865 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:49,683][03976] Updated weights for policy 0, policy_version 71875 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:36:51,168][03423] Fps is (10 sec: 49152.4, 60 sec: 48742.4, 300 sec: 48069.0). Total num frames: 568868864. Throughput: 0: 12158.8. Samples: 29682200. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:36:51,168][03423] Avg episode reward: [(0, '56.058')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:51,322][03976] Updated weights for policy 0, policy_version 71885 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:53,021][03976] Updated weights for policy 0, policy_version 71895 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:54,684][03976] Updated weights for policy 0, policy_version 71905 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:36:56,168][03423] Fps is (10 sec: 48333.1, 60 sec: 48605.8, 300 sec: 48069.0). Total num frames: 569106432. Throughput: 0: 12183.9. Samples: 29756152. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:36:56,169][03423] Avg episode reward: [(0, '55.063')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:56,382][03976] Updated weights for policy 0, policy_version 71915 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:58,066][03976] Updated weights for policy 0, policy_version 71925 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:36:59,741][03976] Updated weights for policy 0, policy_version 71935 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:37:01,168][03423] Fps is (10 sec: 48332.5, 60 sec: 48742.4, 300 sec: 48096.7). Total num frames: 569352192. Throughput: 0: 12183.2. Samples: 29828664. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:37:01,169][03423] Avg episode reward: [(0, '54.931')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:01,464][03976] Updated weights for policy 0, policy_version 71945 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:03,159][03976] Updated weights for policy 0, policy_version 71955 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:04,861][03976] Updated weights for policy 0, policy_version 71965 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:37:06,167][03423] Fps is (10 sec: 48333.2, 60 sec: 48605.9, 300 sec: 48096.8). Total num frames: 569589760. Throughput: 0: 12168.6. Samples: 29864468. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:37:06,168][03423] Avg episode reward: [(0, '54.630')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:06,508][03976] Updated weights for policy 0, policy_version 71975 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:08,148][03976] Updated weights for policy 0, policy_version 71985 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:09,841][03976] Updated weights for policy 0, policy_version 71995 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:37:11,168][03423] Fps is (10 sec: 48332.1, 60 sec: 48608.7, 300 sec: 48096.7). Total num frames: 569835520. Throughput: 0: 12165.7. Samples: 29938084. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:37:11,171][03423] Avg episode reward: [(0, '55.702')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:11,563][03976] Updated weights for policy 0, policy_version 72005 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:13,296][03976] Updated weights for policy 0, policy_version 72015 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:14,970][03976] Updated weights for policy 0, policy_version 72025 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:37:16,167][03423] Fps is (10 sec: 49152.0, 60 sec: 48742.4, 300 sec: 48124.5). Total num frames: 570081280. Throughput: 0: 12173.4. Samples: 30010324. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:37:16,168][03423] Avg episode reward: [(0, '55.209')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:16,649][03976] Updated weights for policy 0, policy_version 72035 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:18,309][03976] Updated weights for policy 0, policy_version 72045 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:20,055][03976] Updated weights for policy 0, policy_version 72055 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:37:21,168][03423] Fps is (10 sec: 48333.7, 60 sec: 48605.9, 300 sec: 48124.5). Total num frames: 570318848. Throughput: 0: 12172.4. Samples: 30047296. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:37:21,169][03423] Avg episode reward: [(0, '56.427')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:21,691][03976] Updated weights for policy 0, policy_version 72065 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:23,392][03976] Updated weights for policy 0, policy_version 72075 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:25,084][03976] Updated weights for policy 0, policy_version 72085 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:37:26,168][03423] Fps is (10 sec: 48331.6, 60 sec: 48605.7, 300 sec: 48152.3). Total num frames: 570564608. Throughput: 0: 12166.1. Samples: 30119772. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:37:26,169][03423] Avg episode reward: [(0, '56.282')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:26,750][03976] Updated weights for policy 0, policy_version 72095 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:28,444][03976] Updated weights for policy 0, policy_version 72105 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:30,101][03976] Updated weights for policy 0, policy_version 72115 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:37:31,168][03423] Fps is (10 sec: 49152.2, 60 sec: 48742.4, 300 sec: 48152.3). Total num frames: 570810368. Throughput: 0: 12176.9. Samples: 30193168. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:37:31,168][03423] Avg episode reward: [(0, '56.158')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:31,764][03976] Updated weights for policy 0, policy_version 72125 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:33,469][03976] Updated weights for policy 0, policy_version 72135 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:35,163][03976] Updated weights for policy 0, policy_version 72145 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:37:36,167][03423] Fps is (10 sec: 49153.0, 60 sec: 48742.4, 300 sec: 48180.1). Total num frames: 571056128. Throughput: 0: 12163.2. Samples: 30229544. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:37:36,169][03423] Avg episode reward: [(0, '56.070')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:36,833][03976] Updated weights for policy 0, policy_version 72155 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:38,433][03976] Updated weights for policy 0, policy_version 72165 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:40,144][03976] Updated weights for policy 0, policy_version 72175 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:37:41,167][03423] Fps is (10 sec: 49152.3, 60 sec: 48742.5, 300 sec: 48207.9). Total num frames: 571301888. Throughput: 0: 12157.6. Samples: 30303244. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:37:41,168][03423] Avg episode reward: [(0, '53.388')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:41,813][03976] Updated weights for policy 0, policy_version 72185 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:43,492][03976] Updated weights for policy 0, policy_version 72195 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:45,146][03976] Updated weights for policy 0, policy_version 72205 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:37:46,167][03423] Fps is (10 sec: 48332.9, 60 sec: 48606.0, 300 sec: 48207.9). Total num frames: 571539456. Throughput: 0: 12182.2. Samples: 30376860. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:37:46,168][03423] Avg episode reward: [(0, '57.079')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:46,833][03976] Updated weights for policy 0, policy_version 72215 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:48,488][03976] Updated weights for policy 0, policy_version 72225 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:50,204][03976] Updated weights for policy 0, policy_version 72235 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:37:51,168][03423] Fps is (10 sec: 48332.4, 60 sec: 48605.9, 300 sec: 48235.6). Total num frames: 571785216. Throughput: 0: 12203.7. Samples: 30413636. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:37:51,168][03423] Avg episode reward: [(0, '56.034')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:51,957][03976] Updated weights for policy 0, policy_version 72245 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:53,644][03976] Updated weights for policy 0, policy_version 72255 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:55,326][03976] Updated weights for policy 0, policy_version 72265 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:37:56,168][03423] Fps is (10 sec: 48332.1, 60 sec: 48605.8, 300 sec: 48235.6). Total num frames: 572022784. Throughput: 0: 12152.7. Samples: 30484956. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:37:56,169][03423] Avg episode reward: [(0, '53.974')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:56,976][03976] Updated weights for policy 0, policy_version 72275 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:37:58,700][03976] Updated weights for policy 0, policy_version 72285 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:00,370][03976] Updated weights for policy 0, policy_version 72295 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:38:01,168][03423] Fps is (10 sec: 48332.2, 60 sec: 48605.8, 300 sec: 48235.6). Total num frames: 572268544. Throughput: 0: 12176.6. Samples: 30558272. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:38:01,168][03423] Avg episode reward: [(0, '57.297')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:02,073][03976] Updated weights for policy 0, policy_version 72305 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:03,785][03976] Updated weights for policy 0, policy_version 72315 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:05,484][03976] Updated weights for policy 0, policy_version 72325 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:38:06,168][03423] Fps is (10 sec: 48333.3, 60 sec: 48605.8, 300 sec: 48235.6). Total num frames: 572506112. Throughput: 0: 12148.4. Samples: 30593976. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:38:06,168][03423] Avg episode reward: [(0, '54.140')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:06,202][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000072329_572514304.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:38:06,267][03956] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000070911_560898048.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:07,219][03976] Updated weights for policy 0, policy_version 72335 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:08,902][03976] Updated weights for policy 0, policy_version 72345 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:10,570][03976] Updated weights for policy 0, policy_version 72355 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:38:11,168][03423] Fps is (10 sec: 48332.9, 60 sec: 48605.9, 300 sec: 48263.4). Total num frames: 572751872. Throughput: 0: 12150.2. Samples: 30666532. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:38:11,169][03423] Avg episode reward: [(0, '56.498')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:12,227][03976] Updated weights for policy 0, policy_version 72365 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:13,927][03976] Updated weights for policy 0, policy_version 72375 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:15,688][03976] Updated weights for policy 0, policy_version 72385 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:38:16,167][03423] Fps is (10 sec: 49152.2, 60 sec: 48605.8, 300 sec: 48374.5). Total num frames: 572997632. Throughput: 0: 12135.7. Samples: 30739276. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:38:16,168][03423] Avg episode reward: [(0, '55.477')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:17,318][03976] Updated weights for policy 0, policy_version 72395 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:19,039][03976] Updated weights for policy 0, policy_version 72405 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:20,776][03976] Updated weights for policy 0, policy_version 72415 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:38:21,168][03423] Fps is (10 sec: 48332.9, 60 sec: 48605.8, 300 sec: 48346.7). Total num frames: 573235200. Throughput: 0: 12139.3. Samples: 30775812. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:38:21,169][03423] Avg episode reward: [(0, '55.765')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:22,442][03976] Updated weights for policy 0, policy_version 72425 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:24,191][03976] Updated weights for policy 0, policy_version 72435 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:25,857][03976] Updated weights for policy 0, policy_version 72445 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:38:26,167][03423] Fps is (10 sec: 47513.7, 60 sec: 48469.5, 300 sec: 48374.5). Total num frames: 573472768. Throughput: 0: 12098.4. Samples: 30847672. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:38:26,168][03423] Avg episode reward: [(0, '54.485')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:27,537][03976] Updated weights for policy 0, policy_version 72455 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:29,241][03976] Updated weights for policy 0, policy_version 72465 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:31,013][03976] Updated weights for policy 0, policy_version 72475 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:38:31,167][03423] Fps is (10 sec: 48333.4, 60 sec: 48469.4, 300 sec: 48374.4). Total num frames: 573718528. Throughput: 0: 12065.0. Samples: 30919784. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:38:31,169][03423] Avg episode reward: [(0, '56.021')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:32,749][03976] Updated weights for policy 0, policy_version 72485 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:34,440][03976] Updated weights for policy 0, policy_version 72495 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:36,117][03976] Updated weights for policy 0, policy_version 72505 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:38:36,168][03423] Fps is (10 sec: 48332.6, 60 sec: 48332.8, 300 sec: 48374.5). Total num frames: 573956096. Throughput: 0: 12033.8. Samples: 30955156. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:38:36,169][03423] Avg episode reward: [(0, '54.420')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:37,805][03976] Updated weights for policy 0, policy_version 72515 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:39,485][03976] Updated weights for policy 0, policy_version 72525 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:41,153][03976] Updated weights for policy 0, policy_version 72535 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:38:41,167][03423] Fps is (10 sec: 48332.7, 60 sec: 48332.7, 300 sec: 48374.5). Total num frames: 574201856. Throughput: 0: 12066.9. Samples: 31027964. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:38:41,168][03423] Avg episode reward: [(0, '55.227')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:42,820][03976] Updated weights for policy 0, policy_version 72545 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:44,505][03976] Updated weights for policy 0, policy_version 72555 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:38:46,168][03423] Fps is (10 sec: 48332.6, 60 sec: 48332.8, 300 sec: 48346.7). Total num frames: 574439424. Throughput: 0: 12062.3. Samples: 31101076. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:38:46,169][03423] Avg episode reward: [(0, '54.471')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:46,227][03976] Updated weights for policy 0, policy_version 72565 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:47,873][03976] Updated weights for policy 0, policy_version 72575 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:49,531][03976] Updated weights for policy 0, policy_version 72585 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:38:51,167][03423] Fps is (10 sec: 48332.8, 60 sec: 48332.8, 300 sec: 48374.5). Total num frames: 574685184. Throughput: 0: 12090.6. Samples: 31138052. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:38:51,168][03423] Avg episode reward: [(0, '53.569')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:51,184][03976] Updated weights for policy 0, policy_version 72595 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:52,830][03976] Updated weights for policy 0, policy_version 72605 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:54,481][03976] Updated weights for policy 0, policy_version 72615 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:56,160][03976] Updated weights for policy 0, policy_version 72625 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:38:56,167][03423] Fps is (10 sec: 49971.5, 60 sec: 48606.0, 300 sec: 48430.0). Total num frames: 574939136. Throughput: 0: 12123.2. Samples: 31212076. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:38:56,169][03423] Avg episode reward: [(0, '54.826')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:57,860][03976] Updated weights for policy 0, policy_version 72635 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:38:59,503][03976] Updated weights for policy 0, policy_version 72645 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:39:01,168][03423] Fps is (10 sec: 49152.0, 60 sec: 48469.4, 300 sec: 48402.2). Total num frames: 575176704. Throughput: 0: 12145.2. Samples: 31285808. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:39:01,168][03423] Avg episode reward: [(0, '54.825')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:01,181][03976] Updated weights for policy 0, policy_version 72655 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:02,916][03976] Updated weights for policy 0, policy_version 72665 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:04,569][03976] Updated weights for policy 0, policy_version 72675 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:39:06,168][03423] Fps is (10 sec: 48332.4, 60 sec: 48605.8, 300 sec: 48430.0). Total num frames: 575422464. Throughput: 0: 12135.3. Samples: 31321900. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:39:06,169][03423] Avg episode reward: [(0, '54.929')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:06,236][03976] Updated weights for policy 0, policy_version 72685 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:07,945][03976] Updated weights for policy 0, policy_version 72695 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:09,629][03976] Updated weights for policy 0, policy_version 72705 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:39:11,168][03423] Fps is (10 sec: 49152.0, 60 sec: 48606.0, 300 sec: 48457.8). Total num frames: 575668224. Throughput: 0: 12163.1. Samples: 31395012. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:39:11,169][03423] Avg episode reward: [(0, '54.557')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:11,272][03976] Updated weights for policy 0, policy_version 72715 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:12,979][03976] Updated weights for policy 0, policy_version 72725 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:14,700][03976] Updated weights for policy 0, policy_version 72735 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:39:16,168][03423] Fps is (10 sec: 48332.4, 60 sec: 48469.2, 300 sec: 48430.0). Total num frames: 575905792. Throughput: 0: 12184.1. Samples: 31468072. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:39:16,169][03423] Avg episode reward: [(0, '54.526')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:16,326][03976] Updated weights for policy 0, policy_version 72745 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:18,029][03976] Updated weights for policy 0, policy_version 72755 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:19,745][03976] Updated weights for policy 0, policy_version 72765 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:39:21,167][03423] Fps is (10 sec: 48332.9, 60 sec: 48606.0, 300 sec: 48457.8). Total num frames: 576151552. Throughput: 0: 12195.4. Samples: 31503948. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:39:21,168][03423] Avg episode reward: [(0, '52.965')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:21,432][03976] Updated weights for policy 0, policy_version 72775 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:23,124][03976] Updated weights for policy 0, policy_version 72785 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:24,844][03976] Updated weights for policy 0, policy_version 72795 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:39:26,167][03423] Fps is (10 sec: 48333.7, 60 sec: 48605.9, 300 sec: 48430.0). Total num frames: 576389120. Throughput: 0: 12188.9. Samples: 31576464. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:39:26,168][03423] Avg episode reward: [(0, '56.055')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:26,554][03976] Updated weights for policy 0, policy_version 72805 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:28,244][03976] Updated weights for policy 0, policy_version 72815 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:29,960][03976] Updated weights for policy 0, policy_version 72825 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:39:31,169][03423] Fps is (10 sec: 48323.4, 60 sec: 48604.3, 300 sec: 48457.5). Total num frames: 576634880. Throughput: 0: 12170.3. Samples: 31648764. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:39:31,171][03423] Avg episode reward: [(0, '54.151')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:31,643][03976] Updated weights for policy 0, policy_version 72835 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:33,341][03976] Updated weights for policy 0, policy_version 72845 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:35,108][03976] Updated weights for policy 0, policy_version 72855 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:39:36,168][03423] Fps is (10 sec: 48332.5, 60 sec: 48605.8, 300 sec: 48457.8). Total num frames: 576872448. Throughput: 0: 12163.0. Samples: 31685388. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:39:36,169][03423] Avg episode reward: [(0, '54.041')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:36,813][03976] Updated weights for policy 0, policy_version 72865 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:38,546][03976] Updated weights for policy 0, policy_version 72875 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:40,266][03976] Updated weights for policy 0, policy_version 72885 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:39:41,167][03423] Fps is (10 sec: 47522.8, 60 sec: 48469.3, 300 sec: 48430.0). Total num frames: 577110016. Throughput: 0: 12094.0. Samples: 31756304. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:39:41,168][03423] Avg episode reward: [(0, '55.800')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:41,972][03976] Updated weights for policy 0, policy_version 72895 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:43,669][03976] Updated weights for policy 0, policy_version 72905 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:45,357][03976] Updated weights for policy 0, policy_version 72915 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:39:46,168][03423] Fps is (10 sec: 47513.3, 60 sec: 48469.3, 300 sec: 48430.0). Total num frames: 577347584. Throughput: 0: 12055.8. Samples: 31828320. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:39:46,169][03423] Avg episode reward: [(0, '52.945')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:47,023][03976] Updated weights for policy 0, policy_version 72925 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:48,720][03976] Updated weights for policy 0, policy_version 72935 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:50,391][03976] Updated weights for policy 0, policy_version 72945 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:39:51,168][03423] Fps is (10 sec: 48331.4, 60 sec: 48469.1, 300 sec: 48430.0). Total num frames: 577593344. Throughput: 0: 12065.5. Samples: 31864848. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:39:51,169][03423] Avg episode reward: [(0, '56.108')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:52,068][03976] Updated weights for policy 0, policy_version 72955 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:53,772][03976] Updated weights for policy 0, policy_version 72965 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:55,439][03976] Updated weights for policy 0, policy_version 72975 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:39:56,168][03423] Fps is (10 sec: 49151.6, 60 sec: 48332.6, 300 sec: 48457.7). Total num frames: 577839104. Throughput: 0: 12073.1. Samples: 31938304. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:39:56,169][03423] Avg episode reward: [(0, '53.253')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:57,111][03976] Updated weights for policy 0, policy_version 72985 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:39:58,825][03976] Updated weights for policy 0, policy_version 72995 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:00,497][03976] Updated weights for policy 0, policy_version 73005 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:40:01,167][03423] Fps is (10 sec: 48334.1, 60 sec: 48332.8, 300 sec: 48457.8). Total num frames: 578076672. Throughput: 0: 12067.6. Samples: 32011112. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:40:01,168][03423] Avg episode reward: [(0, '55.606')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:02,183][03976] Updated weights for policy 0, policy_version 73015 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:03,845][03976] Updated weights for policy 0, policy_version 73025 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:05,499][03976] Updated weights for policy 0, policy_version 73035 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:40:06,168][03423] Fps is (10 sec: 48332.4, 60 sec: 48332.6, 300 sec: 48485.5). Total num frames: 578322432. Throughput: 0: 12074.0. Samples: 32047280. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:40:06,169][03423] Avg episode reward: [(0, '54.594')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:06,174][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000073039_578330624.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:40:06,241][03956] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000071617_566681600.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:07,176][03976] Updated weights for policy 0, policy_version 73045 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:08,859][03976] Updated weights for policy 0, policy_version 73055 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:10,569][03976] Updated weights for policy 0, policy_version 73065 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:40:11,168][03423] Fps is (10 sec: 49151.6, 60 sec: 48332.8, 300 sec: 48513.3). Total num frames: 578568192. Throughput: 0: 12102.1. Samples: 32121060. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:40:11,169][03423] Avg episode reward: [(0, '55.232')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:12,253][03976] Updated weights for policy 0, policy_version 73075 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:13,913][03976] Updated weights for policy 0, policy_version 73085 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:15,573][03976] Updated weights for policy 0, policy_version 73095 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:40:16,167][03423] Fps is (10 sec: 49153.4, 60 sec: 48469.5, 300 sec: 48541.1). Total num frames: 578813952. Throughput: 0: 12120.0. Samples: 32194140. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:40:16,168][03423] Avg episode reward: [(0, '56.056')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:17,270][03976] Updated weights for policy 0, policy_version 73105 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:18,932][03976] Updated weights for policy 0, policy_version 73115 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:20,604][03976] Updated weights for policy 0, policy_version 73125 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:40:21,168][03423] Fps is (10 sec: 49152.0, 60 sec: 48469.3, 300 sec: 48541.0). Total num frames: 579059712. Throughput: 0: 12112.7. Samples: 32230460. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:40:21,169][03423] Avg episode reward: [(0, '57.289')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:22,261][03976] Updated weights for policy 0, policy_version 73135 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:23,930][03976] Updated weights for policy 0, policy_version 73145 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:25,574][03976] Updated weights for policy 0, policy_version 73155 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:40:26,168][03423] Fps is (10 sec: 48331.9, 60 sec: 48469.2, 300 sec: 48513.3). Total num frames: 579297280. Throughput: 0: 12185.2. Samples: 32304640. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:40:26,169][03423] Avg episode reward: [(0, '55.093')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:27,313][03976] Updated weights for policy 0, policy_version 73165 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:28,971][03976] Updated weights for policy 0, policy_version 73175 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:30,624][03976] Updated weights for policy 0, policy_version 73185 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:40:31,167][03423] Fps is (10 sec: 48333.1, 60 sec: 48470.9, 300 sec: 48513.3). Total num frames: 579543040. Throughput: 0: 12208.9. Samples: 32377720. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:40:31,169][03423] Avg episode reward: [(0, '53.293')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:32,325][03976] Updated weights for policy 0, policy_version 73195 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:34,051][03976] Updated weights for policy 0, policy_version 73205 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:35,760][03976] Updated weights for policy 0, policy_version 73215 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:40:36,167][03423] Fps is (10 sec: 49153.0, 60 sec: 48605.9, 300 sec: 48541.1). Total num frames: 579788800. Throughput: 0: 12209.9. Samples: 32414288. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:40:36,168][03423] Avg episode reward: [(0, '52.147')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:37,460][03976] Updated weights for policy 0, policy_version 73225 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:39,184][03976] Updated weights for policy 0, policy_version 73235 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:40,891][03976] Updated weights for policy 0, policy_version 73245 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:40:41,167][03423] Fps is (10 sec: 48332.7, 60 sec: 48605.9, 300 sec: 48541.1). Total num frames: 580026368. Throughput: 0: 12171.9. Samples: 32486036. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:40:41,169][03423] Avg episode reward: [(0, '55.639')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:42,558][03976] Updated weights for policy 0, policy_version 73255 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:44,235][03976] Updated weights for policy 0, policy_version 73265 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:45,938][03976] Updated weights for policy 0, policy_version 73275 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:40:46,168][03423] Fps is (10 sec: 48332.5, 60 sec: 48742.4, 300 sec: 48568.8). Total num frames: 580272128. Throughput: 0: 12171.2. Samples: 32558816. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:40:46,169][03423] Avg episode reward: [(0, '55.276')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:47,639][03976] Updated weights for policy 0, policy_version 73285 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:49,314][03976] Updated weights for policy 0, policy_version 73295 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:50,977][03976] Updated weights for policy 0, policy_version 73305 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:40:51,167][03423] Fps is (10 sec: 48333.1, 60 sec: 48606.1, 300 sec: 48541.1). Total num frames: 580509696. Throughput: 0: 12161.6. Samples: 32594548. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:40:51,168][03423] Avg episode reward: [(0, '53.045')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:52,690][03976] Updated weights for policy 0, policy_version 73315 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:54,392][03976] Updated weights for policy 0, policy_version 73325 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:56,061][03976] Updated weights for policy 0, policy_version 73335 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:40:56,168][03423] Fps is (10 sec: 48332.8, 60 sec: 48606.0, 300 sec: 48568.8). Total num frames: 580755456. Throughput: 0: 12145.5. Samples: 32667608. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:40:56,169][03423] Avg episode reward: [(0, '56.377')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:57,723][03976] Updated weights for policy 0, policy_version 73345 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:40:59,436][03976] Updated weights for policy 0, policy_version 73355 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:01,142][03976] Updated weights for policy 0, policy_version 73365 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:41:01,167][03423] Fps is (10 sec: 49151.7, 60 sec: 48742.4, 300 sec: 48568.8). Total num frames: 581001216. Throughput: 0: 12144.0. Samples: 32740620. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:41:01,168][03423] Avg episode reward: [(0, '56.599')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:02,808][03976] Updated weights for policy 0, policy_version 73375 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:04,472][03976] Updated weights for policy 0, policy_version 73385 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:06,150][03976] Updated weights for policy 0, policy_version 73395 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:41:06,167][03423] Fps is (10 sec: 49152.3, 60 sec: 48742.6, 300 sec: 48569.5). Total num frames: 581246976. Throughput: 0: 12146.2. Samples: 32777040. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:41:06,168][03423] Avg episode reward: [(0, '57.171')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:07,887][03976] Updated weights for policy 0, policy_version 73405 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:09,671][03976] Updated weights for policy 0, policy_version 73415 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:41:11,167][03423] Fps is (10 sec: 48332.8, 60 sec: 48605.9, 300 sec: 48568.8). Total num frames: 581484544. Throughput: 0: 12097.0. Samples: 32849004. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:41:11,168][03423] Avg episode reward: [(0, '55.715')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:11,367][03976] Updated weights for policy 0, policy_version 73425 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:13,037][03976] Updated weights for policy 0, policy_version 73435 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:14,733][03976] Updated weights for policy 0, policy_version 73445 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:41:16,168][03423] Fps is (10 sec: 47513.4, 60 sec: 48469.3, 300 sec: 48541.1). Total num frames: 581722112. Throughput: 0: 12076.0. Samples: 32921140. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:41:16,169][03423] Avg episode reward: [(0, '55.956')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:16,412][03976] Updated weights for policy 0, policy_version 73455 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:18,097][03976] Updated weights for policy 0, policy_version 73465 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:19,788][03976] Updated weights for policy 0, policy_version 73475 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:41:21,168][03423] Fps is (10 sec: 48332.3, 60 sec: 48469.3, 300 sec: 48541.1). Total num frames: 581967872. Throughput: 0: 12074.5. Samples: 32957644. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:41:21,169][03423] Avg episode reward: [(0, '55.307')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:21,451][03976] Updated weights for policy 0, policy_version 73485 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:23,126][03976] Updated weights for policy 0, policy_version 73495 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:24,764][03976] Updated weights for policy 0, policy_version 73505 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:41:26,168][03423] Fps is (10 sec: 49149.0, 60 sec: 48605.5, 300 sec: 48568.7). Total num frames: 582213632. Throughput: 0: 12115.5. Samples: 33031240. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:41:26,169][03423] Avg episode reward: [(0, '57.195')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:26,489][03976] Updated weights for policy 0, policy_version 73515 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:28,192][03976] Updated weights for policy 0, policy_version 73525 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:29,974][03976] Updated weights for policy 0, policy_version 73535 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:41:31,168][03423] Fps is (10 sec: 48333.0, 60 sec: 48469.3, 300 sec: 48541.1). Total num frames: 582451200. Throughput: 0: 12084.2. Samples: 33102604. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:41:31,169][03423] Avg episode reward: [(0, '53.946')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:31,686][03976] Updated weights for policy 0, policy_version 73545 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:33,363][03976] Updated weights for policy 0, policy_version 73555 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:35,043][03976] Updated weights for policy 0, policy_version 73565 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:41:36,168][03423] Fps is (10 sec: 48335.8, 60 sec: 48469.3, 300 sec: 48541.1). Total num frames: 582696960. Throughput: 0: 12104.9. Samples: 33139268. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:41:36,168][03423] Avg episode reward: [(0, '54.878')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:36,719][03976] Updated weights for policy 0, policy_version 73575 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:38,361][03976] Updated weights for policy 0, policy_version 73585 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:40,006][03976] Updated weights for policy 0, policy_version 73595 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:41:41,167][03423] Fps is (10 sec: 48333.2, 60 sec: 48469.4, 300 sec: 48513.3). Total num frames: 582934528. Throughput: 0: 12111.1. Samples: 33212608. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:41:41,168][03423] Avg episode reward: [(0, '53.051')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:41,721][03976] Updated weights for policy 0, policy_version 73605 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:43,453][03976] Updated weights for policy 0, policy_version 73615 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:45,172][03976] Updated weights for policy 0, policy_version 73625 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:41:46,168][03423] Fps is (10 sec: 47513.0, 60 sec: 48332.7, 300 sec: 48485.5). Total num frames: 583172096. Throughput: 0: 12082.6. Samples: 33284340. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:41:46,169][03423] Avg episode reward: [(0, '55.566')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:46,907][03976] Updated weights for policy 0, policy_version 73635 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:48,584][03976] Updated weights for policy 0, policy_version 73645 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:50,248][03976] Updated weights for policy 0, policy_version 73655 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:41:51,168][03423] Fps is (10 sec: 48332.6, 60 sec: 48469.3, 300 sec: 48513.3). Total num frames: 583417856. Throughput: 0: 12080.3. Samples: 33320652. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:41:51,169][03423] Avg episode reward: [(0, '52.480')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:51,932][03976] Updated weights for policy 0, policy_version 73665 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:53,643][03976] Updated weights for policy 0, policy_version 73675 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:55,400][03976] Updated weights for policy 0, policy_version 73685 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:41:56,167][03423] Fps is (10 sec: 48333.8, 60 sec: 48332.9, 300 sec: 48485.6). Total num frames: 583655424. Throughput: 0: 12089.3. Samples: 33393020. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:41:56,168][03423] Avg episode reward: [(0, '55.880')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:57,126][03976] Updated weights for policy 0, policy_version 73695 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:41:58,795][03976] Updated weights for policy 0, policy_version 73705 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:00,424][03976] Updated weights for policy 0, policy_version 73715 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:42:01,168][03423] Fps is (10 sec: 48332.6, 60 sec: 48332.8, 300 sec: 48513.3). Total num frames: 583901184. Throughput: 0: 12100.4. Samples: 33465660. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:42:01,169][03423] Avg episode reward: [(0, '54.856')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:02,160][03976] Updated weights for policy 0, policy_version 73725 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:03,865][03976] Updated weights for policy 0, policy_version 73735 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:05,532][03976] Updated weights for policy 0, policy_version 73745 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:42:06,167][03423] Fps is (10 sec: 48332.7, 60 sec: 48196.3, 300 sec: 48485.6). Total num frames: 584138752. Throughput: 0: 12086.3. Samples: 33501528. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:42:06,169][03423] Avg episode reward: [(0, '52.930')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:06,218][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000073749_584146944.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:42:06,284][03956] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000072329_572514304.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:07,208][03976] Updated weights for policy 0, policy_version 73755 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:08,883][03976] Updated weights for policy 0, policy_version 73765 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:10,547][03976] Updated weights for policy 0, policy_version 73775 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:42:11,167][03423] Fps is (10 sec: 48333.2, 60 sec: 48332.8, 300 sec: 48485.5). Total num frames: 584384512. Throughput: 0: 12081.7. Samples: 33574908. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:42:11,168][03423] Avg episode reward: [(0, '54.521')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:12,203][03976] Updated weights for policy 0, policy_version 73785 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:13,871][03976] Updated weights for policy 0, policy_version 73795 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:15,553][03976] Updated weights for policy 0, policy_version 73805 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:42:16,167][03423] Fps is (10 sec: 49151.8, 60 sec: 48469.4, 300 sec: 48513.3). Total num frames: 584630272. Throughput: 0: 12130.8. Samples: 33648488. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:42:16,169][03423] Avg episode reward: [(0, '56.605')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:17,267][03976] Updated weights for policy 0, policy_version 73815 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:18,942][03976] Updated weights for policy 0, policy_version 73825 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:20,711][03976] Updated weights for policy 0, policy_version 73835 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:42:21,167][03423] Fps is (10 sec: 48332.7, 60 sec: 48332.9, 300 sec: 48485.6). Total num frames: 584867840. Throughput: 0: 12115.8. Samples: 33684480. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:42:21,169][03423] Avg episode reward: [(0, '55.942')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:22,369][03976] Updated weights for policy 0, policy_version 73845 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:24,097][03976] Updated weights for policy 0, policy_version 73855 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:25,769][03976] Updated weights for policy 0, policy_version 73865 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:42:26,167][03423] Fps is (10 sec: 48332.8, 60 sec: 48333.3, 300 sec: 48485.5). Total num frames: 585113600. Throughput: 0: 12082.0. Samples: 33756300. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:42:26,168][03423] Avg episode reward: [(0, '55.099')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:27,462][03976] Updated weights for policy 0, policy_version 73875 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:29,136][03976] Updated weights for policy 0, policy_version 73885 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:30,909][03976] Updated weights for policy 0, policy_version 73895 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:42:31,168][03423] Fps is (10 sec: 48331.3, 60 sec: 48332.6, 300 sec: 48457.7). Total num frames: 585351168. Throughput: 0: 12111.7. Samples: 33829368. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:42:31,169][03423] Avg episode reward: [(0, '54.323')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:32,596][03976] Updated weights for policy 0, policy_version 73905 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:34,287][03976] Updated weights for policy 0, policy_version 73915 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:35,968][03976] Updated weights for policy 0, policy_version 73925 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:42:36,168][03423] Fps is (10 sec: 48332.0, 60 sec: 48332.7, 300 sec: 48457.7). Total num frames: 585596928. Throughput: 0: 12110.1. Samples: 33865608. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:42:36,169][03423] Avg episode reward: [(0, '55.651')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:37,668][03976] Updated weights for policy 0, policy_version 73935 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:39,333][03976] Updated weights for policy 0, policy_version 73945 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:41,041][03976] Updated weights for policy 0, policy_version 73955 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:42:41,167][03423] Fps is (10 sec: 48334.4, 60 sec: 48332.8, 300 sec: 48457.8). Total num frames: 585834496. Throughput: 0: 12103.6. Samples: 33937684. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:42:41,168][03423] Avg episode reward: [(0, '53.653')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:42,741][03976] Updated weights for policy 0, policy_version 73965 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:44,417][03976] Updated weights for policy 0, policy_version 73975 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:46,086][03976] Updated weights for policy 0, policy_version 73985 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:42:46,167][03423] Fps is (10 sec: 48333.6, 60 sec: 48469.5, 300 sec: 48457.8). Total num frames: 586080256. Throughput: 0: 12118.6. Samples: 34010996. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:42:46,168][03423] Avg episode reward: [(0, '56.136')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:47,773][03976] Updated weights for policy 0, policy_version 73995 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:49,459][03976] Updated weights for policy 0, policy_version 74005 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:42:51,168][03423] Fps is (10 sec: 48332.3, 60 sec: 48332.7, 300 sec: 48457.8). Total num frames: 586317824. Throughput: 0: 12128.3. Samples: 34047304. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:42:51,169][03423] Avg episode reward: [(0, '54.209')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:51,192][03976] Updated weights for policy 0, policy_version 74015 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:52,857][03976] Updated weights for policy 0, policy_version 74025 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:54,553][03976] Updated weights for policy 0, policy_version 74035 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:42:56,168][03423] Fps is (10 sec: 48332.6, 60 sec: 48469.2, 300 sec: 48457.8). Total num frames: 586563584. Throughput: 0: 12102.1. Samples: 34119504. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:42:56,169][03423] Avg episode reward: [(0, '53.605')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:56,249][03976] Updated weights for policy 0, policy_version 74045 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:57,930][03976] Updated weights for policy 0, policy_version 74055 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:42:59,684][03976] Updated weights for policy 0, policy_version 74065 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:43:01,167][03423] Fps is (10 sec: 48333.3, 60 sec: 48332.9, 300 sec: 48457.8). Total num frames: 586801152. Throughput: 0: 12063.3. Samples: 34191336. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:43:01,169][03423] Avg episode reward: [(0, '56.439')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:01,417][03976] Updated weights for policy 0, policy_version 74075 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:03,051][03976] Updated weights for policy 0, policy_version 74085 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:04,749][03976] Updated weights for policy 0, policy_version 74095 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:43:06,168][03423] Fps is (10 sec: 48332.9, 60 sec: 48469.3, 300 sec: 48457.8). Total num frames: 587046912. Throughput: 0: 12077.4. Samples: 34227964. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:43:06,169][03423] Avg episode reward: [(0, '55.268')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:06,440][03976] Updated weights for policy 0, policy_version 74105 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:08,117][03976] Updated weights for policy 0, policy_version 74115 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:09,879][03976] Updated weights for policy 0, policy_version 74125 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:43:11,168][03423] Fps is (10 sec: 48332.2, 60 sec: 48332.7, 300 sec: 48430.0). Total num frames: 587284480. Throughput: 0: 12088.9. Samples: 34300300. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:43:11,169][03423] Avg episode reward: [(0, '54.949')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:11,593][03976] Updated weights for policy 0, policy_version 74135 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:13,261][03976] Updated weights for policy 0, policy_version 74145 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:14,966][03976] Updated weights for policy 0, policy_version 74155 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:43:16,168][03423] Fps is (10 sec: 47513.5, 60 sec: 48196.2, 300 sec: 48430.0). Total num frames: 587522048. Throughput: 0: 12062.8. Samples: 34372192. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:43:16,169][03423] Avg episode reward: [(0, '53.853')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:16,641][03976] Updated weights for policy 0, policy_version 74165 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:18,322][03976] Updated weights for policy 0, policy_version 74175 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:20,038][03976] Updated weights for policy 0, policy_version 74185 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:43:21,168][03423] Fps is (10 sec: 48333.3, 60 sec: 48332.8, 300 sec: 48457.8). Total num frames: 587767808. Throughput: 0: 12067.5. Samples: 34408644. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:43:21,169][03423] Avg episode reward: [(0, '55.823')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:21,754][03976] Updated weights for policy 0, policy_version 74195 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:23,427][03976] Updated weights for policy 0, policy_version 74205 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:25,134][03976] Updated weights for policy 0, policy_version 74215 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:43:26,168][03423] Fps is (10 sec: 49152.1, 60 sec: 48332.8, 300 sec: 48457.8). Total num frames: 588013568. Throughput: 0: 12086.4. Samples: 34481572. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:43:26,169][03423] Avg episode reward: [(0, '55.682')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:26,824][03976] Updated weights for policy 0, policy_version 74225 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:28,500][03976] Updated weights for policy 0, policy_version 74235 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:30,164][03976] Updated weights for policy 0, policy_version 74245 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:43:31,167][03423] Fps is (10 sec: 49152.4, 60 sec: 48469.6, 300 sec: 48485.5). Total num frames: 588259328. Throughput: 0: 12079.1. Samples: 34554556. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:43:31,168][03423] Avg episode reward: [(0, '55.698')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:31,840][03976] Updated weights for policy 0, policy_version 74255 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:33,516][03976] Updated weights for policy 0, policy_version 74265 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:35,217][03976] Updated weights for policy 0, policy_version 74275 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:43:36,168][03423] Fps is (10 sec: 48332.6, 60 sec: 48332.9, 300 sec: 48457.8). Total num frames: 588496896. Throughput: 0: 12084.3. Samples: 34591096. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:43:36,169][03423] Avg episode reward: [(0, '55.852')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:36,853][03976] Updated weights for policy 0, policy_version 74285 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:38,532][03976] Updated weights for policy 0, policy_version 74295 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:40,217][03976] Updated weights for policy 0, policy_version 74305 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:43:41,168][03423] Fps is (10 sec: 48332.4, 60 sec: 48469.3, 300 sec: 48485.5). Total num frames: 588742656. Throughput: 0: 12115.0. Samples: 34664680. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:43:41,169][03423] Avg episode reward: [(0, '56.174')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:41,946][03976] Updated weights for policy 0, policy_version 74315 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:43,653][03976] Updated weights for policy 0, policy_version 74325 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:45,357][03976] Updated weights for policy 0, policy_version 74335 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:43:46,168][03423] Fps is (10 sec: 48332.8, 60 sec: 48332.8, 300 sec: 48457.8). Total num frames: 588980224. Throughput: 0: 12124.0. Samples: 34736916. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:43:46,169][03423] Avg episode reward: [(0, '54.117')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:47,068][03976] Updated weights for policy 0, policy_version 74345 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:48,763][03976] Updated weights for policy 0, policy_version 74355 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:50,499][03976] Updated weights for policy 0, policy_version 74365 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:43:51,168][03423] Fps is (10 sec: 47513.6, 60 sec: 48332.9, 300 sec: 48402.2). Total num frames: 589217792. Throughput: 0: 12106.3. Samples: 34772748. Policy #0 lag: (min: 0.0, avg: 1.4, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:43:51,169][03423] Avg episode reward: [(0, '56.454')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:52,269][03976] Updated weights for policy 0, policy_version 74375 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:54,014][03976] Updated weights for policy 0, policy_version 74385 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:55,753][03976] Updated weights for policy 0, policy_version 74395 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:43:56,168][03423] Fps is (10 sec: 47513.7, 60 sec: 48196.3, 300 sec: 48402.2). Total num frames: 589455360. Throughput: 0: 12056.9. Samples: 34842860. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:43:56,169][03423] Avg episode reward: [(0, '54.610')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:57,472][03976] Updated weights for policy 0, policy_version 74405 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:43:59,207][03976] Updated weights for policy 0, policy_version 74415 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:00,914][03976] Updated weights for policy 0, policy_version 74425 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:44:01,167][03423] Fps is (10 sec: 47513.8, 60 sec: 48196.3, 300 sec: 48374.5). Total num frames: 589692928. Throughput: 0: 12046.2. Samples: 34914272. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:44:01,169][03423] Avg episode reward: [(0, '51.871')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:02,606][03976] Updated weights for policy 0, policy_version 74435 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:04,278][03976] Updated weights for policy 0, policy_version 74445 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:06,009][03976] Updated weights for policy 0, policy_version 74455 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:44:06,167][03423] Fps is (10 sec: 48333.0, 60 sec: 48196.3, 300 sec: 48374.5). Total num frames: 589938688. Throughput: 0: 12031.5. Samples: 34950060. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:44:06,168][03423] Avg episode reward: [(0, '54.885')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:06,172][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000074456_589938688.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:44:06,244][03956] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000073039_578330624.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:07,708][03976] Updated weights for policy 0, policy_version 74465 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:09,393][03976] Updated weights for policy 0, policy_version 74475 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:11,088][03976] Updated weights for policy 0, policy_version 74485 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:44:11,167][03423] Fps is (10 sec: 48332.7, 60 sec: 48196.4, 300 sec: 48374.5). Total num frames: 590176256. Throughput: 0: 12018.2. Samples: 35022392. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:44:11,169][03423] Avg episode reward: [(0, '54.073')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:12,844][03976] Updated weights for policy 0, policy_version 74495 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:14,561][03976] Updated weights for policy 0, policy_version 74505 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:44:16,167][03423] Fps is (10 sec: 47513.7, 60 sec: 48196.3, 300 sec: 48346.7). Total num frames: 590413824. Throughput: 0: 12004.9. Samples: 35094776. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:44:16,168][03423] Avg episode reward: [(0, '53.506')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:16,214][03976] Updated weights for policy 0, policy_version 74515 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:17,915][03976] Updated weights for policy 0, policy_version 74525 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:19,575][03976] Updated weights for policy 0, policy_version 74535 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:44:21,168][03423] Fps is (10 sec: 48332.4, 60 sec: 48196.2, 300 sec: 48374.4). Total num frames: 590659584. Throughput: 0: 12000.4. Samples: 35131112. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:44:21,169][03423] Avg episode reward: [(0, '55.010')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:21,269][03976] Updated weights for policy 0, policy_version 74545 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:22,953][03976] Updated weights for policy 0, policy_version 74555 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:24,628][03976] Updated weights for policy 0, policy_version 74565 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:44:26,168][03423] Fps is (10 sec: 48332.6, 60 sec: 48059.7, 300 sec: 48347.0). Total num frames: 590897152. Throughput: 0: 11988.5. Samples: 35204164. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:44:26,168][03423] Avg episode reward: [(0, '56.523')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:26,340][03976] Updated weights for policy 0, policy_version 74575 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:28,022][03976] Updated weights for policy 0, policy_version 74585 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:29,689][03976] Updated weights for policy 0, policy_version 74595 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:44:31,168][03423] Fps is (10 sec: 48333.0, 60 sec: 48059.7, 300 sec: 48374.5). Total num frames: 591142912. Throughput: 0: 12001.6. Samples: 35276988. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:44:31,169][03423] Avg episode reward: [(0, '54.672')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:31,349][03976] Updated weights for policy 0, policy_version 74605 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:33,020][03976] Updated weights for policy 0, policy_version 74615 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:34,714][03976] Updated weights for policy 0, policy_version 74625 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:44:36,168][03423] Fps is (10 sec: 49151.7, 60 sec: 48196.2, 300 sec: 48402.2). Total num frames: 591388672. Throughput: 0: 12017.4. Samples: 35313532. Policy #0 lag: (min: 0.0, avg: 1.2, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:44:36,169][03423] Avg episode reward: [(0, '53.483')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:36,402][03976] Updated weights for policy 0, policy_version 74635 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:38,192][03976] Updated weights for policy 0, policy_version 74645 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:39,940][03976] Updated weights for policy 0, policy_version 74655 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:44:41,168][03423] Fps is (10 sec: 48332.6, 60 sec: 48059.7, 300 sec: 48402.2). Total num frames: 591626240. Throughput: 0: 12063.8. Samples: 35385732. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:44:41,169][03423] Avg episode reward: [(0, '55.064')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:41,595][03976] Updated weights for policy 0, policy_version 74665 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:43,243][03976] Updated weights for policy 0, policy_version 74675 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:44,922][03976] Updated weights for policy 0, policy_version 74685 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:44:46,167][03423] Fps is (10 sec: 48333.3, 60 sec: 48196.3, 300 sec: 48402.3). Total num frames: 591872000. Throughput: 0: 12101.6. Samples: 35458844. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:44:46,168][03423] Avg episode reward: [(0, '54.356')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:46,549][03976] Updated weights for policy 0, policy_version 74695 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:48,225][03976] Updated weights for policy 0, policy_version 74705 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:49,870][03976] Updated weights for policy 0, policy_version 74715 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:44:51,168][03423] Fps is (10 sec: 49151.3, 60 sec: 48332.6, 300 sec: 48402.2). Total num frames: 592117760. Throughput: 0: 12131.3. Samples: 35495972. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:44:51,169][03423] Avg episode reward: [(0, '55.473')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:51,569][03976] Updated weights for policy 0, policy_version 74725 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:53,315][03976] Updated weights for policy 0, policy_version 74735 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:55,002][03976] Updated weights for policy 0, policy_version 74745 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:44:56,167][03423] Fps is (10 sec: 48332.9, 60 sec: 48332.9, 300 sec: 48402.2). Total num frames: 592355328. Throughput: 0: 12128.2. Samples: 35568160. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:44:56,168][03423] Avg episode reward: [(0, '54.956')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:56,717][03976] Updated weights for policy 0, policy_version 74755 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:44:58,432][03976] Updated weights for policy 0, policy_version 74765 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:00,145][03976] Updated weights for policy 0, policy_version 74775 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:45:01,168][03423] Fps is (10 sec: 47514.4, 60 sec: 48332.7, 300 sec: 48374.5). Total num frames: 592592896. Throughput: 0: 12112.9. Samples: 35639856. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:45:01,169][03423] Avg episode reward: [(0, '51.890')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:01,901][03976] Updated weights for policy 0, policy_version 74785 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:03,571][03976] Updated weights for policy 0, policy_version 74795 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:05,262][03976] Updated weights for policy 0, policy_version 74805 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:45:06,167][03423] Fps is (10 sec: 48332.9, 60 sec: 48332.9, 300 sec: 48374.5). Total num frames: 592838656. Throughput: 0: 12112.3. Samples: 35676164. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:45:06,168][03423] Avg episode reward: [(0, '52.693')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:06,960][03976] Updated weights for policy 0, policy_version 74815 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:08,678][03976] Updated weights for policy 0, policy_version 74825 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:10,427][03976] Updated weights for policy 0, policy_version 74835 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:45:11,167][03423] Fps is (10 sec: 48333.0, 60 sec: 48332.8, 300 sec: 48346.7). Total num frames: 593076224. Throughput: 0: 12083.6. Samples: 35747924. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:45:11,169][03423] Avg episode reward: [(0, '54.710')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:12,142][03976] Updated weights for policy 0, policy_version 74845 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:13,811][03976] Updated weights for policy 0, policy_version 74855 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:15,502][03976] Updated weights for policy 0, policy_version 74865 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:45:16,168][03423] Fps is (10 sec: 47512.9, 60 sec: 48332.7, 300 sec: 48318.9). Total num frames: 593313792. Throughput: 0: 12066.9. Samples: 35820000. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:45:16,169][03423] Avg episode reward: [(0, '55.795')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:17,220][03976] Updated weights for policy 0, policy_version 74875 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:18,938][03976] Updated weights for policy 0, policy_version 74885 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:20,675][03976] Updated weights for policy 0, policy_version 74895 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:45:21,168][03423] Fps is (10 sec: 48332.0, 60 sec: 48332.7, 300 sec: 48346.7). Total num frames: 593559552. Throughput: 0: 12059.5. Samples: 35856212. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:45:21,169][03423] Avg episode reward: [(0, '55.950')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:22,351][03976] Updated weights for policy 0, policy_version 74905 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:23,998][03976] Updated weights for policy 0, policy_version 74915 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:25,685][03976] Updated weights for policy 0, policy_version 74925 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:45:26,167][03423] Fps is (10 sec: 48333.3, 60 sec: 48332.8, 300 sec: 48318.9). Total num frames: 593797120. Throughput: 0: 12054.6. Samples: 35928188. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:45:26,169][03423] Avg episode reward: [(0, '54.563')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:27,369][03976] Updated weights for policy 0, policy_version 74935 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:29,056][03976] Updated weights for policy 0, policy_version 74945 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:30,705][03976] Updated weights for policy 0, policy_version 74955 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:45:31,168][03423] Fps is (10 sec: 48333.5, 60 sec: 48332.8, 300 sec: 48318.9). Total num frames: 594042880. Throughput: 0: 12061.9. Samples: 36001632. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:45:31,169][03423] Avg episode reward: [(0, '56.344')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:32,411][03976] Updated weights for policy 0, policy_version 74965 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:34,124][03976] Updated weights for policy 0, policy_version 74975 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:35,790][03976] Updated weights for policy 0, policy_version 74985 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:45:36,168][03423] Fps is (10 sec: 49151.9, 60 sec: 48332.9, 300 sec: 48346.7). Total num frames: 594288640. Throughput: 0: 12040.4. Samples: 36037788. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:45:36,169][03423] Avg episode reward: [(0, '54.701')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:37,486][03976] Updated weights for policy 0, policy_version 74995 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:39,186][03976] Updated weights for policy 0, policy_version 75005 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:40,846][03976] Updated weights for policy 0, policy_version 75015 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:45:41,168][03423] Fps is (10 sec: 48332.7, 60 sec: 48332.8, 300 sec: 48318.9). Total num frames: 594526208. Throughput: 0: 12069.8. Samples: 36111300. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:45:41,168][03423] Avg episode reward: [(0, '52.992')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:42,480][03976] Updated weights for policy 0, policy_version 75025 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:44,194][03976] Updated weights for policy 0, policy_version 75035 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:45,867][03976] Updated weights for policy 0, policy_version 75045 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:45:46,167][03423] Fps is (10 sec: 48333.0, 60 sec: 48332.8, 300 sec: 48346.7). Total num frames: 594771968. Throughput: 0: 12092.7. Samples: 36184028. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:45:46,168][03423] Avg episode reward: [(0, '55.630')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:47,608][03976] Updated weights for policy 0, policy_version 75055 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:49,276][03976] Updated weights for policy 0, policy_version 75065 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:50,944][03976] Updated weights for policy 0, policy_version 75075 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:45:51,167][03423] Fps is (10 sec: 49152.2, 60 sec: 48333.0, 300 sec: 48346.7). Total num frames: 595017728. Throughput: 0: 12099.1. Samples: 36220624. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:45:51,169][03423] Avg episode reward: [(0, '52.711')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:52,610][03976] Updated weights for policy 0, policy_version 75085 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:54,318][03976] Updated weights for policy 0, policy_version 75095 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:56,042][03976] Updated weights for policy 0, policy_version 75105 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:45:56,168][03423] Fps is (10 sec: 49151.1, 60 sec: 48469.2, 300 sec: 48346.7). Total num frames: 595263488. Throughput: 0: 12112.9. Samples: 36293008. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:45:56,168][03423] Avg episode reward: [(0, '55.105')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:57,683][03976] Updated weights for policy 0, policy_version 75115 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:45:59,367][03976] Updated weights for policy 0, policy_version 75125 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:01,030][03976] Updated weights for policy 0, policy_version 75135 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:46:01,168][03423] Fps is (10 sec: 48332.6, 60 sec: 48469.3, 300 sec: 48318.9). Total num frames: 595501056. Throughput: 0: 12155.7. Samples: 36367004. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:46:01,168][03423] Avg episode reward: [(0, '54.393')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:02,705][03976] Updated weights for policy 0, policy_version 75145 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:04,364][03976] Updated weights for policy 0, policy_version 75155 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:06,004][03976] Updated weights for policy 0, policy_version 75165 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:46:06,167][03423] Fps is (10 sec: 48333.5, 60 sec: 48469.3, 300 sec: 48346.7). Total num frames: 595746816. Throughput: 0: 12168.2. Samples: 36403780. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:46:06,168][03423] Avg episode reward: [(0, '53.915')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:06,193][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000075166_595755008.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:46:06,267][03956] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000073749_584146944.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:07,730][03976] Updated weights for policy 0, policy_version 75175 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:09,368][03976] Updated weights for policy 0, policy_version 75185 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:11,026][03976] Updated weights for policy 0, policy_version 75195 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:46:11,168][03423] Fps is (10 sec: 49152.0, 60 sec: 48605.8, 300 sec: 48374.5). Total num frames: 595992576. Throughput: 0: 12194.8. Samples: 36476956. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:46:11,168][03423] Avg episode reward: [(0, '54.709')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:12,719][03976] Updated weights for policy 0, policy_version 75205 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:14,380][03976] Updated weights for policy 0, policy_version 75215 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:16,066][03976] Updated weights for policy 0, policy_version 75225 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:46:16,167][03423] Fps is (10 sec: 49152.1, 60 sec: 48742.5, 300 sec: 48374.5). Total num frames: 596238336. Throughput: 0: 12197.4. Samples: 36550516. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:46:16,168][03423] Avg episode reward: [(0, '55.415')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:17,769][03976] Updated weights for policy 0, policy_version 75235 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:19,428][03976] Updated weights for policy 0, policy_version 75245 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:21,064][03976] Updated weights for policy 0, policy_version 75255 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:46:21,167][03423] Fps is (10 sec: 49152.4, 60 sec: 48742.6, 300 sec: 48374.6). Total num frames: 596484096. Throughput: 0: 12207.3. Samples: 36587116. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:46:21,169][03423] Avg episode reward: [(0, '55.659')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:22,753][03976] Updated weights for policy 0, policy_version 75265 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:24,462][03976] Updated weights for policy 0, policy_version 75275 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:26,120][03976] Updated weights for policy 0, policy_version 75285 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:46:26,167][03423] Fps is (10 sec: 49152.1, 60 sec: 48879.0, 300 sec: 48402.2). Total num frames: 596729856. Throughput: 0: 12201.9. Samples: 36660384. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:46:26,168][03423] Avg episode reward: [(0, '55.013')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:27,790][03976] Updated weights for policy 0, policy_version 75295 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:29,445][03976] Updated weights for policy 0, policy_version 75305 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:31,121][03976] Updated weights for policy 0, policy_version 75315 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:46:31,168][03423] Fps is (10 sec: 49151.7, 60 sec: 48878.9, 300 sec: 48402.2). Total num frames: 596975616. Throughput: 0: 12224.1. Samples: 36734112. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:46:31,168][03423] Avg episode reward: [(0, '57.652')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:32,743][03976] Updated weights for policy 0, policy_version 75325 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:34,379][03976] Updated weights for policy 0, policy_version 75335 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:36,065][03976] Updated weights for policy 0, policy_version 75345 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:46:36,168][03423] Fps is (10 sec: 49151.7, 60 sec: 48878.9, 300 sec: 48430.0). Total num frames: 597221376. Throughput: 0: 12234.8. Samples: 36771192. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:46:36,169][03423] Avg episode reward: [(0, '55.284')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:37,747][03976] Updated weights for policy 0, policy_version 75355 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:39,404][03976] Updated weights for policy 0, policy_version 75365 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:41,065][03976] Updated weights for policy 0, policy_version 75375 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:46:41,167][03423] Fps is (10 sec: 49152.1, 60 sec: 49015.5, 300 sec: 48457.8). Total num frames: 597467136. Throughput: 0: 12273.5. Samples: 36845312. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:46:41,168][03423] Avg episode reward: [(0, '56.245')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:42,723][03976] Updated weights for policy 0, policy_version 75385 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:44,445][03976] Updated weights for policy 0, policy_version 75395 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:46,114][03976] Updated weights for policy 0, policy_version 75405 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:46:46,168][03423] Fps is (10 sec: 49152.0, 60 sec: 49015.4, 300 sec: 48457.8). Total num frames: 597712896. Throughput: 0: 12262.5. Samples: 36918816. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:46:46,169][03423] Avg episode reward: [(0, '56.658')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:47,807][03976] Updated weights for policy 0, policy_version 75415 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:49,520][03976] Updated weights for policy 0, policy_version 75425 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:46:51,167][03423] Fps is (10 sec: 48333.0, 60 sec: 48879.0, 300 sec: 48457.8). Total num frames: 597950464. Throughput: 0: 12245.9. Samples: 36954844. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:46:51,168][03423] Avg episode reward: [(0, '54.088')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:51,193][03976] Updated weights for policy 0, policy_version 75435 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:52,896][03976] Updated weights for policy 0, policy_version 75445 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:54,558][03976] Updated weights for policy 0, policy_version 75455 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:46:56,168][03423] Fps is (10 sec: 48332.6, 60 sec: 48879.0, 300 sec: 48457.8). Total num frames: 598196224. Throughput: 0: 12242.4. Samples: 37027864. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:46:56,168][03423] Avg episode reward: [(0, '56.858')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:56,261][03976] Updated weights for policy 0, policy_version 75465 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:57,934][03976] Updated weights for policy 0, policy_version 75475 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:46:59,604][03976] Updated weights for policy 0, policy_version 75485 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:01,168][03423] Fps is (10 sec: 49151.7, 60 sec: 49015.5, 300 sec: 48485.5). Total num frames: 598441984. Throughput: 0: 12229.9. Samples: 37100860. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:01,169][03423] Avg episode reward: [(0, '54.779')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:01,344][03976] Updated weights for policy 0, policy_version 75495 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:03,061][03976] Updated weights for policy 0, policy_version 75505 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:04,790][03976] Updated weights for policy 0, policy_version 75515 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:06,168][03423] Fps is (10 sec: 48332.7, 60 sec: 48878.8, 300 sec: 48457.7). Total num frames: 598679552. Throughput: 0: 12209.4. Samples: 37136540. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:06,169][03423] Avg episode reward: [(0, '54.798')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:06,491][03976] Updated weights for policy 0, policy_version 75525 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:08,162][03976] Updated weights for policy 0, policy_version 75535 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:09,841][03976] Updated weights for policy 0, policy_version 75545 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:11,168][03423] Fps is (10 sec: 47513.6, 60 sec: 48742.4, 300 sec: 48430.0). Total num frames: 598917120. Throughput: 0: 12176.8. Samples: 37208340. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:11,169][03423] Avg episode reward: [(0, '53.910')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:11,513][03976] Updated weights for policy 0, policy_version 75555 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:13,245][03976] Updated weights for policy 0, policy_version 75565 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:14,961][03976] Updated weights for policy 0, policy_version 75575 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:16,168][03423] Fps is (10 sec: 48332.4, 60 sec: 48742.2, 300 sec: 48457.7). Total num frames: 599162880. Throughput: 0: 12151.4. Samples: 37280928. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:16,169][03423] Avg episode reward: [(0, '56.013')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:16,644][03976] Updated weights for policy 0, policy_version 75585 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:18,305][03976] Updated weights for policy 0, policy_version 75595 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:19,983][03976] Updated weights for policy 0, policy_version 75605 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:21,168][03423] Fps is (10 sec: 48332.7, 60 sec: 48605.8, 300 sec: 48430.0). Total num frames: 599400448. Throughput: 0: 12142.3. Samples: 37317596. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:21,168][03423] Avg episode reward: [(0, '57.751')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:21,660][03976] Updated weights for policy 0, policy_version 75615 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:23,323][03976] Updated weights for policy 0, policy_version 75625 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:24,984][03976] Updated weights for policy 0, policy_version 75635 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:26,167][03423] Fps is (10 sec: 49153.0, 60 sec: 48742.4, 300 sec: 48485.6). Total num frames: 599654400. Throughput: 0: 12122.4. Samples: 37390820. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:26,169][03423] Avg episode reward: [(0, '54.900')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:26,632][03976] Updated weights for policy 0, policy_version 75645 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:28,283][03976] Updated weights for policy 0, policy_version 75655 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:29,979][03976] Updated weights for policy 0, policy_version 75665 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:31,168][03423] Fps is (10 sec: 49971.3, 60 sec: 48742.4, 300 sec: 48485.6). Total num frames: 599900160. Throughput: 0: 12123.6. Samples: 37464376. Policy #0 lag: (min: 0.0, avg: 1.3, max: 3.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:31,169][03423] Avg episode reward: [(0, '56.118')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:31,677][03976] Updated weights for policy 0, policy_version 75675 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:33,340][03976] Updated weights for policy 0, policy_version 75685 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:33,504][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000075686_600014848.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,510][03956] Stopping Batcher_0...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,510][03956] Loop batcher_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,519][03423] Component Batcher_0 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,547][03423] Component RolloutWorker_w3 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,550][03423] Component RolloutWorker_w10 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,550][03986] Stopping RolloutWorker_w10...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,552][03423] Component RolloutWorker_w1 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,552][03986] Loop rollout_proc10_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,552][03981] Stopping RolloutWorker_w2...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,552][04006] Stopping RolloutWorker_w14...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,553][03981] Loop rollout_proc2_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,553][04006] Loop rollout_proc14_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,553][03423] Component RolloutWorker_w14 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,552][03978] Stopping RolloutWorker_w1...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,553][03978] Loop rollout_proc1_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,553][04007] Stopping RolloutWorker_w15...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,554][03423] Component RolloutWorker_w13 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,554][04007] Loop rollout_proc15_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,555][03423] Component RolloutWorker_w2 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,555][03423] Component RolloutWorker_w11 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,556][03423] Component RolloutWorker_w0 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,557][03423] Component RolloutWorker_w15 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,557][03423] Component RolloutWorker_w7 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,557][03984] Stopping RolloutWorker_w8...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,550][03979] Stopping RolloutWorker_w3...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,557][03423] Component RolloutWorker_w5 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,557][03984] Loop rollout_proc8_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,558][03979] Loop rollout_proc3_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,558][03423] Component RolloutWorker_w6 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,559][03423] Component RolloutWorker_w8 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,559][03423] Component RolloutWorker_w9 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,555][03987] Stopping RolloutWorker_w11...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,561][03987] Loop rollout_proc11_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,557][03985] Stopping RolloutWorker_w7...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,556][03977] Stopping RolloutWorker_w0...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,562][03985] Loop rollout_proc7_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,563][03977] Loop rollout_proc0_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,555][04004] Stopping RolloutWorker_w13...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,560][03988] Stopping RolloutWorker_w9...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,564][04004] Loop rollout_proc13_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,565][03988] Loop rollout_proc9_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,567][03423] Component RolloutWorker_w4 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,567][03980] Stopping RolloutWorker_w4...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,568][03980] Loop rollout_proc4_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,574][03956] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000074456_589938688.pth\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,558][03983] Stopping RolloutWorker_w5...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,576][03983] Loop rollout_proc5_evt_loop terminating...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:33,576][03956] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000075686_600014848.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,558][03982] Stopping RolloutWorker_w6...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,580][03982] Loop rollout_proc6_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,581][03423] Component RolloutWorker_w12 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,631][03976] Weights refcount: 2 0\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,632][03976] Stopping InferenceWorker_p0-w0...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,633][03423] Component InferenceWorker_p0-w0 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,633][03976] Loop inference_proc0-0_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,584][04005] Stopping RolloutWorker_w12...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,644][04005] Loop rollout_proc12_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,664][03423] Component LearnerWorker_p0 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,663][03956] Stopping LearnerWorker_p0...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,665][03423] Waiting for process learner_proc0 to stop...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:33,664][03956] Loop learner_proc0_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:34,939][03423] Waiting for process inference_proc0-0 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:34,940][03423] Waiting for process rollout_proc0 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:34,940][03423] Waiting for process rollout_proc1 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:34,941][03423] Waiting for process rollout_proc2 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:34,941][03423] Waiting for process rollout_proc3 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:34,941][03423] Waiting for process rollout_proc4 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:34,941][03423] Waiting for process rollout_proc5 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:34,942][03423] Waiting for process rollout_proc6 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:34,942][03423] Waiting for process rollout_proc7 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:35,050][03423] Waiting for process rollout_proc8 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:35,051][03423] Waiting for process rollout_proc9 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:35,052][03423] Waiting for process rollout_proc10 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:35,052][03423] Waiting for process rollout_proc11 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:35,052][03423] Waiting for process rollout_proc12 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:35,053][03423] Waiting for process rollout_proc13 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:35,053][03423] Waiting for process rollout_proc14 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:35,053][03423] Waiting for process rollout_proc15 to join...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:35,054][03423] Batcher 0 profile tree view:\n",
      "batching: 226.9723, releasing_batches: 0.4062\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:35,054][03423] InferenceWorker_p0-w0 profile tree view:\n",
      "wait_policy: 0.0000\n",
      "  wait_policy_total: 152.4061\n",
      "update_model: 47.2008\n",
      "  weight_update: 0.0007\n",
      "one_step: 0.0025\n",
      "  handle_policy_step: 2893.1792\n",
      "    deserialize: 233.1637, stack: 16.4005, obs_to_device_normalize: 684.9469, forward: 1336.5257, send_messages: 142.3971\n",
      "    prepare_outputs: 380.2794\n",
      "      to_cpu: 235.5221\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:35,055][03423] Learner 0 profile tree view:\n",
      "misc: 0.0765, prepare_batch: 301.8805\n",
      "train: 709.7991\n",
      "  epoch_init: 0.0606, minibatch_init: 0.0816, losses_postprocess: 3.1675, kl_divergence: 3.7417, after_optimizer: 2.9246\n",
      "  calculate_losses: 277.5781\n",
      "    losses_init: 0.0306, forward_head: 10.9008, bptt_initial: 226.0494, tail: 8.6468, advantages_returns: 2.4628, losses: 12.7700\n",
      "    bptt: 13.7327\n",
      "      bptt_forward_core: 13.0757\n",
      "  update: 416.4638\n",
      "    clip: 11.6032\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:35,055][03423] RolloutWorker_w0 profile tree view:\n",
      "wait_for_trajectories: 1.4183, enqueue_policy_requests: 94.3193, env_step: 1551.9875, overhead: 157.4841, complete_rollouts: 3.3137\n",
      "save_policy_outputs: 119.0424\n",
      "  split_output_tensors: 55.2342\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:35,055][03423] RolloutWorker_w15 profile tree view:\n",
      "wait_for_trajectories: 1.4423, enqueue_policy_requests: 96.5808, env_step: 1554.5923, overhead: 156.0953, complete_rollouts: 3.6541\n",
      "save_policy_outputs: 117.3319\n",
      "  split_output_tensors: 55.3652\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:47:35,056][03423] Loop Runner_EvtLoop terminating...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:35,057][03423] Runner profile tree view:\n",
      "main_loop: 3202.8765\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:47:35,057][03423] Collected {0: 600014848}, FPS: 46834.1\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Start the training, this should take around 15 minutes\n",
    "register_vizdoom_components()\n",
    "\n",
    "# The scenario we train on today is health gathering\n",
    "# other scenarios include \"doom_basic\", \"doom_two_colors_easy\", \"doom_dm\", \"doom_dwango5\", \"doom_my_way_home\", \"doom_deadly_corridor\", \"doom_defend_the_center\", \"doom_defend_the_line\"\n",
    "env = \"doom_health_gathering_supreme\"\n",
    "cfg = parse_vizdoom_cfg(\n",
    "    argv=[f\"--env={env}\", \n",
    "          \"--seed=200\",\n",
    "          \"--num_workers=16\", \n",
    "          \"--num_envs_per_worker=8\", \n",
    "          \"--batch_size=2048\",\n",
    "          \"--train_for_env_steps=600000000\"]\n",
    ")\n",
    "\n",
    "# status = run_rl(cfg)\n",
    "run_rl(cfg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2024-07-05 15:48:15,708][03423] Loading existing experiment configuration from /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/config.json\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:15,709][03423] Overriding arg 'num_workers' with value 1 passed from command line\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:15,709][03423] Adding new argument 'no_render'=True that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:15,710][03423] Adding new argument 'save_video'=True that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:15,710][03423] Adding new argument 'video_frames'=1000000000.0 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:15,710][03423] Adding new argument 'video_name'=None that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:15,711][03423] Adding new argument 'max_num_frames'=1000000000.0 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:15,711][03423] Adding new argument 'max_num_episodes'=10 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:15,711][03423] Adding new argument 'push_to_hub'=False that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:15,711][03423] Adding new argument 'hf_repository'=None that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:15,712][03423] Adding new argument 'policy_index'=0 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:15,712][03423] Adding new argument 'eval_deterministic'=False that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:15,712][03423] Adding new argument 'train_script'=None that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:15,713][03423] Adding new argument 'enjoy_script'=None that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:15,713][03423] Using frameskip 1 and render_action_repeat=4 for evaluation\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:15,746][03423] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-05 15:48:15,749][03423] RunningMeanStd input shape: (3, 72, 128)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:15,754][03423] RunningMeanStd input shape: (1,)\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:15,786][03423] ConvEncoder: input_channels=3\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:15,924][03423] Conv encoder output size: 512\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:15,924][03423] Policy head output size: 512\u001b[0m\n",
      "\u001b[33m[2024-07-05 15:48:17,649][03423] Loading state from checkpoint /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/checkpoint_p0/checkpoint_000075686_600014848.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:18,496][03423] Num frames 100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:18,562][03423] Num frames 200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:18,626][03423] Num frames 300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:18,690][03423] Num frames 400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:18,757][03423] Num frames 500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:18,819][03423] Num frames 600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:18,882][03423] Num frames 700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:18,949][03423] Num frames 800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:19,013][03423] Num frames 900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:19,075][03423] Num frames 1000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:19,139][03423] Num frames 1100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:19,203][03423] Num frames 1200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:19,267][03423] Num frames 1300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:19,331][03423] Num frames 1400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:19,394][03423] Num frames 1500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:19,457][03423] Num frames 1600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:19,522][03423] Num frames 1700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:19,585][03423] Num frames 1800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:19,650][03423] Num frames 1900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:19,711][03423] Num frames 2000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:19,774][03423] Num frames 2100...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:48:19,826][03423] Avg episode rewards: #0: 60.999, true rewards: #0: 21.000\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:48:19,827][03423] Avg episode reward: 60.999, avg true_objective: 21.000\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:19,891][03423] Num frames 2200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:19,953][03423] Num frames 2300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:20,013][03423] Num frames 2400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:20,074][03423] Num frames 2500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:20,135][03423] Num frames 2600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:20,198][03423] Num frames 2700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:20,258][03423] Num frames 2800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:20,319][03423] Num frames 2900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:20,381][03423] Num frames 3000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:20,444][03423] Num frames 3100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:20,506][03423] Num frames 3200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:20,567][03423] Num frames 3300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:20,629][03423] Num frames 3400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:20,693][03423] Num frames 3500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:20,756][03423] Num frames 3600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:20,819][03423] Num frames 3700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:20,882][03423] Num frames 3800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:20,945][03423] Num frames 3900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:21,008][03423] Num frames 4000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:21,071][03423] Num frames 4100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:21,136][03423] Num frames 4200...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:48:21,187][03423] Avg episode rewards: #0: 60.499, true rewards: #0: 21.000\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:48:21,188][03423] Avg episode reward: 60.499, avg true_objective: 21.000\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:21,255][03423] Num frames 4300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:21,317][03423] Num frames 4400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:21,380][03423] Num frames 4500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:21,442][03423] Num frames 4600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:21,505][03423] Num frames 4700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:21,567][03423] Num frames 4800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:21,631][03423] Num frames 4900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:21,694][03423] Num frames 5000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:21,755][03423] Num frames 5100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:21,818][03423] Num frames 5200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:21,883][03423] Num frames 5300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:21,944][03423] Num frames 5400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:22,009][03423] Num frames 5500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:22,073][03423] Num frames 5600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:22,133][03423] Num frames 5700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:22,212][03423] Num frames 5800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:22,273][03423] Num frames 5900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:22,334][03423] Num frames 6000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:22,397][03423] Num frames 6100...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:48:22,462][03423] Avg episode rewards: #0: 60.065, true rewards: #0: 20.400\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:48:22,463][03423] Avg episode reward: 60.065, avg true_objective: 20.400\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:22,517][03423] Num frames 6200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:22,578][03423] Num frames 6300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:22,640][03423] Num frames 6400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:22,702][03423] Num frames 6500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:22,766][03423] Num frames 6600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:22,828][03423] Num frames 6700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:22,892][03423] Num frames 6800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:22,953][03423] Num frames 6900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:23,014][03423] Num frames 7000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:23,078][03423] Num frames 7100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:23,141][03423] Num frames 7200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:23,205][03423] Num frames 7300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:23,269][03423] Num frames 7400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:23,329][03423] Num frames 7500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:23,394][03423] Num frames 7600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:23,457][03423] Num frames 7700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:23,523][03423] Num frames 7800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:23,589][03423] Num frames 7900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:23,652][03423] Num frames 8000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:23,714][03423] Num frames 8100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:23,778][03423] Num frames 8200...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:48:23,844][03423] Avg episode rewards: #0: 60.549, true rewards: #0: 20.550\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:48:23,845][03423] Avg episode reward: 60.549, avg true_objective: 20.550\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:23,899][03423] Num frames 8300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:23,958][03423] Num frames 8400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:24,021][03423] Num frames 8500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:24,084][03423] Num frames 8600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:24,143][03423] Num frames 8700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:24,206][03423] Num frames 8800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:24,269][03423] Num frames 8900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:24,332][03423] Num frames 9000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:24,396][03423] Num frames 9100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:24,461][03423] Num frames 9200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:24,523][03423] Num frames 9300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:24,594][03423] Num frames 9400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:24,657][03423] Num frames 9500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:24,718][03423] Num frames 9600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:24,780][03423] Num frames 9700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:24,842][03423] Num frames 9800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:24,903][03423] Num frames 9900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:24,966][03423] Num frames 10000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:25,030][03423] Num frames 10100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:25,094][03423] Num frames 10200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:25,158][03423] Num frames 10300...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:48:25,225][03423] Avg episode rewards: #0: 60.439, true rewards: #0: 20.640\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:48:25,226][03423] Avg episode reward: 60.439, avg true_objective: 20.640\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:25,280][03423] Num frames 10400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:25,341][03423] Num frames 10500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:25,413][03423] Num frames 10600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:25,483][03423] Num frames 10700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:25,546][03423] Num frames 10800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:25,609][03423] Num frames 10900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:25,675][03423] Num frames 11000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:25,739][03423] Num frames 11100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:25,801][03423] Num frames 11200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:25,867][03423] Num frames 11300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:25,931][03423] Num frames 11400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:25,992][03423] Num frames 11500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:26,056][03423] Num frames 11600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:26,124][03423] Num frames 11700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:26,187][03423] Num frames 11800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:26,247][03423] Num frames 11900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:26,311][03423] Num frames 12000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:26,375][03423] Num frames 12100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:26,438][03423] Num frames 12200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:26,501][03423] Num frames 12300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:26,564][03423] Num frames 12400...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:48:26,630][03423] Avg episode rewards: #0: 60.865, true rewards: #0: 20.700\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:48:26,632][03423] Avg episode reward: 60.865, avg true_objective: 20.700\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:26,689][03423] Num frames 12500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:26,750][03423] Num frames 12600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:26,812][03423] Num frames 12700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:26,884][03423] Num frames 12800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:26,949][03423] Num frames 12900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:27,011][03423] Num frames 13000...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:48:27,121][03423] Avg episode rewards: #0: 54.130, true rewards: #0: 18.703\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:48:27,122][03423] Avg episode reward: 54.130, avg true_objective: 18.703\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:27,131][03423] Num frames 13100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:27,195][03423] Num frames 13200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:27,258][03423] Num frames 13300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:27,321][03423] Num frames 13400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:27,385][03423] Num frames 13500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:27,446][03423] Num frames 13600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:27,518][03423] Num frames 13700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:27,581][03423] Num frames 13800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:27,643][03423] Num frames 13900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:27,708][03423] Num frames 14000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:27,769][03423] Num frames 14100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:27,834][03423] Num frames 14200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:27,896][03423] Num frames 14300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:27,958][03423] Num frames 14400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:28,022][03423] Num frames 14500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:28,085][03423] Num frames 14600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:28,153][03423] Num frames 14700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:28,218][03423] Num frames 14800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:28,281][03423] Num frames 14900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:28,343][03423] Num frames 15000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:28,408][03423] Num frames 15100...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:48:28,521][03423] Avg episode rewards: #0: 54.864, true rewards: #0: 18.990\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:48:28,522][03423] Avg episode reward: 54.864, avg true_objective: 18.990\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:28,532][03423] Num frames 15200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:28,607][03423] Num frames 15300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:28,668][03423] Num frames 15400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:28,729][03423] Num frames 15500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:28,791][03423] Num frames 15600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:28,849][03423] Num frames 15700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:28,911][03423] Num frames 15800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:28,973][03423] Num frames 15900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:29,035][03423] Num frames 16000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:29,095][03423] Num frames 16100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:29,155][03423] Num frames 16200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:29,220][03423] Num frames 16300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:29,282][03423] Num frames 16400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:29,343][03423] Num frames 16500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:29,404][03423] Num frames 16600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:29,465][03423] Num frames 16700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:29,526][03423] Num frames 16800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:29,591][03423] Num frames 16900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:29,653][03423] Num frames 17000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:29,715][03423] Num frames 17100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:29,777][03423] Num frames 17200...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:48:29,887][03423] Avg episode rewards: #0: 55.768, true rewards: #0: 19.213\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:48:29,888][03423] Avg episode reward: 55.768, avg true_objective: 19.213\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:29,898][03423] Num frames 17300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:29,961][03423] Num frames 17400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:30,022][03423] Num frames 17500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:30,082][03423] Num frames 17600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:30,140][03423] Num frames 17700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:30,200][03423] Num frames 17800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:30,259][03423] Num frames 17900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:30,322][03423] Num frames 18000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:30,383][03423] Num frames 18100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:30,444][03423] Num frames 18200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:30,503][03423] Num frames 18300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:30,564][03423] Num frames 18400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:30,623][03423] Num frames 18500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:30,684][03423] Num frames 18600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:30,744][03423] Num frames 18700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:30,807][03423] Num frames 18800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:30,868][03423] Num frames 18900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:30,929][03423] Num frames 19000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:30,991][03423] Num frames 19100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:31,053][03423] Num frames 19200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 15:48:31,115][03423] Num frames 19300...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:48:31,222][03423] Avg episode rewards: #0: 55.891, true rewards: #0: 19.392\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 15:48:31,223][03423] Avg episode reward: 55.891, avg true_objective: 19.392\u001b[0m\n",
      "ffmpeg version 9c33b2f Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 9.3.0 (crosstool-NG 1.24.0.133_b0863d8_dirty)\n",
      "  configuration: --prefix=/home/conda/feedstock_root/build_artifacts/ffmpeg_1627813612080/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1627813612080/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-gnutls --enable-gpl --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-libx264 --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1627813612080/_build_env/bin/pkg-config\n",
      "  libavutil      56. 51.100 / 56. 51.100\n",
      "  libavcodec     58. 91.100 / 58. 91.100\n",
      "  libavformat    58. 45.100 / 58. 45.100\n",
      "  libavdevice    58. 10.100 / 58. 10.100\n",
      "  libavfilter     7. 85.100 /  7. 85.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  7.100 /  5.  7.100\n",
      "  libswresample   3.  7.100 /  3.  7.100\n",
      "  libpostproc    55.  7.100 / 55.  7.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/tmp/sf2_raghu/replay.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf59.27.100\n",
      "  Duration: 00:09:14.34, start: 0.000000, bitrate: 1850 kb/s\n",
      "    Stream #0:0(und): Video: mpeg4 (Simple Profile) (mp4v / 0x7634706D), yuv420p, 240x180 [SAR 1:1 DAR 4:3], 1849 kb/s, 35 fps, 35 tbr, 17920 tbn, 35 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mpeg4 (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x61c70f69b400] using SAR=1/1\n",
      "[libx264 @ 0x61c70f69b400] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
      "[libx264 @ 0x61c70f69b400] profile High, level 1.3, 4:2:0, 8-bit\n",
      "[libx264 @ 0x61c70f69b400] 264 - core 161 r3030M 8bd6d28 - H.264/MPEG-4 AVC codec - Copyleft 2003-2020 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to '/home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/replay.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf58.45.100\n",
      "    Stream #0:0(und): Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 240x180 [SAR 1:1 DAR 4:3], q=-1--1, 35 fps, 17920 tbn, 35 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      encoder         : Lavc58.91.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "frame=19402 fps=1380 q=-1.0 Lsize=   37976kB time=00:09:14.25 bitrate= 561.3kbits/s speed=39.4x    \n",
      "video:37763kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.566233%\n",
      "[libx264 @ 0x61c70f69b400] frame I:180   Avg QP:23.76  size:  5030\n",
      "[libx264 @ 0x61c70f69b400] frame P:7149  Avg QP:26.76  size:  2496\n",
      "[libx264 @ 0x61c70f69b400] frame B:12073 Avg QP:28.29  size:  1650\n",
      "[libx264 @ 0x61c70f69b400] consecutive B-frames: 14.3%  6.3%  5.5% 73.9%\n",
      "[libx264 @ 0x61c70f69b400] mb I  I16..4: 16.8% 69.7% 13.5%\n",
      "[libx264 @ 0x61c70f69b400] mb P  I16..4:  3.8% 18.0%  5.4%  P16..4: 33.5% 23.9% 10.4%  0.0%  0.0%    skip: 4.9%\n",
      "[libx264 @ 0x61c70f69b400] mb B  I16..4:  0.2%  5.2%  2.8%  B16..8: 37.7% 20.7%  6.7%  direct: 7.8%  skip:18.8%  L0:50.7% L1:34.8% BI:14.5%\n",
      "[libx264 @ 0x61c70f69b400] 8x8 transform intra:65.5% inter:62.7%\n",
      "[libx264 @ 0x61c70f69b400] coded y,uvDC,uvAC intra: 68.6% 70.7% 36.0% inter: 45.7% 18.0% 4.2%\n",
      "[libx264 @ 0x61c70f69b400] i16 v,h,dc,p: 65%  4% 30%  1%\n",
      "[libx264 @ 0x61c70f69b400] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 21% 10% 32%  5%  6%  5%  8%  5%  8%\n",
      "[libx264 @ 0x61c70f69b400] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 59%  9% 10%  3%  4%  3%  5%  3%  4%\n",
      "[libx264 @ 0x61c70f69b400] i8c dc,h,v,p: 55% 20% 22%  3%\n",
      "[libx264 @ 0x61c70f69b400] Weighted P-Frames: Y:6.4% UV:0.4%\n",
      "[libx264 @ 0x61c70f69b400] ref P L0: 60.1% 13.4% 16.4%  9.5%  0.6%\n",
      "[libx264 @ 0x61c70f69b400] ref B L0: 88.7%  8.5%  2.8%\n",
      "[libx264 @ 0x61c70f69b400] ref B L1: 96.0%  4.0%\n",
      "[libx264 @ 0x61c70f69b400] kb/s:558.04\n",
      "\u001b[36m[2024-07-05 15:48:51,264][03423] Replay video saved to /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/default_experiment/replay.mp4!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sample_factory.enjoy import enjoy\n",
    "\n",
    "cfg = parse_vizdoom_cfg(\n",
    "    argv=[f\"--env={env}\", \n",
    "          \"--num_workers=1\", \n",
    "          \"--save_video\", \n",
    "          \"--no_render\", \n",
    "          \"--max_num_episodes=10\"], evaluation=True\n",
    ")\n",
    "status = enjoy(cfg)\n",
    "\n",
    "# 20000000: 11.939\n",
    "# 30000000: 15.516\n",
    "# 50000000: 15.954\n",
    "# 70000000: 16.362\n",
    "# 100000000: 18.66\n",
    "# 150000000: 16.279\n",
    "# 200000000: 19.264\n",
    "# 250000000: 19.969\n",
    "# 300000000: 17.653\n",
    "# 350000000: 20.917\n",
    "# 400000000: 18.08\n",
    "# 450000000: 18.223\n",
    "# 600000000: 19.392\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convnet_impala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2024-07-04 23:44:42,771][40844] Environment doom_basic already registered, overwriting...\u001b[0m\n",
      "\u001b[33m[2024-07-04 23:44:42,772][40844] Environment doom_two_colors_easy already registered, overwriting...\u001b[0m\n",
      "\u001b[33m[2024-07-04 23:44:42,773][40844] Environment doom_two_colors_hard already registered, overwriting...\u001b[0m\n",
      "\u001b[33m[2024-07-04 23:44:42,773][40844] Environment doom_dm already registered, overwriting...\u001b[0m\n",
      "\u001b[33m[2024-07-04 23:44:42,773][40844] Environment doom_dwango5 already registered, overwriting...\u001b[0m\n",
      "\u001b[33m[2024-07-04 23:44:42,773][40844] Environment doom_my_way_home_flat_actions already registered, overwriting...\u001b[0m\n",
      "\u001b[33m[2024-07-04 23:44:42,774][40844] Environment doom_defend_the_center_flat_actions already registered, overwriting...\u001b[0m\n",
      "\u001b[33m[2024-07-04 23:44:42,774][40844] Environment doom_my_way_home already registered, overwriting...\u001b[0m\n",
      "\u001b[33m[2024-07-04 23:44:42,774][40844] Environment doom_deadly_corridor already registered, overwriting...\u001b[0m\n",
      "\u001b[33m[2024-07-04 23:44:42,775][40844] Environment doom_defend_the_center already registered, overwriting...\u001b[0m\n",
      "\u001b[33m[2024-07-04 23:44:42,775][40844] Environment doom_defend_the_line already registered, overwriting...\u001b[0m\n",
      "\u001b[33m[2024-07-04 23:44:42,775][40844] Environment doom_health_gathering already registered, overwriting...\u001b[0m\n",
      "\u001b[33m[2024-07-04 23:44:42,776][40844] Environment doom_health_gathering_supreme already registered, overwriting...\u001b[0m\n",
      "\u001b[33m[2024-07-04 23:44:42,776][40844] Environment doom_battle already registered, overwriting...\u001b[0m\n",
      "\u001b[33m[2024-07-04 23:44:42,776][40844] Environment doom_battle2 already registered, overwriting...\u001b[0m\n",
      "\u001b[33m[2024-07-04 23:44:42,776][40844] Environment doom_duel_bots already registered, overwriting...\u001b[0m\n",
      "\u001b[33m[2024-07-04 23:44:42,777][40844] Environment doom_deathmatch_bots already registered, overwriting...\u001b[0m\n",
      "\u001b[33m[2024-07-04 23:44:42,777][40844] Environment doom_duel already registered, overwriting...\u001b[0m\n",
      "\u001b[33m[2024-07-04 23:44:42,777][40844] Environment doom_deathmatch_full already registered, overwriting...\u001b[0m\n",
      "\u001b[33m[2024-07-04 23:44:42,777][40844] Environment doom_benchmark already registered, overwriting...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:42,778][40844] register_encoder_factory: <function make_vizdoom_encoder at 0x756ba1a56560>\u001b[0m\n",
      "\u001b[33m[2024-07-04 23:44:42,784][40844] Saved parameter configuration for experiment conv_impala not found!\u001b[0m\n",
      "\u001b[33m[2024-07-04 23:44:42,785][40844] Starting experiment from scratch!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:42,791][40844] Experiment dir /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_impala already exists!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:42,792][40844] Resuming existing experiment from /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_impala...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:42,792][40844] Weights and Biases integration disabled\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:42,794][40844] Environment var CUDA_VISIBLE_DEVICES is 0\n",
      "\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:45,287][41868] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-04 23:44:45,288][41868] Env info: EnvInfo(obs_space=Dict('obs': Box(0, 255, (3, 72, 128), uint8)), action_space=Discrete(5), num_agents=1, gpu_actions=False, gpu_observations=True, action_splits=None, all_discrete=None, frameskip=4, reward_shaping_scheme=None, env_info_protocol_version=1)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:45,765][40844] Automatically setting recurrence to 32\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:45,766][40844] Starting experiment with the following configuration:\n",
      "help=False\n",
      "algo=APPO\n",
      "env=doom_health_gathering_supreme\n",
      "experiment=conv_impala\n",
      "train_dir=/home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir\n",
      "restart_behavior=resume\n",
      "device=gpu\n",
      "seed=200\n",
      "num_policies=1\n",
      "async_rl=True\n",
      "serial_mode=False\n",
      "batched_sampling=False\n",
      "num_batches_to_accumulate=2\n",
      "worker_num_splits=2\n",
      "policy_workers_per_policy=1\n",
      "max_policy_lag=1000\n",
      "num_workers=8\n",
      "num_envs_per_worker=4\n",
      "batch_size=1024\n",
      "num_batches_per_epoch=1\n",
      "num_epochs=1\n",
      "rollout=32\n",
      "recurrence=32\n",
      "shuffle_minibatches=False\n",
      "gamma=0.99\n",
      "reward_scale=1.0\n",
      "reward_clip=1000.0\n",
      "value_bootstrap=False\n",
      "normalize_returns=True\n",
      "exploration_loss_coeff=0.001\n",
      "value_loss_coeff=0.5\n",
      "kl_loss_coeff=0.0\n",
      "exploration_loss=symmetric_kl\n",
      "gae_lambda=0.95\n",
      "ppo_clip_ratio=0.1\n",
      "ppo_clip_value=0.2\n",
      "with_vtrace=False\n",
      "vtrace_rho=1.0\n",
      "vtrace_c=1.0\n",
      "optimizer=adam\n",
      "adam_eps=1e-06\n",
      "adam_beta1=0.9\n",
      "adam_beta2=0.999\n",
      "max_grad_norm=4.0\n",
      "learning_rate=0.0001\n",
      "lr_schedule=constant\n",
      "lr_schedule_kl_threshold=0.008\n",
      "lr_adaptive_min=1e-06\n",
      "lr_adaptive_max=0.01\n",
      "obs_subtract_mean=0.0\n",
      "obs_scale=255.0\n",
      "normalize_input=True\n",
      "normalize_input_keys=None\n",
      "decorrelate_experience_max_seconds=0\n",
      "decorrelate_envs_on_one_worker=True\n",
      "actor_worker_gpus=[]\n",
      "set_workers_cpu_affinity=True\n",
      "force_envs_single_thread=False\n",
      "default_niceness=0\n",
      "log_to_file=True\n",
      "experiment_summaries_interval=10\n",
      "flush_summaries_interval=30\n",
      "stats_avg=100\n",
      "summaries_use_frameskip=True\n",
      "heartbeat_interval=20\n",
      "heartbeat_reporting_interval=600\n",
      "train_for_env_steps=5000000\n",
      "train_for_seconds=10000000000\n",
      "save_every_sec=120\n",
      "keep_checkpoints=2\n",
      "load_checkpoint_kind=latest\n",
      "save_milestones_sec=-1\n",
      "save_best_every_sec=5\n",
      "save_best_metric=reward\n",
      "save_best_after=100000\n",
      "benchmark=False\n",
      "encoder_mlp_layers=[512, 512]\n",
      "encoder_conv_architecture=convnet_impala\n",
      "encoder_conv_mlp_layers=[512]\n",
      "use_rnn=True\n",
      "rnn_size=512\n",
      "rnn_type=gru\n",
      "rnn_num_layers=1\n",
      "decoder_mlp_layers=[]\n",
      "nonlinearity=elu\n",
      "policy_initialization=orthogonal\n",
      "policy_init_gain=1.0\n",
      "actor_critic_share_weights=True\n",
      "adaptive_stddev=True\n",
      "continuous_tanh_scale=0.0\n",
      "initial_stddev=1.0\n",
      "use_env_info_cache=False\n",
      "env_gpu_actions=False\n",
      "env_gpu_observations=True\n",
      "env_frameskip=4\n",
      "env_framestack=1\n",
      "pixel_format=CHW\n",
      "use_record_episode_statistics=False\n",
      "with_wandb=False\n",
      "wandb_user=None\n",
      "wandb_project=sample_factory\n",
      "wandb_group=None\n",
      "wandb_job_type=SF\n",
      "wandb_tags=[]\n",
      "with_pbt=False\n",
      "pbt_mix_policies_in_one_env=True\n",
      "pbt_period_env_steps=5000000\n",
      "pbt_start_mutation=20000000\n",
      "pbt_replace_fraction=0.3\n",
      "pbt_mutation_rate=0.15\n",
      "pbt_replace_reward_gap=0.1\n",
      "pbt_replace_reward_gap_absolute=1e-06\n",
      "pbt_optimize_gamma=False\n",
      "pbt_target_objective=true_objective\n",
      "pbt_perturb_min=1.1\n",
      "pbt_perturb_max=1.5\n",
      "num_agents=-1\n",
      "num_humans=0\n",
      "num_bots=-1\n",
      "start_bot_difficulty=None\n",
      "timelimit=None\n",
      "res_w=128\n",
      "res_h=72\n",
      "wide_aspect_ratio=False\n",
      "eval_env_frameskip=1\n",
      "fps=35\n",
      "command_line=--env=doom_health_gathering_supreme --experiment=conv_impala --seed=200 --num_workers=8 --num_envs_per_worker=4 --batch_size=1024 --encoder_conv_architecture=convnet_impala --train_for_env_steps=5000000\n",
      "cli_args={'env': 'doom_health_gathering_supreme', 'experiment': 'conv_impala', 'seed': 200, 'num_workers': 8, 'num_envs_per_worker': 4, 'batch_size': 1024, 'train_for_env_steps': 5000000, 'encoder_conv_architecture': 'convnet_impala'}\n",
      "git_hash=unknown\n",
      "git_repo_name=not a git repository\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:45,767][40844] Saving configuration to /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_impala/config.json...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:45,768][40844] Rollout worker 0 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:45,768][40844] Rollout worker 1 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:45,769][40844] Rollout worker 2 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:45,769][40844] Rollout worker 3 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:45,769][40844] Rollout worker 4 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:45,769][40844] Rollout worker 5 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:45,770][40844] Rollout worker 6 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:45,770][40844] Rollout worker 7 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:45,804][40844] Using GPUs [0] for process 0 (actually maps to GPUs [0])\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:45,804][40844] InferenceWorker_p0-w0: min num requests: 2\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:45,831][40844] Starting all processes...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:45,832][40844] Starting process learner_proc0\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:45,881][40844] Starting all processes...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:45,884][40844] Starting process inference_proc0-0\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:45,885][40844] Starting process rollout_proc0\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:45,885][40844] Starting process rollout_proc1\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:45,886][40844] Starting process rollout_proc2\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:45,886][40844] Starting process rollout_proc3\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:45,886][40844] Starting process rollout_proc4\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:45,886][40844] Starting process rollout_proc5\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:45,886][40844] Starting process rollout_proc6\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:45,891][40844] Starting process rollout_proc7\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:48,380][41914] LearnerWorker_p0\tpid 41914\tparent 40844\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:48,381][41914] Using GPUs [0] for process 0 (actually maps to GPUs [0])\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:48,382][41914] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for learning process 0\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:48,473][41933] Rollout worker 7 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:48,473][41933] ROLLOUT worker 7\tpid 41933\tparent 40844\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:48,473][41933] Worker 7 uses CPU cores [14, 15]\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:48,538][41935] Rollout worker 6 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:48,538][41935] ROLLOUT worker 6\tpid 41935\tparent 40844\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:48,538][41935] Worker 6 uses CPU cores [12, 13]\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:48,566][41928] Rollout worker 0 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:48,566][41928] ROLLOUT worker 0\tpid 41928\tparent 40844\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:48,567][41928] Worker 0 uses CPU cores [0, 1]\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:48,587][41932] Rollout worker 4 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:48,587][41932] ROLLOUT worker 4\tpid 41932\tparent 40844\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:48,587][41932] Worker 4 uses CPU cores [8, 9]\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:48,598][41930] Rollout worker 2 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:48,598][41930] ROLLOUT worker 2\tpid 41930\tparent 40844\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:48,599][41930] Worker 2 uses CPU cores [4, 5]\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:48,846][41934] Rollout worker 5 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:48,847][41934] ROLLOUT worker 5\tpid 41934\tparent 40844\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:48,847][41934] Worker 5 uses CPU cores [10, 11]\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:48,850][41931] Rollout worker 3 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:48,850][41931] ROLLOUT worker 3\tpid 41931\tparent 40844\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:48,851][41931] Worker 3 uses CPU cores [6, 7]\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:48,889][41929] Rollout worker 1 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:48,889][41929] ROLLOUT worker 1\tpid 41929\tparent 40844\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:48,889][41929] Worker 1 uses CPU cores [2, 3]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:48,959][41927] InferenceWorker_p0-w0\tpid 41927\tparent 40844\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:48,960][41927] Using GPUs [0] for process 0 (actually maps to GPUs [0])\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:48,960][41927] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for inference process 0\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:49,992][41914] Num visible devices: 1\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:49,992][41927] Num visible devices: 1\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:50,018][41914] Setting fixed seed 200\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:50,019][41914] Using GPUs [0] for process 0 (actually maps to GPUs [0])\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:50,019][41914] Initializing actor-critic model on device cuda:0\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:50,020][41914] RunningMeanStd input shape: (3, 72, 128)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:50,020][41914] RunningMeanStd input shape: (1,)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:50,027][41914] ConvEncoder: input_channels=3\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:50,079][41914] Conv encoder output size: 512\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:50,080][41914] Policy head output size: 512\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:50,088][41914] Created Actor Critic model with architecture:\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:50,088][41914] ActorCriticSharedWeights(\n",
      "  (obs_normalizer): ObservationNormalizer(\n",
      "    (running_mean_std): RunningMeanStdDictInPlace(\n",
      "      (running_mean_std): ModuleDict(\n",
      "        (obs): RunningMeanStdInPlace()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)\n",
      "  (encoder): VizdoomEncoder(\n",
      "    (basic_encoder): ConvEncoder(\n",
      "      (enc): RecursiveScriptModule(\n",
      "        original_name=ConvEncoderImpl\n",
      "        (conv_head): RecursiveScriptModule(\n",
      "          original_name=Sequential\n",
      "          (0): RecursiveScriptModule(original_name=Conv2d)\n",
      "          (1): RecursiveScriptModule(original_name=ELU)\n",
      "          (2): RecursiveScriptModule(original_name=Conv2d)\n",
      "          (3): RecursiveScriptModule(original_name=ELU)\n",
      "        )\n",
      "        (mlp_layers): RecursiveScriptModule(\n",
      "          original_name=Sequential\n",
      "          (0): RecursiveScriptModule(original_name=Linear)\n",
      "          (1): RecursiveScriptModule(original_name=ELU)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (core): ModelCoreRNN(\n",
      "    (core): GRU(512, 512)\n",
      "  )\n",
      "  (decoder): MlpDecoder(\n",
      "    (mlp): Identity()\n",
      "  )\n",
      "  (critic_linear): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (action_parameterization): ActionParameterizationDefault(\n",
      "    (distribution_linear): Linear(in_features=512, out_features=5, bias=True)\n",
      "  )\n",
      ")\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:50,177][41914] Using optimizer <class 'torch.optim.adam.Adam'>\u001b[0m\n",
      "\u001b[33m[2024-07-04 23:44:50,752][41914] No checkpoints found\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:50,752][41914] Did not load from checkpoint, starting from scratch!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:50,752][41914] Initialized policy 0 weights for model version 0\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:50,753][41914] LearnerWorker_p0 finished initialization!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:50,754][41914] Using GPUs [0] for process 0 (actually maps to GPUs [0])\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:50,813][41927] RunningMeanStd input shape: (3, 72, 128)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:50,813][41927] RunningMeanStd input shape: (1,)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:50,821][41927] ConvEncoder: input_channels=3\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:50,872][41927] Conv encoder output size: 512\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:50,873][41927] Policy head output size: 512\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:50,906][40844] Inference worker 0-0 is ready!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:50,907][40844] All inference workers are ready! Signal rollout workers to start!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:50,932][41931] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:50,933][41930] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-04 23:44:50,938][41928] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-04 23:44:50,940][41933] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:50,940][41935] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-04 23:44:50,946][41932] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-04 23:44:50,948][41929] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-04 23:44:50,950][41934] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:51,430][41930] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:51,433][41931] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:51,434][41933] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:51,435][41928] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:51,435][41932] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:51,435][41929] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:51,581][41930] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:51,581][41931] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:51,583][41933] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:51,584][41934] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:51,628][41935] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:51,752][41929] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:51,752][41928] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:51,753][41934] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:51,769][41932] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:51,795][41930] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:51,799][41933] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:51,942][41931] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:51,942][41935] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:51,953][41934] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:51,955][41929] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:51,958][41928] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:52,124][41931] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:52,125][41930] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:52,130][41932] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:52,134][41934] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:52,134][41929] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:52,144][41935] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:52,288][41928] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:52,311][41932] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:52,321][41935] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:52,347][41933] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:52,794][40844] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 0. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:52,795][40844] Avg episode reward: [(0, '1.492')]\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:52,854][41914] Signal inference workers to stop experience collection...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:52,863][41927] InferenceWorker_p0-w0: stopping experience collection\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:54,041][41914] Signal inference workers to resume experience collection...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:54,042][41927] InferenceWorker_p0-w0: resuming experience collection\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:55,575][41927] Updated weights for policy 0, policy_version 10 (0.0101)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:57,216][41927] Updated weights for policy 0, policy_version 20 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:57,794][40844] Fps is (10 sec: 18841.9, 60 sec: 18841.9, 300 sec: 18841.9). Total num frames: 94208. Throughput: 0: 3761.7. Samples: 18808. Policy #0 lag: (min: 0.0, avg: 0.1, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:44:57,795][40844] Avg episode reward: [(0, '4.552')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:44:58,821][41927] Updated weights for policy 0, policy_version 30 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:00,450][41927] Updated weights for policy 0, policy_version 40 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:02,146][41927] Updated weights for policy 0, policy_version 50 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:45:02,794][40844] Fps is (10 sec: 21709.0, 60 sec: 21709.0, 300 sec: 21709.0). Total num frames: 217088. Throughput: 0: 3823.8. Samples: 38238. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:45:02,795][40844] Avg episode reward: [(0, '4.433')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:02,820][41914] Saving new best policy, reward=4.433!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:03,813][41927] Updated weights for policy 0, policy_version 60 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:05,442][41927] Updated weights for policy 0, policy_version 70 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:05,797][40844] Heartbeat connected on Batcher_0\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:05,800][40844] Heartbeat connected on LearnerWorker_p0\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:05,806][40844] Heartbeat connected on InferenceWorker_p0-w0\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:05,808][40844] Heartbeat connected on RolloutWorker_w0\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:05,813][40844] Heartbeat connected on RolloutWorker_w1\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:05,817][40844] Heartbeat connected on RolloutWorker_w2\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:05,823][40844] Heartbeat connected on RolloutWorker_w4\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:05,824][40844] Heartbeat connected on RolloutWorker_w3\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:05,826][40844] Heartbeat connected on RolloutWorker_w5\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:05,832][40844] Heartbeat connected on RolloutWorker_w7\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:05,839][40844] Heartbeat connected on RolloutWorker_w6\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:07,070][41927] Updated weights for policy 0, policy_version 80 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:45:07,794][40844] Fps is (10 sec: 24985.9, 60 sec: 22937.9, 300 sec: 22937.9). Total num frames: 344064. Throughput: 0: 5005.1. Samples: 75076. Policy #0 lag: (min: 0.0, avg: 0.5, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:45:07,795][40844] Avg episode reward: [(0, '4.559')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:07,796][41914] Saving new best policy, reward=4.559!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:08,704][41927] Updated weights for policy 0, policy_version 90 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:10,383][41927] Updated weights for policy 0, policy_version 100 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:11,964][41927] Updated weights for policy 0, policy_version 110 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:45:12,794][40844] Fps is (10 sec: 25395.2, 60 sec: 23552.1, 300 sec: 23552.1). Total num frames: 471040. Throughput: 0: 5642.9. Samples: 112858. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:45:12,795][40844] Avg episode reward: [(0, '4.629')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:12,797][41914] Saving new best policy, reward=4.629!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:13,540][41927] Updated weights for policy 0, policy_version 120 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:15,104][41927] Updated weights for policy 0, policy_version 130 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:16,649][41927] Updated weights for policy 0, policy_version 140 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:45:17,794][40844] Fps is (10 sec: 25395.0, 60 sec: 23920.8, 300 sec: 23920.8). Total num frames: 598016. Throughput: 0: 5295.1. Samples: 132378. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:45:17,795][40844] Avg episode reward: [(0, '4.376')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:18,298][41927] Updated weights for policy 0, policy_version 150 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:19,918][41927] Updated weights for policy 0, policy_version 160 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:21,542][41927] Updated weights for policy 0, policy_version 170 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:45:22,794][40844] Fps is (10 sec: 25395.3, 60 sec: 24166.5, 300 sec: 24166.5). Total num frames: 724992. Throughput: 0: 5692.2. Samples: 170764. Policy #0 lag: (min: 0.0, avg: 0.6, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:45:22,795][40844] Avg episode reward: [(0, '4.548')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:23,231][41927] Updated weights for policy 0, policy_version 180 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:24,876][41927] Updated weights for policy 0, policy_version 190 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:26,505][41927] Updated weights for policy 0, policy_version 200 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:45:27,795][40844] Fps is (10 sec: 24983.6, 60 sec: 24224.4, 300 sec: 24224.4). Total num frames: 847872. Throughput: 0: 5942.9. Samples: 208004. Policy #0 lag: (min: 0.0, avg: 0.6, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:45:27,799][40844] Avg episode reward: [(0, '4.617')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:28,152][41927] Updated weights for policy 0, policy_version 210 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:29,756][41927] Updated weights for policy 0, policy_version 220 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:31,408][41927] Updated weights for policy 0, policy_version 230 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:45:32,794][40844] Fps is (10 sec: 24985.5, 60 sec: 24371.2, 300 sec: 24371.2). Total num frames: 974848. Throughput: 0: 5674.4. Samples: 226974. Policy #0 lag: (min: 0.0, avg: 0.6, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:45:32,795][40844] Avg episode reward: [(0, '4.782')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:32,798][41914] Saving new best policy, reward=4.782!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:33,084][41927] Updated weights for policy 0, policy_version 240 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:34,724][41927] Updated weights for policy 0, policy_version 250 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:36,354][41927] Updated weights for policy 0, policy_version 260 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:45:37,794][40844] Fps is (10 sec: 24987.6, 60 sec: 24394.0, 300 sec: 24394.0). Total num frames: 1097728. Throughput: 0: 5870.9. Samples: 264190. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:45:37,795][40844] Avg episode reward: [(0, '4.616')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:38,022][41927] Updated weights for policy 0, policy_version 270 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:39,683][41927] Updated weights for policy 0, policy_version 280 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:41,334][41927] Updated weights for policy 0, policy_version 290 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:45:42,795][40844] Fps is (10 sec: 24574.0, 60 sec: 24411.8, 300 sec: 24411.8). Total num frames: 1220608. Throughput: 0: 6282.3. Samples: 301516. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:45:42,800][40844] Avg episode reward: [(0, '4.794')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:42,803][41914] Saving new best policy, reward=4.794!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:42,984][41927] Updated weights for policy 0, policy_version 300 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:44,628][41927] Updated weights for policy 0, policy_version 310 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:46,277][41927] Updated weights for policy 0, policy_version 320 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:45:47,794][40844] Fps is (10 sec: 24985.5, 60 sec: 24501.6, 300 sec: 24501.6). Total num frames: 1347584. Throughput: 0: 6264.9. Samples: 320160. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:45:47,795][40844] Avg episode reward: [(0, '5.113')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:47,796][41914] Saving new best policy, reward=5.113!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:47,940][41927] Updated weights for policy 0, policy_version 330 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:49,554][41927] Updated weights for policy 0, policy_version 340 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:51,168][41927] Updated weights for policy 0, policy_version 350 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:45:52,795][40844] Fps is (10 sec: 24985.9, 60 sec: 24507.5, 300 sec: 24507.5). Total num frames: 1470464. Throughput: 0: 6282.0. Samples: 357772. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:45:52,800][40844] Avg episode reward: [(0, '4.964')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:52,806][41927] Updated weights for policy 0, policy_version 360 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:54,433][41927] Updated weights for policy 0, policy_version 370 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:56,058][41927] Updated weights for policy 0, policy_version 380 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:57,629][41927] Updated weights for policy 0, policy_version 390 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:45:57,795][40844] Fps is (10 sec: 24983.2, 60 sec: 25053.5, 300 sec: 24575.7). Total num frames: 1597440. Throughput: 0: 6283.5. Samples: 395622. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:45:57,797][40844] Avg episode reward: [(0, '5.444')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:57,798][41914] Saving new best policy, reward=5.444!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:45:59,308][41927] Updated weights for policy 0, policy_version 400 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:00,952][41927] Updated weights for policy 0, policy_version 410 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:02,573][41927] Updated weights for policy 0, policy_version 420 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:46:02,794][40844] Fps is (10 sec: 25397.0, 60 sec: 25122.1, 300 sec: 24634.5). Total num frames: 1724416. Throughput: 0: 6266.5. Samples: 414372. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:46:02,795][40844] Avg episode reward: [(0, '5.433')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:04,184][41927] Updated weights for policy 0, policy_version 430 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:05,793][41927] Updated weights for policy 0, policy_version 440 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:07,398][41927] Updated weights for policy 0, policy_version 450 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:46:07,794][40844] Fps is (10 sec: 25397.7, 60 sec: 25122.1, 300 sec: 24685.3). Total num frames: 1851392. Throughput: 0: 6257.8. Samples: 452366. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:46:07,795][40844] Avg episode reward: [(0, '6.261')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:07,796][41914] Saving new best policy, reward=6.261!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:09,188][41927] Updated weights for policy 0, policy_version 460 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:10,868][41927] Updated weights for policy 0, policy_version 470 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:12,505][41927] Updated weights for policy 0, policy_version 480 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:46:12,794][40844] Fps is (10 sec: 24576.0, 60 sec: 24985.6, 300 sec: 24627.2). Total num frames: 1970176. Throughput: 0: 6240.4. Samples: 488816. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:46:12,795][40844] Avg episode reward: [(0, '7.087')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:12,797][41914] Saving new best policy, reward=7.087!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:14,154][41927] Updated weights for policy 0, policy_version 490 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:15,801][41927] Updated weights for policy 0, policy_version 500 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:17,437][41927] Updated weights for policy 0, policy_version 510 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:46:17,794][40844] Fps is (10 sec: 24576.1, 60 sec: 24985.6, 300 sec: 24672.4). Total num frames: 2097152. Throughput: 0: 6231.7. Samples: 507402. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:46:17,795][40844] Avg episode reward: [(0, '6.717')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:19,046][41927] Updated weights for policy 0, policy_version 520 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:20,642][41927] Updated weights for policy 0, policy_version 530 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:22,296][41927] Updated weights for policy 0, policy_version 540 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:46:22,794][40844] Fps is (10 sec: 25395.2, 60 sec: 24985.6, 300 sec: 24712.6). Total num frames: 2224128. Throughput: 0: 6246.9. Samples: 545302. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:46:22,795][40844] Avg episode reward: [(0, '6.976')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:23,891][41927] Updated weights for policy 0, policy_version 550 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:25,528][41927] Updated weights for policy 0, policy_version 560 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:27,133][41927] Updated weights for policy 0, policy_version 570 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:46:27,794][40844] Fps is (10 sec: 25395.3, 60 sec: 25054.2, 300 sec: 24748.5). Total num frames: 2351104. Throughput: 0: 6259.1. Samples: 583168. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:46:27,795][40844] Avg episode reward: [(0, '8.822')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:27,796][41914] Saving new best policy, reward=8.822!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:28,774][41927] Updated weights for policy 0, policy_version 580 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:30,396][41927] Updated weights for policy 0, policy_version 590 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:31,990][41927] Updated weights for policy 0, policy_version 600 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:46:32,794][40844] Fps is (10 sec: 25395.2, 60 sec: 25053.9, 300 sec: 24780.8). Total num frames: 2478080. Throughput: 0: 6260.1. Samples: 601866. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:46:32,795][40844] Avg episode reward: [(0, '8.652')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:33,593][41927] Updated weights for policy 0, policy_version 610 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:35,196][41927] Updated weights for policy 0, policy_version 620 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:36,841][41927] Updated weights for policy 0, policy_version 630 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:46:37,794][40844] Fps is (10 sec: 24985.4, 60 sec: 25053.8, 300 sec: 24771.1). Total num frames: 2600960. Throughput: 0: 6275.8. Samples: 640178. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:46:37,795][40844] Avg episode reward: [(0, '9.490')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:37,796][41914] Saving new best policy, reward=9.490!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:38,550][41927] Updated weights for policy 0, policy_version 640 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:40,143][41927] Updated weights for policy 0, policy_version 650 (0.0013)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:41,780][41927] Updated weights for policy 0, policy_version 660 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:46:42,794][40844] Fps is (10 sec: 24575.8, 60 sec: 25054.2, 300 sec: 24762.2). Total num frames: 2723840. Throughput: 0: 6264.3. Samples: 677512. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:46:42,795][40844] Avg episode reward: [(0, '11.409')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:42,808][41914] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_impala/checkpoint_p0/checkpoint_000000666_2727936.pth...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:42,879][41914] Saving new best policy, reward=11.409!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:43,486][41927] Updated weights for policy 0, policy_version 670 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:45,097][41927] Updated weights for policy 0, policy_version 680 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:46,709][41927] Updated weights for policy 0, policy_version 690 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:46:47,794][40844] Fps is (10 sec: 24985.6, 60 sec: 25053.9, 300 sec: 24789.7). Total num frames: 2850816. Throughput: 0: 6258.0. Samples: 695982. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:46:47,795][40844] Avg episode reward: [(0, '14.931')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:47,795][41914] Saving new best policy, reward=14.931!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:48,355][41927] Updated weights for policy 0, policy_version 700 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:49,963][41927] Updated weights for policy 0, policy_version 710 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:51,592][41927] Updated weights for policy 0, policy_version 720 (0.0009)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:46:52,794][40844] Fps is (10 sec: 25395.4, 60 sec: 25122.4, 300 sec: 24815.0). Total num frames: 2977792. Throughput: 0: 6255.2. Samples: 733852. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:46:52,795][40844] Avg episode reward: [(0, '12.769')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:53,185][41927] Updated weights for policy 0, policy_version 730 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:54,785][41927] Updated weights for policy 0, policy_version 740 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:56,408][41927] Updated weights for policy 0, policy_version 750 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:46:57,794][40844] Fps is (10 sec: 25395.3, 60 sec: 25122.6, 300 sec: 24838.2). Total num frames: 3104768. Throughput: 0: 6289.1. Samples: 771824. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:46:57,795][40844] Avg episode reward: [(0, '14.083')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:58,043][41927] Updated weights for policy 0, policy_version 760 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:46:59,677][41927] Updated weights for policy 0, policy_version 770 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:01,286][41927] Updated weights for policy 0, policy_version 780 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:47:02,794][40844] Fps is (10 sec: 25395.3, 60 sec: 25122.1, 300 sec: 24859.6). Total num frames: 3231744. Throughput: 0: 6297.1. Samples: 790770. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:47:02,795][40844] Avg episode reward: [(0, '14.740')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:02,895][41927] Updated weights for policy 0, policy_version 790 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:04,497][41927] Updated weights for policy 0, policy_version 800 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:06,121][41927] Updated weights for policy 0, policy_version 810 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:07,733][41927] Updated weights for policy 0, policy_version 820 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:47:07,794][40844] Fps is (10 sec: 25395.1, 60 sec: 25122.1, 300 sec: 24879.4). Total num frames: 3358720. Throughput: 0: 6304.3. Samples: 828994. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:47:07,795][40844] Avg episode reward: [(0, '15.868')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:07,795][41914] Saving new best policy, reward=15.868!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:09,372][41927] Updated weights for policy 0, policy_version 830 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:10,991][41927] Updated weights for policy 0, policy_version 840 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:12,581][41927] Updated weights for policy 0, policy_version 850 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:47:12,794][40844] Fps is (10 sec: 25395.1, 60 sec: 25258.7, 300 sec: 24897.8). Total num frames: 3485696. Throughput: 0: 6306.5. Samples: 866962. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:47:12,795][40844] Avg episode reward: [(0, '14.844')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:14,176][41927] Updated weights for policy 0, policy_version 860 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:15,781][41927] Updated weights for policy 0, policy_version 870 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:17,419][41927] Updated weights for policy 0, policy_version 880 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:47:17,794][40844] Fps is (10 sec: 25395.3, 60 sec: 25258.7, 300 sec: 24915.0). Total num frames: 3612672. Throughput: 0: 6317.4. Samples: 886150. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:47:17,795][40844] Avg episode reward: [(0, '20.757')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:17,796][41914] Saving new best policy, reward=20.757!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:19,016][41927] Updated weights for policy 0, policy_version 890 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:20,657][41927] Updated weights for policy 0, policy_version 900 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:22,260][41927] Updated weights for policy 0, policy_version 910 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:47:22,794][40844] Fps is (10 sec: 25395.3, 60 sec: 25258.7, 300 sec: 24931.0). Total num frames: 3739648. Throughput: 0: 6309.6. Samples: 924110. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:47:22,795][40844] Avg episode reward: [(0, '16.173')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:23,878][41927] Updated weights for policy 0, policy_version 920 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:25,499][41927] Updated weights for policy 0, policy_version 930 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:27,086][41927] Updated weights for policy 0, policy_version 940 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:47:27,794][40844] Fps is (10 sec: 25395.0, 60 sec: 25258.6, 300 sec: 24946.0). Total num frames: 3866624. Throughput: 0: 6330.1. Samples: 962364. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:47:27,795][40844] Avg episode reward: [(0, '20.508')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:28,693][41927] Updated weights for policy 0, policy_version 950 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:30,308][41927] Updated weights for policy 0, policy_version 960 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:32,004][41927] Updated weights for policy 0, policy_version 970 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:47:32,794][40844] Fps is (10 sec: 24985.5, 60 sec: 25190.4, 300 sec: 24934.4). Total num frames: 3989504. Throughput: 0: 6345.2. Samples: 981518. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:47:32,795][40844] Avg episode reward: [(0, '20.255')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:33,645][41927] Updated weights for policy 0, policy_version 980 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:35,251][41927] Updated weights for policy 0, policy_version 990 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:36,881][41927] Updated weights for policy 0, policy_version 1000 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:47:37,794][40844] Fps is (10 sec: 24985.7, 60 sec: 25258.7, 300 sec: 24948.4). Total num frames: 4116480. Throughput: 0: 6330.3. Samples: 1018714. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:47:37,795][40844] Avg episode reward: [(0, '19.032')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:38,518][41927] Updated weights for policy 0, policy_version 1010 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:40,142][41927] Updated weights for policy 0, policy_version 1020 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:41,882][41927] Updated weights for policy 0, policy_version 1030 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:47:42,794][40844] Fps is (10 sec: 24985.6, 60 sec: 25258.7, 300 sec: 24937.4). Total num frames: 4239360. Throughput: 0: 6308.2. Samples: 1055694. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:47:42,795][40844] Avg episode reward: [(0, '22.413')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:42,798][41914] Saving new best policy, reward=22.413!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:43,537][41927] Updated weights for policy 0, policy_version 1040 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:45,198][41927] Updated weights for policy 0, policy_version 1050 (0.0009)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:46,815][41927] Updated weights for policy 0, policy_version 1060 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:47:47,794][40844] Fps is (10 sec: 24575.9, 60 sec: 25190.4, 300 sec: 24927.1). Total num frames: 4362240. Throughput: 0: 6299.9. Samples: 1074264. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:47:47,795][40844] Avg episode reward: [(0, '19.354')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:48,482][41927] Updated weights for policy 0, policy_version 1070 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:50,115][41927] Updated weights for policy 0, policy_version 1080 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:51,754][41927] Updated weights for policy 0, policy_version 1090 (0.0010)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:47:52,794][40844] Fps is (10 sec: 24985.6, 60 sec: 25190.4, 300 sec: 24940.1). Total num frames: 4489216. Throughput: 0: 6283.9. Samples: 1111770. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:47:52,795][40844] Avg episode reward: [(0, '18.861')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:53,365][41927] Updated weights for policy 0, policy_version 1100 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:55,086][41927] Updated weights for policy 0, policy_version 1110 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:56,720][41927] Updated weights for policy 0, policy_version 1120 (0.0008)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:47:57,794][40844] Fps is (10 sec: 24985.7, 60 sec: 25122.1, 300 sec: 24930.3). Total num frames: 4612096. Throughput: 0: 6265.9. Samples: 1148928. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:47:57,795][40844] Avg episode reward: [(0, '22.148')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:58,341][41927] Updated weights for policy 0, policy_version 1130 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:47:59,943][41927] Updated weights for policy 0, policy_version 1140 (0.0008)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:48:01,590][41927] Updated weights for policy 0, policy_version 1150 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:02,794][40844] Fps is (10 sec: 24985.6, 60 sec: 25122.1, 300 sec: 24942.5). Total num frames: 4739072. Throughput: 0: 6264.8. Samples: 1168068. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:02,795][40844] Avg episode reward: [(0, '19.992')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:48:03,211][41927] Updated weights for policy 0, policy_version 1160 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:48:04,830][41927] Updated weights for policy 0, policy_version 1170 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:48:06,493][41927] Updated weights for policy 0, policy_version 1180 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:07,794][40844] Fps is (10 sec: 24985.4, 60 sec: 25053.8, 300 sec: 24933.1). Total num frames: 4861952. Throughput: 0: 6263.4. Samples: 1205964. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:07,795][40844] Avg episode reward: [(0, '20.179')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:48:08,119][41927] Updated weights for policy 0, policy_version 1190 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:48:09,753][41927] Updated weights for policy 0, policy_version 1200 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:48:11,357][41927] Updated weights for policy 0, policy_version 1210 (0.0006)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:12,795][40844] Fps is (10 sec: 25392.8, 60 sec: 25121.7, 300 sec: 24965.0). Total num frames: 4993024. Throughput: 0: 6248.2. Samples: 1243540. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:12,798][40844] Avg episode reward: [(0, '22.312')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:48:12,962][41927] Updated weights for policy 0, policy_version 1220 (0.0007)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,294][40844] Component Batcher_0 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,294][41914] Stopping Batcher_0...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:48:13,294][41914] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_impala/checkpoint_p0/checkpoint_000001222_5005312.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,295][41914] Loop batcher_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,303][40844] Component RolloutWorker_w3 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,303][41931] Stopping RolloutWorker_w3...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,303][41933] Stopping RolloutWorker_w7...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,303][41931] Loop rollout_proc3_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,303][41933] Loop rollout_proc7_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,303][41934] Stopping RolloutWorker_w5...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,303][41930] Stopping RolloutWorker_w2...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,303][41932] Stopping RolloutWorker_w4...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,303][41928] Stopping RolloutWorker_w0...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,303][41930] Loop rollout_proc2_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,303][41934] Loop rollout_proc5_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,304][41928] Loop rollout_proc0_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,304][41932] Loop rollout_proc4_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,304][40844] Component RolloutWorker_w7 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,305][40844] Component RolloutWorker_w5 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,305][40844] Component RolloutWorker_w2 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,305][41929] Stopping RolloutWorker_w1...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,305][41935] Stopping RolloutWorker_w6...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,305][41935] Loop rollout_proc6_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,305][41929] Loop rollout_proc1_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,306][40844] Component RolloutWorker_w4 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,307][40844] Component RolloutWorker_w0 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,308][40844] Component RolloutWorker_w1 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,308][40844] Component RolloutWorker_w6 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,328][41927] Weights refcount: 2 0\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,329][41927] Stopping InferenceWorker_p0-w0...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,329][40844] Component InferenceWorker_p0-w0 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,329][41927] Loop inference_proc0-0_evt_loop terminating...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:48:13,373][41914] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_impala/checkpoint_p0/checkpoint_000001222_5005312.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,478][40844] Component LearnerWorker_p0 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,478][41914] Stopping LearnerWorker_p0...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,478][41914] Loop learner_proc0_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:13,479][40844] Waiting for process learner_proc0 to stop...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:14,462][40844] Waiting for process inference_proc0-0 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:14,463][40844] Waiting for process rollout_proc0 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:14,464][40844] Waiting for process rollout_proc1 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:14,465][40844] Waiting for process rollout_proc2 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:14,465][40844] Waiting for process rollout_proc3 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:14,466][40844] Waiting for process rollout_proc4 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:14,466][40844] Waiting for process rollout_proc5 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:14,467][40844] Waiting for process rollout_proc6 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:14,468][40844] Waiting for process rollout_proc7 to join...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:48:14,468][40844] Batcher 0 profile tree view:\n",
      "batching: 8.9519, releasing_batches: 0.0442\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:48:14,469][40844] InferenceWorker_p0-w0 profile tree view:\n",
      "wait_policy: 0.0000\n",
      "  wait_policy_total: 3.5372\n",
      "update_model: 3.0506\n",
      "  weight_update: 0.0007\n",
      "one_step: 0.0026\n",
      "  handle_policy_step: 185.4065\n",
      "    deserialize: 7.3074, stack: 1.0763, obs_to_device_normalize: 44.2324, forward: 94.7386, send_messages: 9.8184\n",
      "    prepare_outputs: 20.7927\n",
      "      to_cpu: 12.0566\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:48:14,469][40844] Learner 0 profile tree view:\n",
      "misc: 0.0059, prepare_batch: 14.0331\n",
      "train: 35.4683\n",
      "  epoch_init: 0.0044, minibatch_init: 0.0056, losses_postprocess: 0.1475, kl_divergence: 0.1405, after_optimizer: 12.0168\n",
      "  calculate_losses: 17.1452\n",
      "    losses_init: 0.0025, forward_head: 0.5132, bptt_initial: 14.3674, tail: 0.4548, advantages_returns: 0.1068, losses: 0.5709\n",
      "    bptt: 0.9923\n",
      "      bptt_forward_core: 0.9488\n",
      "  update: 5.4761\n",
      "    clip: 0.6488\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:48:14,469][40844] RolloutWorker_w0 profile tree view:\n",
      "wait_for_trajectories: 0.1428, enqueue_policy_requests: 6.1159, env_step: 78.8270, overhead: 6.6554, complete_rollouts: 0.2322\n",
      "save_policy_outputs: 6.2347\n",
      "  split_output_tensors: 2.9688\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:48:14,469][40844] RolloutWorker_w7 profile tree view:\n",
      "wait_for_trajectories: 0.1340, enqueue_policy_requests: 6.2715, env_step: 83.1125, overhead: 7.0839, complete_rollouts: 0.2263\n",
      "save_policy_outputs: 6.5388\n",
      "  split_output_tensors: 3.1014\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:48:14,470][40844] Loop Runner_EvtLoop terminating...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:48:14,470][40844] Runner profile tree view:\n",
      "main_loop: 208.6392\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:48:14,471][40844] Collected {0: 5005312}, FPS: 23990.3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## Start the training, this should take around 15 minutes\n",
    "register_vizdoom_components()\n",
    "\n",
    "# The scenario we train on today is health gathering\n",
    "# other scenarios include \"doom_basic\", \"doom_two_colors_easy\", \"doom_dm\", \"doom_dwango5\", \"doom_my_way_home\", \"doom_deadly_corridor\", \"doom_defend_the_center\", \"doom_defend_the_line\"\n",
    "env = \"doom_health_gathering_supreme\"\n",
    "cfg = parse_vizdoom_cfg(\n",
    "    argv=[f\"--env={env}\", \n",
    "          \"--experiment=conv_impala\",\n",
    "          \"--seed=200\",\n",
    "          \"--num_workers=8\",                    # Number of parallel environment workers.8\n",
    "          \"--num_envs_per_worker=4\",            # Number of envs on a single CPU actor.4\n",
    "          \"--batch_size=1024\",\n",
    "          \"--encoder_conv_architecture=convnet_impala\",\n",
    "          \"--train_for_env_steps=5000000\"]\n",
    ")\n",
    "\n",
    "# sample_size = num_workers * num_envs_per_worker * rollout \n",
    "# = 8 * 4 * 32 = 1024\n",
    "# = 16 * 8 * 32 = 4096\n",
    "# = 20 * 12 * 32 = 7680\n",
    "\n",
    "# batch_size = 2048\n",
    "\n",
    "status = run_rl(cfg)\n",
    "# run_rl(cfg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2024-07-04 23:50:28,906][40844] Loading existing experiment configuration from /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_impala/config.json\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:28,907][40844] Overriding arg 'num_workers' with value 1 passed from command line\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:28,908][40844] Adding new argument 'no_render'=True that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:28,909][40844] Adding new argument 'save_video'=True that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:28,909][40844] Adding new argument 'video_frames'=1000000000.0 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:28,910][40844] Adding new argument 'video_name'=None that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:28,910][40844] Adding new argument 'max_num_frames'=1000000000.0 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:28,911][40844] Adding new argument 'max_num_episodes'=10 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:28,911][40844] Adding new argument 'push_to_hub'=False that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:28,912][40844] Adding new argument 'hf_repository'=None that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:28,912][40844] Adding new argument 'policy_index'=0 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:28,912][40844] Adding new argument 'eval_deterministic'=False that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:28,913][40844] Adding new argument 'train_script'=None that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:28,913][40844] Adding new argument 'enjoy_script'=None that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:28,914][40844] Using frameskip 1 and render_action_repeat=4 for evaluation\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:28,936][40844] RunningMeanStd input shape: (3, 72, 128)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:28,938][40844] RunningMeanStd input shape: (1,)\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:28,947][40844] ConvEncoder: input_channels=3\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:28,985][40844] Conv encoder output size: 512\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:28,986][40844] Policy head output size: 512\u001b[0m\n",
      "\u001b[33m[2024-07-04 23:50:29,011][40844] Loading state from checkpoint /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_impala/checkpoint_p0/checkpoint_000001222_5005312.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:30,024][40844] Num frames 100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:30,137][40844] Num frames 200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:30,246][40844] Num frames 300...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:30,348][40844] Num frames 400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:30,456][40844] Num frames 500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:30,565][40844] Num frames 600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:30,666][40844] Num frames 700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:30,766][40844] Num frames 800...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:30,870][40844] Num frames 900...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:30,978][40844] Num frames 1000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:31,088][40844] Num frames 1100...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:50:31,245][40844] Avg episode rewards: #0: 25.910, true rewards: #0: 11.910\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:50:31,246][40844] Avg episode reward: 25.910, avg true_objective: 11.910\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:31,257][40844] Num frames 1200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:31,351][40844] Num frames 1300...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:31,451][40844] Num frames 1400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:31,542][40844] Num frames 1500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:31,616][40844] Num frames 1600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:31,703][40844] Num frames 1700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:31,786][40844] Num frames 1800...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:31,860][40844] Num frames 1900...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:50:31,984][40844] Avg episode rewards: #0: 19.955, true rewards: #0: 9.955\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:50:31,985][40844] Avg episode reward: 19.955, avg true_objective: 9.955\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:31,994][40844] Num frames 2000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:32,073][40844] Num frames 2100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:32,147][40844] Num frames 2200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:32,219][40844] Num frames 2300...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:50:32,301][40844] Avg episode rewards: #0: 14.810, true rewards: #0: 7.810\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:50:32,302][40844] Avg episode reward: 14.810, avg true_objective: 7.810\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:32,347][40844] Num frames 2400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:32,423][40844] Num frames 2500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:32,496][40844] Num frames 2600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:32,567][40844] Num frames 2700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:32,644][40844] Num frames 2800...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:32,722][40844] Num frames 2900...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:32,792][40844] Num frames 3000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:32,863][40844] Num frames 3100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:32,946][40844] Num frames 3200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:33,023][40844] Num frames 3300...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:33,100][40844] Num frames 3400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:33,179][40844] Num frames 3500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:33,250][40844] Num frames 3600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:33,327][40844] Num frames 3700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:33,403][40844] Num frames 3800...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:50:33,468][40844] Avg episode rewards: #0: 19.288, true rewards: #0: 9.537\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:50:33,469][40844] Avg episode reward: 19.288, avg true_objective: 9.537\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:33,534][40844] Num frames 3900...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:33,603][40844] Num frames 4000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:33,680][40844] Num frames 4100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:33,756][40844] Num frames 4200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:33,826][40844] Num frames 4300...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:33,898][40844] Num frames 4400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:33,968][40844] Num frames 4500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:34,046][40844] Num frames 4600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:34,120][40844] Num frames 4700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:34,193][40844] Num frames 4800...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:50:34,302][40844] Avg episode rewards: #0: 19.742, true rewards: #0: 9.742\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:50:34,305][40844] Avg episode reward: 19.742, avg true_objective: 9.742\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:34,330][40844] Num frames 4900...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:34,403][40844] Num frames 5000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:34,478][40844] Num frames 5100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:34,554][40844] Num frames 5200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:34,629][40844] Num frames 5300...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:50:34,720][40844] Avg episode rewards: #0: 17.418, true rewards: #0: 8.918\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:50:34,721][40844] Avg episode reward: 17.418, avg true_objective: 8.918\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:34,759][40844] Num frames 5400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:34,835][40844] Num frames 5500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:34,912][40844] Num frames 5600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:34,988][40844] Num frames 5700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:35,064][40844] Num frames 5800...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:50:35,133][40844] Avg episode rewards: #0: 16.310, true rewards: #0: 8.310\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:50:35,134][40844] Avg episode reward: 16.310, avg true_objective: 8.310\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:35,201][40844] Num frames 5900...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:35,275][40844] Num frames 6000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:35,350][40844] Num frames 6100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:35,422][40844] Num frames 6200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:35,494][40844] Num frames 6300...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:35,567][40844] Num frames 6400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:35,638][40844] Num frames 6500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:35,710][40844] Num frames 6600...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:50:35,823][40844] Avg episode rewards: #0: 16.226, true rewards: #0: 8.351\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:50:35,824][40844] Avg episode reward: 16.226, avg true_objective: 8.351\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:35,841][40844] Num frames 6700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:35,916][40844] Num frames 6800...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:35,989][40844] Num frames 6900...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:36,062][40844] Num frames 7000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:36,140][40844] Num frames 7100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:36,221][40844] Num frames 7200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:36,302][40844] Num frames 7300...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:50:36,433][40844] Avg episode rewards: #0: 15.880, true rewards: #0: 8.213\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:50:36,434][40844] Avg episode reward: 15.880, avg true_objective: 8.213\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:36,441][40844] Num frames 7400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:36,515][40844] Num frames 7500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:36,594][40844] Num frames 7600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:36,667][40844] Num frames 7700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:36,730][40844] Num frames 7800...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:36,803][40844] Num frames 7900...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:36,879][40844] Num frames 8000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:36,953][40844] Num frames 8100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:37,031][40844] Num frames 8200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:37,108][40844] Num frames 8300...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:37,184][40844] Num frames 8400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:37,260][40844] Num frames 8500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:37,338][40844] Num frames 8600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 23:50:37,411][40844] Num frames 8700...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:50:37,492][40844] Avg episode rewards: #0: 17.636, true rewards: #0: 8.736\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 23:50:37,493][40844] Avg episode reward: 17.636, avg true_objective: 8.736\u001b[0m\n",
      "ffmpeg version 9c33b2f Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 9.3.0 (crosstool-NG 1.24.0.133_b0863d8_dirty)\n",
      "  configuration: --prefix=/home/conda/feedstock_root/build_artifacts/ffmpeg_1627813612080/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1627813612080/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-gnutls --enable-gpl --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-libx264 --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1627813612080/_build_env/bin/pkg-config\n",
      "  libavutil      56. 51.100 / 56. 51.100\n",
      "  libavcodec     58. 91.100 / 58. 91.100\n",
      "  libavformat    58. 45.100 / 58. 45.100\n",
      "  libavdevice    58. 10.100 / 58. 10.100\n",
      "  libavfilter     7. 85.100 /  7. 85.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  7.100 /  5.  7.100\n",
      "  libswresample   3.  7.100 /  3.  7.100\n",
      "  libpostproc    55.  7.100 / 55.  7.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/tmp/sf2_raghu/replay.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf59.27.100\n",
      "  Duration: 00:04:09.89, start: 0.000000, bitrate: 1706 kb/s\n",
      "    Stream #0:0(und): Video: mpeg4 (Simple Profile) (mp4v / 0x7634706D), yuv420p, 240x180 [SAR 1:1 DAR 4:3], 1705 kb/s, 35 fps, 35 tbr, 17920 tbn, 35 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mpeg4 (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x64c3bffd4040] using SAR=1/1\n",
      "[libx264 @ 0x64c3bffd4040] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
      "[libx264 @ 0x64c3bffd4040] profile High, level 1.3, 4:2:0, 8-bit\n",
      "[libx264 @ 0x64c3bffd4040] 264 - core 161 r3030M 8bd6d28 - H.264/MPEG-4 AVC codec - Copyleft 2003-2020 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to '/home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_impala/replay.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf58.45.100\n",
      "    Stream #0:0(und): Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 240x180 [SAR 1:1 DAR 4:3], q=-1--1, 35 fps, 17920 tbn, 35 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      encoder         : Lavc58.91.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "frame= 8746 fps=1218 q=-1.0 Lsize=   16118kB time=00:04:09.80 bitrate= 528.6kbits/s speed=34.8x    \n",
      "video:16027kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.566353%\n",
      "[libx264 @ 0x64c3bffd4040] frame I:85    Avg QP:23.66  size:  5076\n",
      "[libx264 @ 0x64c3bffd4040] frame P:3974  Avg QP:26.61  size:  2174\n",
      "[libx264 @ 0x64c3bffd4040] frame B:4687  Avg QP:28.14  size:  1566\n",
      "[libx264 @ 0x64c3bffd4040] consecutive B-frames: 24.5% 10.1%  5.8% 59.5%\n",
      "[libx264 @ 0x64c3bffd4040] mb I  I16..4: 17.5% 71.0% 11.5%\n",
      "[libx264 @ 0x64c3bffd4040] mb P  I16..4:  3.3% 14.8%  4.2%  P16..4: 38.1% 23.3%  9.8%  0.0%  0.0%    skip: 6.5%\n",
      "[libx264 @ 0x64c3bffd4040] mb B  I16..4:  0.2%  5.2%  2.4%  B16..8: 38.6% 19.3%  5.9%  direct: 8.1%  skip:20.2%  L0:50.7% L1:35.5% BI:13.8%\n",
      "[libx264 @ 0x64c3bffd4040] 8x8 transform intra:66.8% inter:63.8%\n",
      "[libx264 @ 0x64c3bffd4040] coded y,uvDC,uvAC intra: 67.8% 69.4% 34.3% inter: 44.0% 17.0% 3.5%\n",
      "[libx264 @ 0x64c3bffd4040] i16 v,h,dc,p: 65%  4% 31%  1%\n",
      "[libx264 @ 0x64c3bffd4040] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 21% 10% 33%  5%  5%  4%  7%  5%  8%\n",
      "[libx264 @ 0x64c3bffd4040] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 60%  8% 10%  3%  4%  3%  5%  3%  4%\n",
      "[libx264 @ 0x64c3bffd4040] i8c dc,h,v,p: 56% 21% 21%  2%\n",
      "[libx264 @ 0x64c3bffd4040] Weighted P-Frames: Y:7.1% UV:0.5%\n",
      "[libx264 @ 0x64c3bffd4040] ref P L0: 63.4% 14.7% 13.8%  7.5%  0.6%\n",
      "[libx264 @ 0x64c3bffd4040] ref B L0: 88.5%  8.8%  2.7%\n",
      "[libx264 @ 0x64c3bffd4040] ref B L1: 96.4%  3.6%\n",
      "[libx264 @ 0x64c3bffd4040] kb/s:525.39\n",
      "\u001b[36m[2024-07-04 23:50:47,348][40844] Replay video saved to /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_impala/replay.mp4!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sample_factory.enjoy import enjoy\n",
    "\n",
    "cfg = parse_vizdoom_cfg(\n",
    "    argv=[f\"--env={env}\", \n",
    "          \"--experiment=conv_impala\",\n",
    "          \"--num_workers=1\", \n",
    "          \"--save_video\", \n",
    "          \"--no_render\", \n",
    "          \"--max_num_episodes=10\"], evaluation=True\n",
    ")\n",
    "status = enjoy(cfg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convnet_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[2024-07-05 20:12:24,277][25130] register_encoder_factory: <function make_vizdoom_encoder at 0x75c2f59d2560>\u001b[0m\n",
      "\u001b[33m[2024-07-05 20:12:24,286][25130] Loading existing experiment configuration from /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/config.json\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:24,287][25130] Overriding arg 'train_for_env_steps' with value 100000000 passed from command line\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:24,295][25130] Experiment dir /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet already exists!\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:24,296][25130] Resuming existing experiment from /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:24,296][25130] Weights and Biases integration disabled\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:26,315][25130] Queried available GPUs: 0\n",
      "\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:26,316][25130] Environment var CUDA_VISIBLE_DEVICES is 0\n",
      "\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:28,312][25685] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-05 20:12:28,313][25685] Env info: EnvInfo(obs_space=Dict('obs': Box(0, 255, (3, 72, 128), uint8)), action_space=Discrete(5), num_agents=1, gpu_actions=False, gpu_observations=True, action_splits=None, all_discrete=None, frameskip=4, reward_shaping_scheme=None, env_info_protocol_version=1)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:28,771][25130] Starting experiment with the following configuration:\n",
      "help=False\n",
      "algo=APPO\n",
      "env=doom_health_gathering_supreme\n",
      "experiment=conv_resnet\n",
      "train_dir=/home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir\n",
      "restart_behavior=resume\n",
      "device=gpu\n",
      "seed=200\n",
      "num_policies=1\n",
      "async_rl=True\n",
      "serial_mode=False\n",
      "batched_sampling=False\n",
      "num_batches_to_accumulate=2\n",
      "worker_num_splits=2\n",
      "policy_workers_per_policy=1\n",
      "max_policy_lag=1000\n",
      "num_workers=8\n",
      "num_envs_per_worker=4\n",
      "batch_size=1024\n",
      "num_batches_per_epoch=1\n",
      "num_epochs=1\n",
      "rollout=32\n",
      "recurrence=32\n",
      "shuffle_minibatches=False\n",
      "gamma=0.99\n",
      "reward_scale=1.0\n",
      "reward_clip=1000.0\n",
      "value_bootstrap=False\n",
      "normalize_returns=True\n",
      "exploration_loss_coeff=0.001\n",
      "value_loss_coeff=0.5\n",
      "kl_loss_coeff=0.0\n",
      "exploration_loss=symmetric_kl\n",
      "gae_lambda=0.95\n",
      "ppo_clip_ratio=0.1\n",
      "ppo_clip_value=0.2\n",
      "with_vtrace=False\n",
      "vtrace_rho=1.0\n",
      "vtrace_c=1.0\n",
      "optimizer=adam\n",
      "adam_eps=1e-06\n",
      "adam_beta1=0.9\n",
      "adam_beta2=0.999\n",
      "max_grad_norm=4.0\n",
      "learning_rate=0.0001\n",
      "lr_schedule=constant\n",
      "lr_schedule_kl_threshold=0.008\n",
      "lr_adaptive_min=1e-06\n",
      "lr_adaptive_max=0.01\n",
      "obs_subtract_mean=0.0\n",
      "obs_scale=255.0\n",
      "normalize_input=True\n",
      "normalize_input_keys=None\n",
      "decorrelate_experience_max_seconds=0\n",
      "decorrelate_envs_on_one_worker=True\n",
      "actor_worker_gpus=[]\n",
      "set_workers_cpu_affinity=True\n",
      "force_envs_single_thread=False\n",
      "default_niceness=0\n",
      "log_to_file=True\n",
      "experiment_summaries_interval=10\n",
      "flush_summaries_interval=30\n",
      "stats_avg=100\n",
      "summaries_use_frameskip=True\n",
      "heartbeat_interval=20\n",
      "heartbeat_reporting_interval=600\n",
      "train_for_env_steps=100000000\n",
      "train_for_seconds=10000000000\n",
      "save_every_sec=120\n",
      "keep_checkpoints=2\n",
      "load_checkpoint_kind=latest\n",
      "save_milestones_sec=-1\n",
      "save_best_every_sec=5\n",
      "save_best_metric=reward\n",
      "save_best_after=100000\n",
      "benchmark=False\n",
      "encoder_mlp_layers=[512, 512]\n",
      "encoder_conv_architecture=resnet_impala\n",
      "encoder_conv_mlp_layers=[512]\n",
      "use_rnn=True\n",
      "rnn_size=512\n",
      "rnn_type=gru\n",
      "rnn_num_layers=1\n",
      "decoder_mlp_layers=[]\n",
      "nonlinearity=elu\n",
      "policy_initialization=orthogonal\n",
      "policy_init_gain=1.0\n",
      "actor_critic_share_weights=True\n",
      "adaptive_stddev=True\n",
      "continuous_tanh_scale=0.0\n",
      "initial_stddev=1.0\n",
      "use_env_info_cache=False\n",
      "env_gpu_actions=False\n",
      "env_gpu_observations=True\n",
      "env_frameskip=4\n",
      "env_framestack=1\n",
      "pixel_format=CHW\n",
      "use_record_episode_statistics=False\n",
      "with_wandb=False\n",
      "wandb_user=None\n",
      "wandb_project=sample_factory\n",
      "wandb_group=None\n",
      "wandb_job_type=SF\n",
      "wandb_tags=[]\n",
      "with_pbt=False\n",
      "pbt_mix_policies_in_one_env=True\n",
      "pbt_period_env_steps=5000000\n",
      "pbt_start_mutation=20000000\n",
      "pbt_replace_fraction=0.3\n",
      "pbt_mutation_rate=0.15\n",
      "pbt_replace_reward_gap=0.1\n",
      "pbt_replace_reward_gap_absolute=1e-06\n",
      "pbt_optimize_gamma=False\n",
      "pbt_target_objective=true_objective\n",
      "pbt_perturb_min=1.1\n",
      "pbt_perturb_max=1.5\n",
      "num_agents=-1\n",
      "num_humans=0\n",
      "num_bots=-1\n",
      "start_bot_difficulty=None\n",
      "timelimit=None\n",
      "res_w=128\n",
      "res_h=72\n",
      "wide_aspect_ratio=False\n",
      "eval_env_frameskip=1\n",
      "fps=35\n",
      "command_line=--env=doom_health_gathering_supreme --experiment=conv_resnet --seed=200 --num_workers=8 --num_envs_per_worker=4 --batch_size=1024 --encoder_conv_architecture=resnet_impala --train_for_env_steps=5000000\n",
      "cli_args={'env': 'doom_health_gathering_supreme', 'experiment': 'conv_resnet', 'seed': 200, 'num_workers': 8, 'num_envs_per_worker': 4, 'batch_size': 1024, 'train_for_env_steps': 5000000, 'encoder_conv_architecture': 'resnet_impala'}\n",
      "git_hash=unknown\n",
      "git_repo_name=not a git repository\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:28,772][25130] Saving configuration to /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/config.json...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:28,773][25130] Rollout worker 0 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:28,773][25130] Rollout worker 1 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:28,774][25130] Rollout worker 2 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:28,774][25130] Rollout worker 3 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:28,774][25130] Rollout worker 4 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:28,775][25130] Rollout worker 5 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:28,775][25130] Rollout worker 6 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:28,775][25130] Rollout worker 7 uses device cpu\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:28,808][25130] Using GPUs [0] for process 0 (actually maps to GPUs [0])\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:28,809][25130] InferenceWorker_p0-w0: min num requests: 2\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:28,835][25130] Starting all processes...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:28,836][25130] Starting process learner_proc0\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:29,432][25130] Starting all processes...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:29,438][25130] Starting process inference_proc0-0\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:29,439][25130] Starting process rollout_proc0\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:29,439][25130] Starting process rollout_proc1\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:29,440][25130] Starting process rollout_proc2\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:29,440][25130] Starting process rollout_proc3\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:29,442][25130] Starting process rollout_proc4\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:29,446][25130] Starting process rollout_proc5\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:29,447][25130] Starting process rollout_proc6\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:29,447][25130] Starting process rollout_proc7\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,141][25744] Rollout worker 3 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:32,141][25744] ROLLOUT worker 3\tpid 25744\tparent 25130\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,141][25744] Worker 3 uses CPU cores [6, 7]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,221][25743] Rollout worker 2 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:32,221][25743] ROLLOUT worker 2\tpid 25743\tparent 25130\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,222][25743] Worker 2 uses CPU cores [4, 5]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:32,242][25727] LearnerWorker_p0\tpid 25727\tparent 25130\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,243][25727] Using GPUs [0] for process 0 (actually maps to GPUs [0])\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:32,243][25727] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for learning process 0\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:32,270][25742] InferenceWorker_p0-w0\tpid 25742\tparent 25130\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,271][25742] Using GPUs [0] for process 0 (actually maps to GPUs [0])\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:32,271][25742] Set environment var CUDA_VISIBLE_DEVICES to '0' (GPU indices [0]) for inference process 0\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,295][25727] Num visible devices: 1\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,310][25741] Rollout worker 1 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:32,312][25741] ROLLOUT worker 1\tpid 25741\tparent 25130\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,316][25741] Worker 1 uses CPU cores [2, 3]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:32,316][25727] Setting fixed seed 200\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,318][25727] Using GPUs [0] for process 0 (actually maps to GPUs [0])\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,318][25727] Initializing actor-critic model on device cuda:0\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,319][25727] RunningMeanStd input shape: (3, 72, 128)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,320][25727] RunningMeanStd input shape: (1,)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,328][25727] Num input channels: 3\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,330][25742] Num visible devices: 1\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,337][25748] Rollout worker 6 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:32,338][25748] ROLLOUT worker 6\tpid 25748\tparent 25130\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,341][25727] Convolutional layer output size: 4608\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,351][25748] Worker 6 uses CPU cores [12, 13]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,354][25727] Policy head output size: 512\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,454][25740] Rollout worker 0 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:32,454][25740] ROLLOUT worker 0\tpid 25740\tparent 25130\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,455][25740] Worker 0 uses CPU cores [0, 1]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,459][25727] Created Actor Critic model with architecture:\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,459][25727] ActorCriticSharedWeights(\n",
      "  (obs_normalizer): ObservationNormalizer(\n",
      "    (running_mean_std): RunningMeanStdDictInPlace(\n",
      "      (running_mean_std): ModuleDict(\n",
      "        (obs): RunningMeanStdInPlace()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)\n",
      "  (encoder): VizdoomEncoder(\n",
      "    (basic_encoder): ResnetEncoder(\n",
      "      (conv_head): Sequential(\n",
      "        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "        (2): ResBlock(\n",
      "          (res_block_core): Sequential(\n",
      "            (0): ELU(alpha=1.0)\n",
      "            (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (2): ELU(alpha=1.0)\n",
      "            (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (3): ResBlock(\n",
      "          (res_block_core): Sequential(\n",
      "            (0): ELU(alpha=1.0)\n",
      "            (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (2): ELU(alpha=1.0)\n",
      "            (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (5): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "        (6): ResBlock(\n",
      "          (res_block_core): Sequential(\n",
      "            (0): ELU(alpha=1.0)\n",
      "            (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (2): ELU(alpha=1.0)\n",
      "            (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (7): ResBlock(\n",
      "          (res_block_core): Sequential(\n",
      "            (0): ELU(alpha=1.0)\n",
      "            (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (2): ELU(alpha=1.0)\n",
      "            (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (8): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (9): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "        (10): ResBlock(\n",
      "          (res_block_core): Sequential(\n",
      "            (0): ELU(alpha=1.0)\n",
      "            (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (2): ELU(alpha=1.0)\n",
      "            (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (11): ResBlock(\n",
      "          (res_block_core): Sequential(\n",
      "            (0): ELU(alpha=1.0)\n",
      "            (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (2): ELU(alpha=1.0)\n",
      "            (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (12): ELU(alpha=1.0)\n",
      "      )\n",
      "      (mlp_layers): Sequential(\n",
      "        (0): Linear(in_features=4608, out_features=512, bias=True)\n",
      "        (1): ELU(alpha=1.0)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (core): ModelCoreRNN(\n",
      "    (core): GRU(512, 512)\n",
      "  )\n",
      "  (decoder): MlpDecoder(\n",
      "    (mlp): Identity()\n",
      "  )\n",
      "  (critic_linear): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (action_parameterization): ActionParameterizationDefault(\n",
      "    (distribution_linear): Linear(in_features=512, out_features=5, bias=True)\n",
      "  )\n",
      ")\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,477][25745] Rollout worker 4 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:32,477][25745] ROLLOUT worker 4\tpid 25745\tparent 25130\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,477][25745] Worker 4 uses CPU cores [8, 9]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,570][25727] Using optimizer <class 'torch.optim.adam.Adam'>\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,598][25747] Rollout worker 7 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:32,598][25747] ROLLOUT worker 7\tpid 25747\tparent 25130\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,598][25747] Worker 7 uses CPU cores [14, 15]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,603][25746] Rollout worker 5 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:32,603][25746] ROLLOUT worker 5\tpid 25746\tparent 25130\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:32,603][25746] Worker 5 uses CPU cores [10, 11]\u001b[0m\n",
      "\u001b[33m[2024-07-05 20:12:33,071][25727] Loading state from checkpoint /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000017091_70004736.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:33,104][25727] Loading model from checkpoint\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:33,106][25727] Loaded experiment state at self.train_step=17091, self.env_steps=70004736\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:33,106][25727] Initialized policy 0 weights for model version 17091\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:33,107][25727] LearnerWorker_p0 finished initialization!\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:33,108][25727] Using GPUs [0] for process 0 (actually maps to GPUs [0])\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:33,184][25742] RunningMeanStd input shape: (3, 72, 128)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:33,185][25742] RunningMeanStd input shape: (1,)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:33,192][25742] Num input channels: 3\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:33,202][25742] Convolutional layer output size: 4608\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:33,213][25742] Policy head output size: 512\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:33,335][25130] Inference worker 0-0 is ready!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:33,337][25130] All inference workers are ready! Signal rollout workers to start!\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:33,372][25740] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-05 20:12:33,373][25741] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-05 20:12:33,375][25744] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:33,375][25745] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:33,375][25748] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-05 20:12:33,383][25743] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-05 20:12:33,388][25747] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-05 20:12:33,391][25746] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:33,903][25745] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:33,906][25740] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:33,906][25743] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:33,906][25746] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:33,907][25744] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:33,907][25741] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:33,909][25747] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:34,063][25741] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:34,064][25745] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:34,065][25740] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:34,067][25747] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:34,080][25748] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:34,127][25746] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:34,241][25748] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:34,266][25741] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:34,266][25745] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:34,282][25743] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:34,299][25130] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 70004736. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:34,328][25746] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:34,432][25747] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:34,446][25748] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:34,450][25745] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:34,450][25741] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:34,476][25744] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:34,491][25743] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:34,629][25740] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:34,629][25748] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:34,671][25746] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:34,672][25743] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:34,697][25747] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:34,808][25740] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:34,838][25744] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:35,011][25744] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:35,479][25727] Signal inference workers to stop experience collection...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:35,485][25742] InferenceWorker_p0-w0: stopping experience collection\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:38,772][25727] Signal inference workers to resume experience collection...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:38,773][25742] InferenceWorker_p0-w0: resuming experience collection\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:39,299][25130] Fps is (10 sec: 819.2, 60 sec: 819.2, 300 sec: 819.2). Total num frames: 70008832. Throughput: 0: 480.8. Samples: 2404. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:39,300][25130] Avg episode reward: [(0, '1.933')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:43,188][25742] Updated weights for policy 0, policy_version 17101 (0.0101)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:44,299][25130] Fps is (10 sec: 4915.2, 60 sec: 4915.2, 300 sec: 4915.2). Total num frames: 70053888. Throughput: 0: 946.0. Samples: 9460. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:44,300][25130] Avg episode reward: [(0, '18.325')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:47,652][25742] Updated weights for policy 0, policy_version 17111 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:48,801][25130] Heartbeat connected on Batcher_0\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:48,813][25130] Heartbeat connected on RolloutWorker_w0\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:48,816][25130] Heartbeat connected on RolloutWorker_w1\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:48,819][25130] Heartbeat connected on RolloutWorker_w2\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:48,822][25130] Heartbeat connected on InferenceWorker_p0-w0\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:48,824][25130] Heartbeat connected on RolloutWorker_w3\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:48,826][25130] Heartbeat connected on RolloutWorker_w4\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:48,828][25130] Heartbeat connected on RolloutWorker_w5\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:48,831][25130] Heartbeat connected on RolloutWorker_w6\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:48,834][25130] Heartbeat connected on RolloutWorker_w7\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:48,985][25130] Heartbeat connected on LearnerWorker_p0\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:49,300][25130] Fps is (10 sec: 9010.7, 60 sec: 6280.3, 300 sec: 6280.3). Total num frames: 70098944. Throughput: 0: 1543.4. Samples: 23152. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:49,301][25130] Avg episode reward: [(0, '44.060')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:52,147][25742] Updated weights for policy 0, policy_version 17121 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:54,299][25130] Fps is (10 sec: 9011.1, 60 sec: 6963.2, 300 sec: 6963.2). Total num frames: 70144000. Throughput: 0: 1504.6. Samples: 30092. Policy #0 lag: (min: 0.0, avg: 0.9, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:54,301][25130] Avg episode reward: [(0, '49.452')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:12:56,633][25742] Updated weights for policy 0, policy_version 17131 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:59,300][25130] Fps is (10 sec: 9011.6, 60 sec: 7372.8, 300 sec: 7372.8). Total num frames: 70189056. Throughput: 0: 1747.0. Samples: 43676. Policy #0 lag: (min: 0.0, avg: 0.9, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:12:59,301][25130] Avg episode reward: [(0, '48.456')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:13:01,146][25742] Updated weights for policy 0, policy_version 17141 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:13:04,299][25130] Fps is (10 sec: 9420.8, 60 sec: 7782.4, 300 sec: 7782.4). Total num frames: 70238208. Throughput: 0: 1905.5. Samples: 57166. Policy #0 lag: (min: 0.0, avg: 0.9, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:13:04,300][25130] Avg episode reward: [(0, '53.194')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:13:05,667][25742] Updated weights for policy 0, policy_version 17151 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:13:09,299][25130] Fps is (10 sec: 9421.0, 60 sec: 7958.0, 300 sec: 7958.0). Total num frames: 70283264. Throughput: 0: 1831.3. Samples: 64096. Policy #0 lag: (min: 0.0, avg: 0.9, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:13:09,300][25130] Avg episode reward: [(0, '53.233')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:13:10,180][25742] Updated weights for policy 0, policy_version 17161 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:13:14,300][25130] Fps is (10 sec: 9010.6, 60 sec: 8089.5, 300 sec: 8089.5). Total num frames: 70328320. Throughput: 0: 1941.4. Samples: 77658. Policy #0 lag: (min: 0.0, avg: 0.9, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:13:14,302][25130] Avg episode reward: [(0, '51.628')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:13:14,664][25742] Updated weights for policy 0, policy_version 17171 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:13:19,114][25742] Updated weights for policy 0, policy_version 17181 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:13:19,299][25130] Fps is (10 sec: 9011.1, 60 sec: 8192.0, 300 sec: 8192.0). Total num frames: 70373376. Throughput: 0: 2035.0. Samples: 91576. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:13:19,300][25130] Avg episode reward: [(0, '52.623')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:13:23,633][25742] Updated weights for policy 0, policy_version 17191 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:13:24,299][25130] Fps is (10 sec: 9011.9, 60 sec: 8273.9, 300 sec: 8273.9). Total num frames: 70418432. Throughput: 0: 2128.6. Samples: 98190. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:13:24,300][25130] Avg episode reward: [(0, '51.901')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:13:28,122][25742] Updated weights for policy 0, policy_version 17201 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:13:29,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8340.9, 300 sec: 8340.9). Total num frames: 70463488. Throughput: 0: 2280.3. Samples: 112072. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:13:29,300][25130] Avg episode reward: [(0, '50.709')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:13:32,693][25742] Updated weights for policy 0, policy_version 17211 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:13:34,299][25130] Fps is (10 sec: 9011.1, 60 sec: 8396.8, 300 sec: 8396.8). Total num frames: 70508544. Throughput: 0: 2273.9. Samples: 125476. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:13:34,300][25130] Avg episode reward: [(0, '51.335')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:13:37,262][25742] Updated weights for policy 0, policy_version 17221 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:13:39,299][25130] Fps is (10 sec: 9011.2, 60 sec: 9079.5, 300 sec: 8444.1). Total num frames: 70553600. Throughput: 0: 2266.0. Samples: 132060. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:13:39,300][25130] Avg episode reward: [(0, '51.084')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:13:41,855][25742] Updated weights for policy 0, policy_version 17231 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:13:44,299][25130] Fps is (10 sec: 9011.1, 60 sec: 9079.4, 300 sec: 8484.6). Total num frames: 70598656. Throughput: 0: 2263.4. Samples: 145530. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:13:44,301][25130] Avg episode reward: [(0, '52.155')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:13:46,497][25742] Updated weights for policy 0, policy_version 17241 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:13:49,299][25130] Fps is (10 sec: 9011.3, 60 sec: 9079.6, 300 sec: 8519.7). Total num frames: 70643712. Throughput: 0: 2260.5. Samples: 158888. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:13:49,300][25130] Avg episode reward: [(0, '53.427')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:13:51,014][25742] Updated weights for policy 0, policy_version 17251 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:13:54,300][25130] Fps is (10 sec: 9010.7, 60 sec: 9079.4, 300 sec: 8550.3). Total num frames: 70688768. Throughput: 0: 2257.0. Samples: 165664. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:13:54,301][25130] Avg episode reward: [(0, '53.872')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:13:55,565][25742] Updated weights for policy 0, policy_version 17261 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:13:59,300][25130] Fps is (10 sec: 9010.6, 60 sec: 9079.4, 300 sec: 8577.4). Total num frames: 70733824. Throughput: 0: 2254.7. Samples: 179118. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:13:59,301][25130] Avg episode reward: [(0, '54.745')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:14:00,114][25742] Updated weights for policy 0, policy_version 17271 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:14:04,300][25130] Fps is (10 sec: 9011.1, 60 sec: 9011.1, 300 sec: 8601.5). Total num frames: 70778880. Throughput: 0: 2247.5. Samples: 192714. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:14:04,303][25130] Avg episode reward: [(0, '54.736')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:14:04,659][25742] Updated weights for policy 0, policy_version 17281 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:14:09,240][25742] Updated weights for policy 0, policy_version 17291 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:14:09,299][25130] Fps is (10 sec: 9011.8, 60 sec: 9011.2, 300 sec: 8623.2). Total num frames: 70823936. Throughput: 0: 2249.8. Samples: 199430. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:14:09,300][25130] Avg episode reward: [(0, '52.244')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:14:13,799][25742] Updated weights for policy 0, policy_version 17301 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:14:14,300][25130] Fps is (10 sec: 9011.4, 60 sec: 9011.2, 300 sec: 8642.5). Total num frames: 70868992. Throughput: 0: 2238.5. Samples: 212806. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:14:14,301][25130] Avg episode reward: [(0, '52.016')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:14:18,350][25742] Updated weights for policy 0, policy_version 17311 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:14:19,299][25130] Fps is (10 sec: 9011.3, 60 sec: 9011.2, 300 sec: 8660.1). Total num frames: 70914048. Throughput: 0: 2239.8. Samples: 226268. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:14:19,300][25130] Avg episode reward: [(0, '51.572')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:14:22,912][25742] Updated weights for policy 0, policy_version 17321 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:14:24,299][25130] Fps is (10 sec: 9011.8, 60 sec: 9011.2, 300 sec: 8676.1). Total num frames: 70959104. Throughput: 0: 2246.0. Samples: 233128. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:14:24,300][25130] Avg episode reward: [(0, '51.048')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:14:24,303][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000017324_70959104.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:14:24,450][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000016993_69603328.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:14:27,561][25742] Updated weights for policy 0, policy_version 17331 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:14:29,299][25130] Fps is (10 sec: 8601.4, 60 sec: 8942.9, 300 sec: 8655.0). Total num frames: 71000064. Throughput: 0: 2242.2. Samples: 246430. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:14:29,302][25130] Avg episode reward: [(0, '51.642')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:14:32,155][25742] Updated weights for policy 0, policy_version 17341 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:14:34,299][25130] Fps is (10 sec: 8601.5, 60 sec: 8942.9, 300 sec: 8669.9). Total num frames: 71045120. Throughput: 0: 2241.8. Samples: 259768. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:14:34,300][25130] Avg episode reward: [(0, '52.851')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:14:36,771][25742] Updated weights for policy 0, policy_version 17351 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:14:39,299][25130] Fps is (10 sec: 9011.4, 60 sec: 8943.0, 300 sec: 8683.5). Total num frames: 71090176. Throughput: 0: 2239.3. Samples: 266430. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:14:39,300][25130] Avg episode reward: [(0, '52.769')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:14:41,376][25742] Updated weights for policy 0, policy_version 17361 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:14:44,299][25130] Fps is (10 sec: 9011.3, 60 sec: 8943.0, 300 sec: 8696.1). Total num frames: 71135232. Throughput: 0: 2233.1. Samples: 279608. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:14:44,302][25130] Avg episode reward: [(0, '52.735')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:14:46,013][25742] Updated weights for policy 0, policy_version 17371 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:14:49,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8942.9, 300 sec: 8707.8). Total num frames: 71180288. Throughput: 0: 2225.2. Samples: 292846. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:14:49,300][25130] Avg episode reward: [(0, '52.861')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:14:50,641][25742] Updated weights for policy 0, policy_version 17381 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:14:54,299][25130] Fps is (10 sec: 8601.5, 60 sec: 8874.8, 300 sec: 8689.4). Total num frames: 71221248. Throughput: 0: 2226.9. Samples: 299642. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:14:54,300][25130] Avg episode reward: [(0, '51.577')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:14:55,246][25742] Updated weights for policy 0, policy_version 17391 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:14:59,299][25130] Fps is (10 sec: 8601.5, 60 sec: 8874.8, 300 sec: 8700.5). Total num frames: 71266304. Throughput: 0: 2224.3. Samples: 312900. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:14:59,302][25130] Avg episode reward: [(0, '50.494')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:14:59,906][25742] Updated weights for policy 0, policy_version 17401 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:15:04,299][25130] Fps is (10 sec: 9011.3, 60 sec: 8874.8, 300 sec: 8710.8). Total num frames: 71311360. Throughput: 0: 2219.1. Samples: 326128. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:15:04,300][25130] Avg episode reward: [(0, '51.180')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:15:04,595][25742] Updated weights for policy 0, policy_version 17411 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:15:09,259][25742] Updated weights for policy 0, policy_version 17421 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:15:09,299][25130] Fps is (10 sec: 9011.1, 60 sec: 8874.7, 300 sec: 8720.5). Total num frames: 71356416. Throughput: 0: 2210.6. Samples: 332606. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:15:09,300][25130] Avg episode reward: [(0, '52.207')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:15:13,861][25742] Updated weights for policy 0, policy_version 17431 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:15:14,299][25130] Fps is (10 sec: 8601.6, 60 sec: 8806.5, 300 sec: 8704.0). Total num frames: 71397376. Throughput: 0: 2210.4. Samples: 345898. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:15:14,300][25130] Avg episode reward: [(0, '53.778')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:15:18,525][25742] Updated weights for policy 0, policy_version 17441 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:15:19,299][25130] Fps is (10 sec: 8601.6, 60 sec: 8806.4, 300 sec: 8713.3). Total num frames: 71442432. Throughput: 0: 2207.6. Samples: 359112. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:15:19,300][25130] Avg episode reward: [(0, '54.804')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:15:23,122][25742] Updated weights for policy 0, policy_version 17451 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:15:24,299][25130] Fps is (10 sec: 9011.1, 60 sec: 8806.4, 300 sec: 8722.1). Total num frames: 71487488. Throughput: 0: 2212.2. Samples: 365980. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:15:24,300][25130] Avg episode reward: [(0, '54.896')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:15:27,749][25742] Updated weights for policy 0, policy_version 17461 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:15:29,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8874.7, 300 sec: 8730.3). Total num frames: 71532544. Throughput: 0: 2214.3. Samples: 379252. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:15:29,300][25130] Avg episode reward: [(0, '54.279')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:15:32,382][25742] Updated weights for policy 0, policy_version 17471 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:15:34,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8874.7, 300 sec: 8738.1). Total num frames: 71577600. Throughput: 0: 2211.1. Samples: 392344. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:15:34,300][25130] Avg episode reward: [(0, '54.452')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:15:37,022][25742] Updated weights for policy 0, policy_version 17481 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:15:39,300][25130] Fps is (10 sec: 8601.3, 60 sec: 8806.3, 300 sec: 8723.4). Total num frames: 71618560. Throughput: 0: 2208.9. Samples: 399042. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:15:39,301][25130] Avg episode reward: [(0, '54.030')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:15:41,635][25742] Updated weights for policy 0, policy_version 17491 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:15:44,299][25130] Fps is (10 sec: 8601.7, 60 sec: 8806.4, 300 sec: 8731.0). Total num frames: 71663616. Throughput: 0: 2208.4. Samples: 412276. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:15:44,300][25130] Avg episode reward: [(0, '53.276')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:15:46,290][25742] Updated weights for policy 0, policy_version 17501 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:15:49,299][25130] Fps is (10 sec: 9011.5, 60 sec: 8806.4, 300 sec: 8738.1). Total num frames: 71708672. Throughput: 0: 2209.1. Samples: 425540. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:15:49,300][25130] Avg episode reward: [(0, '52.560')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:15:50,962][25742] Updated weights for policy 0, policy_version 17511 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:15:54,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8874.7, 300 sec: 8745.0). Total num frames: 71753728. Throughput: 0: 2210.3. Samples: 432068. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:15:54,300][25130] Avg episode reward: [(0, '51.647')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:15:55,522][25742] Updated weights for policy 0, policy_version 17521 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:15:59,299][25130] Fps is (10 sec: 9011.3, 60 sec: 8874.7, 300 sec: 8751.5). Total num frames: 71798784. Throughput: 0: 2213.0. Samples: 445482. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:15:59,300][25130] Avg episode reward: [(0, '50.756')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:16:00,147][25742] Updated weights for policy 0, policy_version 17531 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:16:04,299][25130] Fps is (10 sec: 8601.6, 60 sec: 8806.4, 300 sec: 8738.1). Total num frames: 71839744. Throughput: 0: 2214.5. Samples: 458762. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:16:04,300][25130] Avg episode reward: [(0, '51.543')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:16:04,780][25742] Updated weights for policy 0, policy_version 17541 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:16:09,299][25130] Fps is (10 sec: 8601.7, 60 sec: 8806.4, 300 sec: 8744.5). Total num frames: 71884800. Throughput: 0: 2211.6. Samples: 465502. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:16:09,300][25130] Avg episode reward: [(0, '52.115')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:16:09,402][25742] Updated weights for policy 0, policy_version 17551 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:16:14,064][25742] Updated weights for policy 0, policy_version 17561 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:16:14,299][25130] Fps is (10 sec: 9011.1, 60 sec: 8874.7, 300 sec: 8750.5). Total num frames: 71929856. Throughput: 0: 2212.0. Samples: 478794. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:16:14,300][25130] Avg episode reward: [(0, '52.572')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:16:18,650][25742] Updated weights for policy 0, policy_version 17571 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:16:19,299][25130] Fps is (10 sec: 9011.1, 60 sec: 8874.7, 300 sec: 8756.3). Total num frames: 71974912. Throughput: 0: 2216.9. Samples: 492106. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:16:19,300][25130] Avg episode reward: [(0, '53.459')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:16:23,278][25742] Updated weights for policy 0, policy_version 17581 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:16:24,299][25130] Fps is (10 sec: 9011.3, 60 sec: 8874.7, 300 sec: 8761.9). Total num frames: 72019968. Throughput: 0: 2212.2. Samples: 498592. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:16:24,300][25130] Avg episode reward: [(0, '52.500')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:16:24,650][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000017584_72024064.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:16:24,731][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000017091_70004736.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:16:27,888][25742] Updated weights for policy 0, policy_version 17591 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:16:29,299][25130] Fps is (10 sec: 8601.6, 60 sec: 8806.4, 300 sec: 8749.8). Total num frames: 72060928. Throughput: 0: 2214.3. Samples: 511920. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:16:29,300][25130] Avg episode reward: [(0, '51.845')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:16:32,520][25742] Updated weights for policy 0, policy_version 17601 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:16:34,299][25130] Fps is (10 sec: 8601.5, 60 sec: 8806.4, 300 sec: 8755.2). Total num frames: 72105984. Throughput: 0: 2213.7. Samples: 525156. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:16:34,300][25130] Avg episode reward: [(0, '50.902')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:16:37,164][25742] Updated weights for policy 0, policy_version 17611 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:16:39,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8874.7, 300 sec: 8760.4). Total num frames: 72151040. Throughput: 0: 2220.0. Samples: 531970. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:16:39,300][25130] Avg episode reward: [(0, '50.716')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:16:41,801][25742] Updated weights for policy 0, policy_version 17621 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:16:44,299][25130] Fps is (10 sec: 9011.3, 60 sec: 8874.7, 300 sec: 8765.4). Total num frames: 72196096. Throughput: 0: 2216.7. Samples: 545232. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:16:44,300][25130] Avg episode reward: [(0, '51.315')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:16:46,456][25742] Updated weights for policy 0, policy_version 17631 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:16:49,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8874.7, 300 sec: 8770.3). Total num frames: 72241152. Throughput: 0: 2216.0. Samples: 558482. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:16:49,300][25130] Avg episode reward: [(0, '52.104')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:16:51,079][25742] Updated weights for policy 0, policy_version 17641 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:16:54,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8874.7, 300 sec: 8774.9). Total num frames: 72286208. Throughput: 0: 2210.1. Samples: 564956. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:16:54,300][25130] Avg episode reward: [(0, '52.272')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:16:55,650][25742] Updated weights for policy 0, policy_version 17651 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:16:59,300][25130] Fps is (10 sec: 8601.3, 60 sec: 8806.3, 300 sec: 8763.9). Total num frames: 72327168. Throughput: 0: 2214.1. Samples: 578428. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:16:59,301][25130] Avg episode reward: [(0, '51.060')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:17:00,196][25742] Updated weights for policy 0, policy_version 17661 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:17:04,299][25130] Fps is (10 sec: 8601.6, 60 sec: 8874.7, 300 sec: 8768.5). Total num frames: 72372224. Throughput: 0: 2215.0. Samples: 591782. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:17:04,300][25130] Avg episode reward: [(0, '51.602')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:17:04,786][25742] Updated weights for policy 0, policy_version 17671 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:17:09,299][25130] Fps is (10 sec: 9011.6, 60 sec: 8874.7, 300 sec: 8772.9). Total num frames: 72417280. Throughput: 0: 2222.5. Samples: 598604. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:17:09,300][25130] Avg episode reward: [(0, '51.600')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:17:09,370][25742] Updated weights for policy 0, policy_version 17681 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:17:13,936][25742] Updated weights for policy 0, policy_version 17691 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:17:14,299][25130] Fps is (10 sec: 9011.1, 60 sec: 8874.7, 300 sec: 8777.1). Total num frames: 72462336. Throughput: 0: 2224.5. Samples: 612022. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:17:14,300][25130] Avg episode reward: [(0, '51.538')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:17:18,486][25742] Updated weights for policy 0, policy_version 17701 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:17:19,300][25130] Fps is (10 sec: 9010.8, 60 sec: 8874.6, 300 sec: 8781.2). Total num frames: 72507392. Throughput: 0: 2229.0. Samples: 625460. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:17:19,301][25130] Avg episode reward: [(0, '52.929')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:17:23,067][25742] Updated weights for policy 0, policy_version 17711 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:17:24,299][25130] Fps is (10 sec: 9011.3, 60 sec: 8874.7, 300 sec: 8785.2). Total num frames: 72552448. Throughput: 0: 2229.5. Samples: 632298. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:17:24,300][25130] Avg episode reward: [(0, '52.871')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:17:27,620][25742] Updated weights for policy 0, policy_version 17721 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:17:29,299][25130] Fps is (10 sec: 9011.5, 60 sec: 8942.9, 300 sec: 8789.0). Total num frames: 72597504. Throughput: 0: 2232.7. Samples: 645704. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:17:29,300][25130] Avg episode reward: [(0, '52.305')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:17:32,233][25742] Updated weights for policy 0, policy_version 17731 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:17:34,299][25130] Fps is (10 sec: 9011.1, 60 sec: 8942.9, 300 sec: 8927.9). Total num frames: 72642560. Throughput: 0: 2234.6. Samples: 659038. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:17:34,301][25130] Avg episode reward: [(0, '52.746')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:17:36,848][25742] Updated weights for policy 0, policy_version 17741 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:17:39,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8942.9, 300 sec: 8927.9). Total num frames: 72687616. Throughput: 0: 2235.5. Samples: 665554. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:17:39,300][25130] Avg episode reward: [(0, '52.775')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:17:41,445][25742] Updated weights for policy 0, policy_version 17751 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:17:44,300][25130] Fps is (10 sec: 9010.3, 60 sec: 8942.8, 300 sec: 8927.9). Total num frames: 72732672. Throughput: 0: 2232.7. Samples: 678900. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:17:44,302][25130] Avg episode reward: [(0, '52.120')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:17:46,041][25742] Updated weights for policy 0, policy_version 17761 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:17:49,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8942.9, 300 sec: 8927.9). Total num frames: 72777728. Throughput: 0: 2233.4. Samples: 692286. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:17:49,300][25130] Avg episode reward: [(0, '51.564')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:17:50,625][25742] Updated weights for policy 0, policy_version 17771 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:17:54,299][25130] Fps is (10 sec: 9012.2, 60 sec: 8942.9, 300 sec: 8927.9). Total num frames: 72822784. Throughput: 0: 2233.6. Samples: 699118. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:17:54,300][25130] Avg episode reward: [(0, '51.470')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:17:55,166][25742] Updated weights for policy 0, policy_version 17781 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:17:59,299][25130] Fps is (10 sec: 8601.6, 60 sec: 8943.0, 300 sec: 8900.1). Total num frames: 72863744. Throughput: 0: 2232.9. Samples: 712504. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:17:59,300][25130] Avg episode reward: [(0, '50.776')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:17:59,788][25742] Updated weights for policy 0, policy_version 17791 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:18:04,299][25130] Fps is (10 sec: 8601.5, 60 sec: 8942.9, 300 sec: 8900.1). Total num frames: 72908800. Throughput: 0: 2231.4. Samples: 725872. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:18:04,300][25130] Avg episode reward: [(0, '48.355')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:18:04,356][25742] Updated weights for policy 0, policy_version 17801 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:18:08,914][25742] Updated weights for policy 0, policy_version 17811 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:18:09,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8942.9, 300 sec: 8900.1). Total num frames: 72953856. Throughput: 0: 2231.4. Samples: 732710. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:18:09,300][25130] Avg episode reward: [(0, '49.294')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:18:13,470][25742] Updated weights for policy 0, policy_version 17821 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:18:14,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8942.9, 300 sec: 8900.1). Total num frames: 72998912. Throughput: 0: 2232.3. Samples: 746158. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:18:14,300][25130] Avg episode reward: [(0, '49.557')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:18:18,029][25742] Updated weights for policy 0, policy_version 17831 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:18:19,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8943.0, 300 sec: 8900.1). Total num frames: 73043968. Throughput: 0: 2234.9. Samples: 759610. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:18:19,300][25130] Avg episode reward: [(0, '50.767')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:18:22,594][25742] Updated weights for policy 0, policy_version 17841 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:18:24,299][25130] Fps is (10 sec: 9011.3, 60 sec: 8942.9, 300 sec: 8900.1). Total num frames: 73089024. Throughput: 0: 2242.2. Samples: 766452. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:18:24,300][25130] Avg episode reward: [(0, '52.917')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:18:24,418][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000017845_73093120.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:18:24,508][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000017324_70959104.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:18:27,234][25742] Updated weights for policy 0, policy_version 17851 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:18:29,299][25130] Fps is (10 sec: 9011.3, 60 sec: 8942.9, 300 sec: 8900.1). Total num frames: 73134080. Throughput: 0: 2240.8. Samples: 779732. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:18:29,300][25130] Avg episode reward: [(0, '52.588')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:18:31,859][25742] Updated weights for policy 0, policy_version 17861 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:18:34,299][25130] Fps is (10 sec: 9011.1, 60 sec: 8942.9, 300 sec: 8900.1). Total num frames: 73179136. Throughput: 0: 2238.5. Samples: 793020. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:18:34,300][25130] Avg episode reward: [(0, '53.010')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:18:36,446][25742] Updated weights for policy 0, policy_version 17871 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:18:39,300][25130] Fps is (10 sec: 9010.8, 60 sec: 8942.9, 300 sec: 8900.1). Total num frames: 73224192. Throughput: 0: 2233.0. Samples: 799602. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:18:39,300][25130] Avg episode reward: [(0, '51.871')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:18:40,977][25742] Updated weights for policy 0, policy_version 17881 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:18:44,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8943.1, 300 sec: 8900.1). Total num frames: 73269248. Throughput: 0: 2234.0. Samples: 813036. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:18:44,300][25130] Avg episode reward: [(0, '51.321')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:18:45,545][25742] Updated weights for policy 0, policy_version 17891 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:18:49,300][25130] Fps is (10 sec: 9010.9, 60 sec: 8942.8, 300 sec: 8900.1). Total num frames: 73314304. Throughput: 0: 2239.7. Samples: 826662. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:18:49,304][25130] Avg episode reward: [(0, '52.121')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:18:50,112][25742] Updated weights for policy 0, policy_version 17901 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:18:54,299][25130] Fps is (10 sec: 8601.7, 60 sec: 8874.7, 300 sec: 8886.3). Total num frames: 73355264. Throughput: 0: 2235.8. Samples: 833322. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:18:54,300][25130] Avg episode reward: [(0, '49.857')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:18:54,770][25742] Updated weights for policy 0, policy_version 17911 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:18:59,299][25130] Fps is (10 sec: 8602.3, 60 sec: 8943.0, 300 sec: 8886.3). Total num frames: 73400320. Throughput: 0: 2231.6. Samples: 846578. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:18:59,300][25130] Avg episode reward: [(0, '48.882')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:18:59,406][25742] Updated weights for policy 0, policy_version 17921 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:19:03,980][25742] Updated weights for policy 0, policy_version 17931 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:19:04,300][25130] Fps is (10 sec: 9010.6, 60 sec: 8942.8, 300 sec: 8886.2). Total num frames: 73445376. Throughput: 0: 2228.6. Samples: 859900. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:19:04,304][25130] Avg episode reward: [(0, '50.032')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:19:08,541][25742] Updated weights for policy 0, policy_version 17941 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:19:09,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8942.9, 300 sec: 8886.3). Total num frames: 73490432. Throughput: 0: 2228.8. Samples: 866748. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:19:09,300][25130] Avg episode reward: [(0, '50.060')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:19:13,134][25742] Updated weights for policy 0, policy_version 17951 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:19:14,300][25130] Fps is (10 sec: 9011.5, 60 sec: 8942.9, 300 sec: 8886.2). Total num frames: 73535488. Throughput: 0: 2231.1. Samples: 880132. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:19:14,301][25130] Avg episode reward: [(0, '51.564')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:19:17,707][25742] Updated weights for policy 0, policy_version 17961 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:19:19,299][25130] Fps is (10 sec: 9011.1, 60 sec: 8942.9, 300 sec: 8886.2). Total num frames: 73580544. Throughput: 0: 2233.7. Samples: 893536. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:19:19,300][25130] Avg episode reward: [(0, '52.416')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:19:22,272][25742] Updated weights for policy 0, policy_version 17971 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:19:24,299][25130] Fps is (10 sec: 9011.5, 60 sec: 8942.9, 300 sec: 8900.1). Total num frames: 73625600. Throughput: 0: 2233.5. Samples: 900108. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:19:24,300][25130] Avg episode reward: [(0, '51.597')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:19:26,841][25742] Updated weights for policy 0, policy_version 17981 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:19:29,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8942.9, 300 sec: 8900.1). Total num frames: 73670656. Throughput: 0: 2236.9. Samples: 913696. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:19:29,300][25130] Avg episode reward: [(0, '50.784')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:19:31,459][25742] Updated weights for policy 0, policy_version 17991 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:19:34,299][25130] Fps is (10 sec: 9011.3, 60 sec: 8942.9, 300 sec: 8900.1). Total num frames: 73715712. Throughput: 0: 2233.0. Samples: 927144. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:19:34,300][25130] Avg episode reward: [(0, '49.836')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:19:35,982][25742] Updated weights for policy 0, policy_version 18001 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:19:39,300][25130] Fps is (10 sec: 9010.9, 60 sec: 8942.9, 300 sec: 8900.1). Total num frames: 73760768. Throughput: 0: 2232.7. Samples: 933794. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:19:39,300][25130] Avg episode reward: [(0, '50.452')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:19:40,537][25742] Updated weights for policy 0, policy_version 18011 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:19:44,301][25130] Fps is (10 sec: 9010.1, 60 sec: 8942.8, 300 sec: 8900.1). Total num frames: 73805824. Throughput: 0: 2236.1. Samples: 947206. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:19:44,303][25130] Avg episode reward: [(0, '52.338')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:19:45,121][25742] Updated weights for policy 0, policy_version 18021 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:19:49,299][25130] Fps is (10 sec: 9011.6, 60 sec: 8943.1, 300 sec: 8914.0). Total num frames: 73850880. Throughput: 0: 2246.2. Samples: 960976. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:19:49,300][25130] Avg episode reward: [(0, '53.159')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:19:49,636][25742] Updated weights for policy 0, policy_version 18031 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:19:54,243][25742] Updated weights for policy 0, policy_version 18041 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:19:54,299][25130] Fps is (10 sec: 9012.3, 60 sec: 9011.2, 300 sec: 8914.0). Total num frames: 73895936. Throughput: 0: 2239.9. Samples: 967544. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:19:54,300][25130] Avg episode reward: [(0, '52.392')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:19:58,806][25742] Updated weights for policy 0, policy_version 18051 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:19:59,299][25130] Fps is (10 sec: 9011.2, 60 sec: 9011.2, 300 sec: 8914.0). Total num frames: 73940992. Throughput: 0: 2239.0. Samples: 980884. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:19:59,300][25130] Avg episode reward: [(0, '52.525')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:20:03,394][25742] Updated weights for policy 0, policy_version 18061 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:20:04,299][25130] Fps is (10 sec: 8601.6, 60 sec: 8943.0, 300 sec: 8900.1). Total num frames: 73981952. Throughput: 0: 2238.4. Samples: 994262. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:20:04,300][25130] Avg episode reward: [(0, '51.823')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:20:07,998][25742] Updated weights for policy 0, policy_version 18071 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:20:09,299][25130] Fps is (10 sec: 8601.5, 60 sec: 8942.9, 300 sec: 8914.0). Total num frames: 74027008. Throughput: 0: 2243.5. Samples: 1001066. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:20:09,300][25130] Avg episode reward: [(0, '51.708')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:20:12,582][25742] Updated weights for policy 0, policy_version 18081 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:20:14,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8943.0, 300 sec: 8914.0). Total num frames: 74072064. Throughput: 0: 2239.0. Samples: 1014450. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:20:14,300][25130] Avg episode reward: [(0, '51.784')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:20:17,147][25742] Updated weights for policy 0, policy_version 18091 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:20:19,299][25130] Fps is (10 sec: 9011.3, 60 sec: 8942.9, 300 sec: 8914.0). Total num frames: 74117120. Throughput: 0: 2238.0. Samples: 1027852. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:20:19,300][25130] Avg episode reward: [(0, '52.812')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:20:21,714][25742] Updated weights for policy 0, policy_version 18101 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:20:24,299][25130] Fps is (10 sec: 9011.3, 60 sec: 8942.9, 300 sec: 8914.0). Total num frames: 74162176. Throughput: 0: 2241.1. Samples: 1034644. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:20:24,300][25130] Avg episode reward: [(0, '53.013')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:20:24,542][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000018107_74166272.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:20:24,630][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000017584_72024064.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:20:26,434][25742] Updated weights for policy 0, policy_version 18111 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:20:29,299][25130] Fps is (10 sec: 9011.1, 60 sec: 8942.9, 300 sec: 8914.0). Total num frames: 74207232. Throughput: 0: 2229.6. Samples: 1047534. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:20:29,301][25130] Avg episode reward: [(0, '53.696')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:20:31,018][25742] Updated weights for policy 0, policy_version 18121 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:20:34,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8942.9, 300 sec: 8927.9). Total num frames: 74252288. Throughput: 0: 2220.3. Samples: 1060890. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:20:34,300][25130] Avg episode reward: [(0, '53.344')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:20:35,629][25742] Updated weights for policy 0, policy_version 18131 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:20:39,300][25130] Fps is (10 sec: 8601.3, 60 sec: 8874.7, 300 sec: 8914.0). Total num frames: 74293248. Throughput: 0: 2225.9. Samples: 1067712. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:20:39,301][25130] Avg episode reward: [(0, '54.445')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:20:40,208][25742] Updated weights for policy 0, policy_version 18141 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:20:44,299][25130] Fps is (10 sec: 8601.5, 60 sec: 8874.8, 300 sec: 8914.0). Total num frames: 74338304. Throughput: 0: 2225.3. Samples: 1081024. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:20:44,300][25130] Avg episode reward: [(0, '54.314')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:20:44,803][25742] Updated weights for policy 0, policy_version 18151 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:20:49,299][25130] Fps is (10 sec: 9011.6, 60 sec: 8874.7, 300 sec: 8914.0). Total num frames: 74383360. Throughput: 0: 2225.3. Samples: 1094400. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:20:49,300][25130] Avg episode reward: [(0, '53.748')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:20:49,436][25742] Updated weights for policy 0, policy_version 18161 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:20:54,045][25742] Updated weights for policy 0, policy_version 18171 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:20:54,300][25130] Fps is (10 sec: 9010.7, 60 sec: 8874.6, 300 sec: 8914.0). Total num frames: 74428416. Throughput: 0: 2224.6. Samples: 1101176. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:20:54,301][25130] Avg episode reward: [(0, '53.465')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:20:58,654][25742] Updated weights for policy 0, policy_version 18181 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:20:59,299][25130] Fps is (10 sec: 9011.1, 60 sec: 8874.6, 300 sec: 8927.9). Total num frames: 74473472. Throughput: 0: 2223.1. Samples: 1114488. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:20:59,301][25130] Avg episode reward: [(0, '53.736')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:21:03,254][25742] Updated weights for policy 0, policy_version 18191 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:21:04,299][25130] Fps is (10 sec: 9011.8, 60 sec: 8942.9, 300 sec: 8927.9). Total num frames: 74518528. Throughput: 0: 2222.0. Samples: 1127842. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:21:04,300][25130] Avg episode reward: [(0, '52.121')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:21:07,856][25742] Updated weights for policy 0, policy_version 18201 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:21:09,299][25130] Fps is (10 sec: 9011.4, 60 sec: 8943.0, 300 sec: 8927.9). Total num frames: 74563584. Throughput: 0: 2217.1. Samples: 1134412. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:21:09,300][25130] Avg episode reward: [(0, '50.650')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:21:12,460][25742] Updated weights for policy 0, policy_version 18211 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:21:14,299][25130] Fps is (10 sec: 8601.5, 60 sec: 8874.7, 300 sec: 8914.0). Total num frames: 74604544. Throughput: 0: 2226.0. Samples: 1147704. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:21:14,300][25130] Avg episode reward: [(0, '50.190')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:21:17,072][25742] Updated weights for policy 0, policy_version 18221 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:21:19,299][25130] Fps is (10 sec: 8601.5, 60 sec: 8874.7, 300 sec: 8914.0). Total num frames: 74649600. Throughput: 0: 2225.6. Samples: 1161044. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:21:19,300][25130] Avg episode reward: [(0, '51.179')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:21:21,661][25742] Updated weights for policy 0, policy_version 18231 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:21:24,299][25130] Fps is (10 sec: 9011.3, 60 sec: 8874.7, 300 sec: 8927.9). Total num frames: 74694656. Throughput: 0: 2226.2. Samples: 1167888. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:21:24,300][25130] Avg episode reward: [(0, '51.190')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:21:26,271][25742] Updated weights for policy 0, policy_version 18241 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:21:29,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8874.7, 300 sec: 8927.9). Total num frames: 74739712. Throughput: 0: 2226.3. Samples: 1181206. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:21:29,300][25130] Avg episode reward: [(0, '51.851')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:21:30,866][25742] Updated weights for policy 0, policy_version 18251 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:21:34,300][25130] Fps is (10 sec: 9010.8, 60 sec: 8874.6, 300 sec: 8927.9). Total num frames: 74784768. Throughput: 0: 2226.2. Samples: 1194582. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:21:34,301][25130] Avg episode reward: [(0, '52.913')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:21:35,449][25742] Updated weights for policy 0, policy_version 18261 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:21:39,299][25130] Fps is (10 sec: 9011.3, 60 sec: 8943.0, 300 sec: 8927.9). Total num frames: 74829824. Throughput: 0: 2221.0. Samples: 1201120. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:21:39,300][25130] Avg episode reward: [(0, '52.832')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:21:40,097][25742] Updated weights for policy 0, policy_version 18271 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:21:44,299][25130] Fps is (10 sec: 9011.5, 60 sec: 8942.9, 300 sec: 8927.9). Total num frames: 74874880. Throughput: 0: 2219.9. Samples: 1214382. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:21:44,304][25130] Avg episode reward: [(0, '51.424')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:21:44,677][25742] Updated weights for policy 0, policy_version 18281 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:21:49,276][25742] Updated weights for policy 0, policy_version 18291 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:21:49,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8942.9, 300 sec: 8927.9). Total num frames: 74919936. Throughput: 0: 2220.0. Samples: 1227744. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:21:49,300][25130] Avg episode reward: [(0, '50.978')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:21:53,890][25742] Updated weights for policy 0, policy_version 18301 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:21:54,299][25130] Fps is (10 sec: 8601.7, 60 sec: 8874.8, 300 sec: 8927.9). Total num frames: 74960896. Throughput: 0: 2225.0. Samples: 1234538. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:21:54,300][25130] Avg episode reward: [(0, '50.485')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:21:58,497][25742] Updated weights for policy 0, policy_version 18311 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:21:59,300][25130] Fps is (10 sec: 8601.2, 60 sec: 8874.6, 300 sec: 8927.9). Total num frames: 75005952. Throughput: 0: 2226.7. Samples: 1247908. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:21:59,300][25130] Avg episode reward: [(0, '49.387')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:22:03,133][25742] Updated weights for policy 0, policy_version 18321 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:22:04,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8874.7, 300 sec: 8927.9). Total num frames: 75051008. Throughput: 0: 2223.6. Samples: 1261108. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:22:04,300][25130] Avg episode reward: [(0, '51.415')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:22:07,723][25742] Updated weights for policy 0, policy_version 18331 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:22:09,299][25130] Fps is (10 sec: 9011.5, 60 sec: 8874.6, 300 sec: 8927.9). Total num frames: 75096064. Throughput: 0: 2217.4. Samples: 1267672. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:22:09,300][25130] Avg episode reward: [(0, '51.904')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:22:12,314][25742] Updated weights for policy 0, policy_version 18341 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:22:14,300][25130] Fps is (10 sec: 9010.8, 60 sec: 8942.9, 300 sec: 8927.9). Total num frames: 75141120. Throughput: 0: 2221.5. Samples: 1281176. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:22:14,301][25130] Avg episode reward: [(0, '53.866')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:22:16,925][25742] Updated weights for policy 0, policy_version 18351 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:22:19,299][25130] Fps is (10 sec: 9011.3, 60 sec: 8942.9, 300 sec: 8927.9). Total num frames: 75186176. Throughput: 0: 2217.6. Samples: 1294372. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:22:19,300][25130] Avg episode reward: [(0, '54.177')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:22:21,551][25742] Updated weights for policy 0, policy_version 18361 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:22:24,299][25130] Fps is (10 sec: 9011.6, 60 sec: 8942.9, 300 sec: 8927.9). Total num frames: 75231232. Throughput: 0: 2222.4. Samples: 1301128. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:22:24,300][25130] Avg episode reward: [(0, '54.517')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:22:24,303][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000018367_75231232.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:22:24,401][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000017845_73093120.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:22:26,213][25742] Updated weights for policy 0, policy_version 18371 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:22:29,299][25130] Fps is (10 sec: 8601.5, 60 sec: 8874.7, 300 sec: 8914.0). Total num frames: 75272192. Throughput: 0: 2221.2. Samples: 1314336. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:22:29,300][25130] Avg episode reward: [(0, '52.391')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:22:30,845][25742] Updated weights for policy 0, policy_version 18381 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:22:34,299][25130] Fps is (10 sec: 8601.5, 60 sec: 8874.7, 300 sec: 8914.0). Total num frames: 75317248. Throughput: 0: 2221.2. Samples: 1327700. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:22:34,300][25130] Avg episode reward: [(0, '50.806')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:22:35,403][25742] Updated weights for policy 0, policy_version 18391 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:22:39,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8874.7, 300 sec: 8914.0). Total num frames: 75362304. Throughput: 0: 2216.5. Samples: 1334282. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:22:39,301][25130] Avg episode reward: [(0, '51.650')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:22:40,026][25742] Updated weights for policy 0, policy_version 18401 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:22:44,300][25130] Fps is (10 sec: 9010.6, 60 sec: 8874.6, 300 sec: 8914.0). Total num frames: 75407360. Throughput: 0: 2212.3. Samples: 1347464. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:22:44,304][25130] Avg episode reward: [(0, '50.944')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:22:44,676][25742] Updated weights for policy 0, policy_version 18411 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:22:49,292][25742] Updated weights for policy 0, policy_version 18421 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:22:49,299][25130] Fps is (10 sec: 9011.3, 60 sec: 8874.7, 300 sec: 8914.0). Total num frames: 75452416. Throughput: 0: 2215.5. Samples: 1360804. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:22:49,300][25130] Avg episode reward: [(0, '50.224')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:22:53,930][25742] Updated weights for policy 0, policy_version 18431 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:22:54,299][25130] Fps is (10 sec: 8602.2, 60 sec: 8874.7, 300 sec: 8914.0). Total num frames: 75493376. Throughput: 0: 2219.9. Samples: 1367566. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:22:54,300][25130] Avg episode reward: [(0, '51.188')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:22:58,543][25742] Updated weights for policy 0, policy_version 18441 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:22:59,300][25130] Fps is (10 sec: 8601.1, 60 sec: 8874.6, 300 sec: 8914.0). Total num frames: 75538432. Throughput: 0: 2215.5. Samples: 1380872. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:22:59,301][25130] Avg episode reward: [(0, '49.788')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:23:03,207][25742] Updated weights for policy 0, policy_version 18451 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:23:04,299][25130] Fps is (10 sec: 9011.1, 60 sec: 8874.7, 300 sec: 8914.0). Total num frames: 75583488. Throughput: 0: 2216.3. Samples: 1394108. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:23:04,300][25130] Avg episode reward: [(0, '50.081')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:23:07,811][25742] Updated weights for policy 0, policy_version 18461 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:23:09,299][25130] Fps is (10 sec: 9011.6, 60 sec: 8874.7, 300 sec: 8914.0). Total num frames: 75628544. Throughput: 0: 2211.0. Samples: 1400622. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:23:09,300][25130] Avg episode reward: [(0, '50.373')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:23:12,402][25742] Updated weights for policy 0, policy_version 18471 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:23:14,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8874.7, 300 sec: 8914.0). Total num frames: 75673600. Throughput: 0: 2214.8. Samples: 1414004. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:23:14,300][25130] Avg episode reward: [(0, '51.386')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:23:17,140][25742] Updated weights for policy 0, policy_version 18481 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:23:19,300][25130] Fps is (10 sec: 8601.1, 60 sec: 8806.3, 300 sec: 8900.1). Total num frames: 75714560. Throughput: 0: 2208.5. Samples: 1427082. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:23:19,302][25130] Avg episode reward: [(0, '53.076')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:23:21,806][25742] Updated weights for policy 0, policy_version 18491 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:23:24,299][25130] Fps is (10 sec: 8601.7, 60 sec: 8806.4, 300 sec: 8900.1). Total num frames: 75759616. Throughput: 0: 2206.3. Samples: 1433566. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:23:24,300][25130] Avg episode reward: [(0, '52.924')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:23:26,441][25742] Updated weights for policy 0, policy_version 18501 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:23:29,299][25130] Fps is (10 sec: 9011.9, 60 sec: 8874.7, 300 sec: 8900.1). Total num frames: 75804672. Throughput: 0: 2208.8. Samples: 1446858. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:23:29,300][25130] Avg episode reward: [(0, '51.667')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:23:31,112][25742] Updated weights for policy 0, policy_version 18511 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:23:34,300][25130] Fps is (10 sec: 8601.2, 60 sec: 8806.4, 300 sec: 8886.2). Total num frames: 75845632. Throughput: 0: 2206.1. Samples: 1460080. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:23:34,301][25130] Avg episode reward: [(0, '51.924')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:23:35,726][25742] Updated weights for policy 0, policy_version 18521 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:23:39,301][25130] Fps is (10 sec: 8600.6, 60 sec: 8806.2, 300 sec: 8886.2). Total num frames: 75890688. Throughput: 0: 2207.0. Samples: 1466884. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:23:39,302][25130] Avg episode reward: [(0, '51.791')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:23:40,405][25742] Updated weights for policy 0, policy_version 18531 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:23:44,299][25130] Fps is (10 sec: 9011.5, 60 sec: 8806.5, 300 sec: 8886.3). Total num frames: 75935744. Throughput: 0: 2205.5. Samples: 1480118. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:23:44,300][25130] Avg episode reward: [(0, '50.048')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:23:45,007][25742] Updated weights for policy 0, policy_version 18541 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:23:49,299][25130] Fps is (10 sec: 9012.1, 60 sec: 8806.4, 300 sec: 8900.1). Total num frames: 75980800. Throughput: 0: 2207.1. Samples: 1493426. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:23:49,300][25130] Avg episode reward: [(0, '51.250')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:23:49,648][25742] Updated weights for policy 0, policy_version 18551 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:23:54,276][25742] Updated weights for policy 0, policy_version 18561 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:23:54,299][25130] Fps is (10 sec: 9011.3, 60 sec: 8874.7, 300 sec: 8900.1). Total num frames: 76025856. Throughput: 0: 2206.8. Samples: 1499926. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:23:54,300][25130] Avg episode reward: [(0, '51.999')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:23:58,880][25742] Updated weights for policy 0, policy_version 18571 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:23:59,299][25130] Fps is (10 sec: 8601.6, 60 sec: 8806.5, 300 sec: 8886.3). Total num frames: 76066816. Throughput: 0: 2203.7. Samples: 1513172. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:23:59,300][25130] Avg episode reward: [(0, '53.136')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:24:03,552][25742] Updated weights for policy 0, policy_version 18581 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:24:04,300][25130] Fps is (10 sec: 8601.0, 60 sec: 8806.3, 300 sec: 8886.2). Total num frames: 76111872. Throughput: 0: 2207.7. Samples: 1526428. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:24:04,301][25130] Avg episode reward: [(0, '53.177')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:24:08,227][25742] Updated weights for policy 0, policy_version 18591 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:24:09,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8806.4, 300 sec: 8886.2). Total num frames: 76156928. Throughput: 0: 2206.5. Samples: 1532858. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:24:09,301][25130] Avg episode reward: [(0, '53.123')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:24:12,835][25742] Updated weights for policy 0, policy_version 18601 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:24:14,299][25130] Fps is (10 sec: 9011.7, 60 sec: 8806.4, 300 sec: 8886.2). Total num frames: 76201984. Throughput: 0: 2207.0. Samples: 1546172. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:24:14,304][25130] Avg episode reward: [(0, '51.885')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:24:17,497][25742] Updated weights for policy 0, policy_version 18611 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:24:19,300][25130] Fps is (10 sec: 8601.3, 60 sec: 8806.4, 300 sec: 8872.3). Total num frames: 76242944. Throughput: 0: 2205.8. Samples: 1559340. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:24:19,301][25130] Avg episode reward: [(0, '51.526')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:24:22,144][25742] Updated weights for policy 0, policy_version 18621 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:24:24,299][25130] Fps is (10 sec: 8601.6, 60 sec: 8806.4, 300 sec: 8872.4). Total num frames: 76288000. Throughput: 0: 2205.1. Samples: 1566110. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:24:24,300][25130] Avg episode reward: [(0, '50.827')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:24:24,478][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000018626_76292096.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:24:24,567][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000018107_74166272.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:24:26,835][25742] Updated weights for policy 0, policy_version 18631 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:24:29,300][25130] Fps is (10 sec: 9010.8, 60 sec: 8806.3, 300 sec: 8872.3). Total num frames: 76333056. Throughput: 0: 2201.6. Samples: 1579192. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:24:29,301][25130] Avg episode reward: [(0, '50.890')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:24:31,546][25742] Updated weights for policy 0, policy_version 18641 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:24:34,299][25130] Fps is (10 sec: 8601.7, 60 sec: 8806.5, 300 sec: 8858.5). Total num frames: 76374016. Throughput: 0: 2193.2. Samples: 1592118. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:24:34,300][25130] Avg episode reward: [(0, '50.483')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:24:36,239][25742] Updated weights for policy 0, policy_version 18651 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:24:39,299][25130] Fps is (10 sec: 8602.3, 60 sec: 8806.6, 300 sec: 8858.5). Total num frames: 76419072. Throughput: 0: 2197.4. Samples: 1598808. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:24:39,300][25130] Avg episode reward: [(0, '50.025')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:24:41,016][25742] Updated weights for policy 0, policy_version 18661 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:24:44,299][25130] Fps is (10 sec: 8601.5, 60 sec: 8738.1, 300 sec: 8844.6). Total num frames: 76460032. Throughput: 0: 2185.9. Samples: 1611538. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:24:44,300][25130] Avg episode reward: [(0, '49.431')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:24:45,683][25742] Updated weights for policy 0, policy_version 18671 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:24:49,299][25130] Fps is (10 sec: 8601.5, 60 sec: 8738.1, 300 sec: 8844.6). Total num frames: 76505088. Throughput: 0: 2185.9. Samples: 1624792. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:24:49,300][25130] Avg episode reward: [(0, '50.271')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:24:50,337][25742] Updated weights for policy 0, policy_version 18681 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:24:54,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8738.1, 300 sec: 8844.6). Total num frames: 76550144. Throughput: 0: 2192.4. Samples: 1631514. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:24:54,300][25130] Avg episode reward: [(0, '52.401')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:24:54,999][25742] Updated weights for policy 0, policy_version 18691 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:24:59,300][25130] Fps is (10 sec: 9010.7, 60 sec: 8806.3, 300 sec: 8858.4). Total num frames: 76595200. Throughput: 0: 2184.3. Samples: 1644466. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:24:59,304][25130] Avg episode reward: [(0, '52.555')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:24:59,681][25742] Updated weights for policy 0, policy_version 18701 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:25:04,300][25130] Fps is (10 sec: 8601.3, 60 sec: 8738.2, 300 sec: 8844.6). Total num frames: 76636160. Throughput: 0: 2185.8. Samples: 1657700. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:25:04,301][25130] Avg episode reward: [(0, '53.160')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:25:04,307][25742] Updated weights for policy 0, policy_version 18711 (0.0013)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:25:08,946][25742] Updated weights for policy 0, policy_version 18721 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:25:09,299][25130] Fps is (10 sec: 8602.1, 60 sec: 8738.1, 300 sec: 8844.6). Total num frames: 76681216. Throughput: 0: 2185.3. Samples: 1664450. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:25:09,300][25130] Avg episode reward: [(0, '52.394')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:25:13,554][25742] Updated weights for policy 0, policy_version 18731 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:25:14,299][25130] Fps is (10 sec: 9011.6, 60 sec: 8738.1, 300 sec: 8844.6). Total num frames: 76726272. Throughput: 0: 2190.3. Samples: 1677756. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:25:14,300][25130] Avg episode reward: [(0, '53.356')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:25:18,173][25742] Updated weights for policy 0, policy_version 18741 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:25:19,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8806.4, 300 sec: 8844.6). Total num frames: 76771328. Throughput: 0: 2198.6. Samples: 1691054. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:25:19,300][25130] Avg episode reward: [(0, '52.977')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:25:22,805][25742] Updated weights for policy 0, policy_version 18751 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:25:24,299][25130] Fps is (10 sec: 9011.0, 60 sec: 8806.4, 300 sec: 8844.6). Total num frames: 76816384. Throughput: 0: 2194.8. Samples: 1697576. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:25:24,300][25130] Avg episode reward: [(0, '52.840')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:25:27,412][25742] Updated weights for policy 0, policy_version 18761 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:25:29,300][25130] Fps is (10 sec: 9011.0, 60 sec: 8806.5, 300 sec: 8844.6). Total num frames: 76861440. Throughput: 0: 2207.7. Samples: 1710886. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:25:29,301][25130] Avg episode reward: [(0, '53.810')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:25:32,072][25742] Updated weights for policy 0, policy_version 18771 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:25:34,300][25130] Fps is (10 sec: 8601.3, 60 sec: 8806.3, 300 sec: 8844.6). Total num frames: 76902400. Throughput: 0: 2206.4. Samples: 1724080. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:25:34,301][25130] Avg episode reward: [(0, '54.083')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:25:36,703][25742] Updated weights for policy 0, policy_version 18781 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:25:39,299][25130] Fps is (10 sec: 8601.9, 60 sec: 8806.4, 300 sec: 8844.6). Total num frames: 76947456. Throughput: 0: 2208.5. Samples: 1730898. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:25:39,300][25130] Avg episode reward: [(0, '54.661')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:25:41,478][25742] Updated weights for policy 0, policy_version 18791 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:25:44,299][25130] Fps is (10 sec: 8602.0, 60 sec: 8806.4, 300 sec: 8830.7). Total num frames: 76988416. Throughput: 0: 2201.8. Samples: 1743544. Policy #0 lag: (min: 0.0, avg: 0.8, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:25:44,300][25130] Avg episode reward: [(0, '54.172')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:25:46,148][25742] Updated weights for policy 0, policy_version 18801 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:25:49,300][25130] Fps is (10 sec: 8601.3, 60 sec: 8806.4, 300 sec: 8830.7). Total num frames: 77033472. Throughput: 0: 2205.6. Samples: 1756954. Policy #0 lag: (min: 0.0, avg: 0.8, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:25:49,301][25130] Avg episode reward: [(0, '54.205')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:25:50,715][25742] Updated weights for policy 0, policy_version 18811 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:25:54,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8806.4, 300 sec: 8830.7). Total num frames: 77078528. Throughput: 0: 2204.8. Samples: 1763664. Policy #0 lag: (min: 0.0, avg: 0.8, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:25:54,300][25130] Avg episode reward: [(0, '53.914')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:25:55,457][25742] Updated weights for policy 0, policy_version 18821 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:25:59,299][25130] Fps is (10 sec: 9011.3, 60 sec: 8806.5, 300 sec: 8830.7). Total num frames: 77123584. Throughput: 0: 2202.4. Samples: 1776864. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:25:59,300][25130] Avg episode reward: [(0, '53.814')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:26:00,045][25742] Updated weights for policy 0, policy_version 18831 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:26:04,299][25130] Fps is (10 sec: 9011.3, 60 sec: 8874.7, 300 sec: 8830.7). Total num frames: 77168640. Throughput: 0: 2200.1. Samples: 1790056. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:26:04,300][25130] Avg episode reward: [(0, '53.968')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:26:04,690][25742] Updated weights for policy 0, policy_version 18841 (0.0013)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:26:09,250][25742] Updated weights for policy 0, policy_version 18851 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:26:09,299][25130] Fps is (10 sec: 9011.4, 60 sec: 8874.7, 300 sec: 8844.6). Total num frames: 77213696. Throughput: 0: 2204.1. Samples: 1796762. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:26:09,300][25130] Avg episode reward: [(0, '53.334')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:26:13,865][25742] Updated weights for policy 0, policy_version 18861 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:26:14,299][25130] Fps is (10 sec: 8601.5, 60 sec: 8806.4, 300 sec: 8830.7). Total num frames: 77254656. Throughput: 0: 2205.7. Samples: 1810140. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:26:14,300][25130] Avg episode reward: [(0, '53.419')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:26:18,536][25742] Updated weights for policy 0, policy_version 18871 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:26:19,300][25130] Fps is (10 sec: 8601.2, 60 sec: 8806.4, 300 sec: 8830.7). Total num frames: 77299712. Throughput: 0: 2206.4. Samples: 1823366. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:26:19,301][25130] Avg episode reward: [(0, '54.143')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:26:23,164][25742] Updated weights for policy 0, policy_version 18881 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:26:24,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8806.4, 300 sec: 8830.7). Total num frames: 77344768. Throughput: 0: 2198.7. Samples: 1829838. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:26:24,301][25130] Avg episode reward: [(0, '54.419')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:26:24,546][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000018884_77348864.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:26:24,640][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000018367_75231232.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:26:27,782][25742] Updated weights for policy 0, policy_version 18891 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:26:29,299][25130] Fps is (10 sec: 9011.5, 60 sec: 8806.4, 300 sec: 8830.7). Total num frames: 77389824. Throughput: 0: 2215.1. Samples: 1843224. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:26:29,301][25130] Avg episode reward: [(0, '55.406')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:26:32,450][25742] Updated weights for policy 0, policy_version 18901 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:26:34,299][25130] Fps is (10 sec: 8601.8, 60 sec: 8806.5, 300 sec: 8816.8). Total num frames: 77430784. Throughput: 0: 2209.5. Samples: 1856380. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:26:34,300][25130] Avg episode reward: [(0, '54.907')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:26:37,061][25742] Updated weights for policy 0, policy_version 18911 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:26:39,299][25130] Fps is (10 sec: 8601.7, 60 sec: 8806.4, 300 sec: 8816.8). Total num frames: 77475840. Throughput: 0: 2211.4. Samples: 1863176. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:26:39,300][25130] Avg episode reward: [(0, '55.298')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:26:41,671][25742] Updated weights for policy 0, policy_version 18921 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:26:44,300][25130] Fps is (10 sec: 9010.8, 60 sec: 8874.6, 300 sec: 8816.8). Total num frames: 77520896. Throughput: 0: 2214.8. Samples: 1876532. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:26:44,300][25130] Avg episode reward: [(0, '52.767')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:26:46,272][25742] Updated weights for policy 0, policy_version 18931 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:26:49,299][25130] Fps is (10 sec: 9011.1, 60 sec: 8874.7, 300 sec: 8830.7). Total num frames: 77565952. Throughput: 0: 2218.0. Samples: 1889864. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:26:49,300][25130] Avg episode reward: [(0, '52.601')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:26:50,888][25742] Updated weights for policy 0, policy_version 18941 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:26:54,299][25130] Fps is (10 sec: 9011.4, 60 sec: 8874.7, 300 sec: 8830.7). Total num frames: 77611008. Throughput: 0: 2212.6. Samples: 1896328. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:26:54,300][25130] Avg episode reward: [(0, '51.534')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:26:55,527][25742] Updated weights for policy 0, policy_version 18951 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:26:59,300][25130] Fps is (10 sec: 9011.0, 60 sec: 8874.7, 300 sec: 8830.7). Total num frames: 77656064. Throughput: 0: 2210.2. Samples: 1909598. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:26:59,301][25130] Avg episode reward: [(0, '52.256')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:27:00,239][25742] Updated weights for policy 0, policy_version 18961 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:27:04,299][25130] Fps is (10 sec: 8601.7, 60 sec: 8806.4, 300 sec: 8816.8). Total num frames: 77697024. Throughput: 0: 2207.1. Samples: 1922684. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:27:04,300][25130] Avg episode reward: [(0, '52.606')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:27:04,935][25742] Updated weights for policy 0, policy_version 18971 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:27:09,299][25130] Fps is (10 sec: 8601.8, 60 sec: 8806.4, 300 sec: 8816.8). Total num frames: 77742080. Throughput: 0: 2206.1. Samples: 1929112. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:27:09,300][25130] Avg episode reward: [(0, '52.810')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:27:09,655][25742] Updated weights for policy 0, policy_version 18981 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:27:14,299][25130] Fps is (10 sec: 8601.6, 60 sec: 8806.4, 300 sec: 8802.9). Total num frames: 77783040. Throughput: 0: 2200.3. Samples: 1942236. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:27:14,300][25130] Avg episode reward: [(0, '54.763')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:27:14,332][25742] Updated weights for policy 0, policy_version 18991 (0.0015)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:27:18,993][25742] Updated weights for policy 0, policy_version 19001 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:27:19,300][25130] Fps is (10 sec: 8601.5, 60 sec: 8806.4, 300 sec: 8802.9). Total num frames: 77828096. Throughput: 0: 2203.2. Samples: 1955526. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:27:19,301][25130] Avg episode reward: [(0, '54.773')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:27:23,719][25742] Updated weights for policy 0, policy_version 19011 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:27:24,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8806.4, 300 sec: 8816.8). Total num frames: 77873152. Throughput: 0: 2194.1. Samples: 1961912. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:27:24,301][25130] Avg episode reward: [(0, '54.146')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:27:28,315][25742] Updated weights for policy 0, policy_version 19021 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:27:29,299][25130] Fps is (10 sec: 9011.4, 60 sec: 8806.4, 300 sec: 8816.8). Total num frames: 77918208. Throughput: 0: 2191.5. Samples: 1975148. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:27:29,300][25130] Avg episode reward: [(0, '53.493')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:27:33,001][25742] Updated weights for policy 0, policy_version 19031 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:27:34,299][25130] Fps is (10 sec: 8601.5, 60 sec: 8806.4, 300 sec: 8802.9). Total num frames: 77959168. Throughput: 0: 2189.1. Samples: 1988376. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:27:34,301][25130] Avg episode reward: [(0, '54.198')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:27:37,738][25742] Updated weights for policy 0, policy_version 19041 (0.0016)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:27:39,300][25130] Fps is (10 sec: 8600.9, 60 sec: 8806.3, 300 sec: 8802.9). Total num frames: 78004224. Throughput: 0: 2189.2. Samples: 1994844. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:27:39,305][25130] Avg episode reward: [(0, '52.628')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:27:42,507][25742] Updated weights for policy 0, policy_version 19051 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:27:44,299][25130] Fps is (10 sec: 8601.7, 60 sec: 8738.2, 300 sec: 8789.0). Total num frames: 78045184. Throughput: 0: 2181.7. Samples: 2007774. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:27:44,301][25130] Avg episode reward: [(0, '51.537')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:27:47,291][25742] Updated weights for policy 0, policy_version 19061 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:27:49,299][25130] Fps is (10 sec: 8602.3, 60 sec: 8738.1, 300 sec: 8802.9). Total num frames: 78090240. Throughput: 0: 2176.9. Samples: 2020646. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:27:49,300][25130] Avg episode reward: [(0, '51.815')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:27:52,139][25742] Updated weights for policy 0, policy_version 19071 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:27:54,299][25130] Fps is (10 sec: 8601.7, 60 sec: 8669.9, 300 sec: 8789.1). Total num frames: 78131200. Throughput: 0: 2174.0. Samples: 2026944. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:27:54,300][25130] Avg episode reward: [(0, '50.180')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:27:56,894][25742] Updated weights for policy 0, policy_version 19081 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:27:59,299][25130] Fps is (10 sec: 8192.0, 60 sec: 8601.6, 300 sec: 8775.2). Total num frames: 78172160. Throughput: 0: 2165.9. Samples: 2039700. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:27:59,300][25130] Avg episode reward: [(0, '50.803')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:28:01,704][25742] Updated weights for policy 0, policy_version 19091 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:28:04,299][25130] Fps is (10 sec: 8601.6, 60 sec: 8669.9, 300 sec: 8775.2). Total num frames: 78217216. Throughput: 0: 2156.8. Samples: 2052582. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:28:04,300][25130] Avg episode reward: [(0, '51.018')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:28:06,489][25742] Updated weights for policy 0, policy_version 19101 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:28:09,299][25130] Fps is (10 sec: 8601.5, 60 sec: 8601.6, 300 sec: 8761.3). Total num frames: 78258176. Throughput: 0: 2156.1. Samples: 2058938. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:28:09,300][25130] Avg episode reward: [(0, '51.179')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:28:11,340][25742] Updated weights for policy 0, policy_version 19111 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:28:14,299][25130] Fps is (10 sec: 8601.5, 60 sec: 8669.9, 300 sec: 8775.2). Total num frames: 78303232. Throughput: 0: 2146.8. Samples: 2071752. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:28:14,300][25130] Avg episode reward: [(0, '51.853')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:28:16,152][25742] Updated weights for policy 0, policy_version 19121 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:28:19,299][25130] Fps is (10 sec: 8601.7, 60 sec: 8601.6, 300 sec: 8761.3). Total num frames: 78344192. Throughput: 0: 2135.3. Samples: 2084466. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:28:19,301][25130] Avg episode reward: [(0, '54.033')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:28:20,963][25742] Updated weights for policy 0, policy_version 19131 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:28:24,299][25130] Fps is (10 sec: 8191.9, 60 sec: 8533.3, 300 sec: 8747.4). Total num frames: 78385152. Throughput: 0: 2131.5. Samples: 2090758. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:28:24,300][25130] Avg episode reward: [(0, '53.837')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:28:24,304][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000019138_78389248.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:28:24,436][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000018626_76292096.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:28:25,751][25742] Updated weights for policy 0, policy_version 19141 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:28:29,299][25130] Fps is (10 sec: 8601.6, 60 sec: 8533.3, 300 sec: 8761.3). Total num frames: 78430208. Throughput: 0: 2132.7. Samples: 2103744. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:28:29,300][25130] Avg episode reward: [(0, '53.798')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:28:30,521][25742] Updated weights for policy 0, policy_version 19151 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:28:34,299][25130] Fps is (10 sec: 8601.7, 60 sec: 8533.4, 300 sec: 8747.4). Total num frames: 78471168. Throughput: 0: 2128.3. Samples: 2116418. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:28:34,300][25130] Avg episode reward: [(0, '53.422')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:28:35,326][25742] Updated weights for policy 0, policy_version 19161 (0.0016)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:28:39,299][25130] Fps is (10 sec: 8601.6, 60 sec: 8533.5, 300 sec: 8747.4). Total num frames: 78516224. Throughput: 0: 2131.4. Samples: 2122858. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:28:39,300][25130] Avg episode reward: [(0, '51.663')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:28:40,098][25742] Updated weights for policy 0, policy_version 19171 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:28:44,299][25130] Fps is (10 sec: 8601.6, 60 sec: 8533.4, 300 sec: 8733.5). Total num frames: 78557184. Throughput: 0: 2133.4. Samples: 2135702. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:28:44,300][25130] Avg episode reward: [(0, '52.027')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:28:44,880][25742] Updated weights for policy 0, policy_version 19181 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:28:49,299][25130] Fps is (10 sec: 8601.5, 60 sec: 8533.3, 300 sec: 8733.5). Total num frames: 78602240. Throughput: 0: 2138.6. Samples: 2148818. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:28:49,300][25130] Avg episode reward: [(0, '51.919')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:28:49,549][25742] Updated weights for policy 0, policy_version 19191 (0.0014)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:28:54,132][25742] Updated weights for policy 0, policy_version 19201 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:28:54,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8601.6, 300 sec: 8747.4). Total num frames: 78647296. Throughput: 0: 2143.3. Samples: 2155384. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:28:54,300][25130] Avg episode reward: [(0, '53.177')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:28:58,832][25742] Updated weights for policy 0, policy_version 19211 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:28:59,299][25130] Fps is (10 sec: 8601.7, 60 sec: 8601.6, 300 sec: 8733.5). Total num frames: 78688256. Throughput: 0: 2153.3. Samples: 2168652. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:28:59,300][25130] Avg episode reward: [(0, '52.774')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:29:04,300][25130] Fps is (10 sec: 6552.9, 60 sec: 8260.1, 300 sec: 8664.1). Total num frames: 78712832. Throughput: 0: 2069.0. Samples: 2177574. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:29:04,305][25130] Avg episode reward: [(0, '53.831')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:29:09,300][25130] Fps is (10 sec: 3686.1, 60 sec: 7782.3, 300 sec: 8553.0). Total num frames: 78725120. Throughput: 0: 1980.0. Samples: 2179858. Policy #0 lag: (min: 0.0, avg: 1.0, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:29:09,304][25130] Avg episode reward: [(0, '53.638')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:29:10,158][25742] Updated weights for policy 0, policy_version 19221 (0.0070)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:29:14,301][25130] Fps is (10 sec: 2457.6, 60 sec: 7236.1, 300 sec: 8455.8). Total num frames: 78737408. Throughput: 0: 1782.6. Samples: 2183962. Policy #0 lag: (min: 0.0, avg: 1.0, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:29:14,305][25130] Avg episode reward: [(0, '53.138')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:29:19,300][25130] Fps is (10 sec: 2867.2, 60 sec: 6826.6, 300 sec: 8358.6). Total num frames: 78753792. Throughput: 0: 1591.6. Samples: 2188042. Policy #0 lag: (min: 0.0, avg: 1.0, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:29:19,304][25130] Avg episode reward: [(0, '53.335')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:29:24,300][25130] Fps is (10 sec: 2867.3, 60 sec: 6348.7, 300 sec: 8247.5). Total num frames: 78766080. Throughput: 0: 1492.9. Samples: 2190038. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:29:24,304][25130] Avg episode reward: [(0, '53.487')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:29:25,226][25742] Updated weights for policy 0, policy_version 19231 (0.0080)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:29:29,300][25130] Fps is (10 sec: 2457.6, 60 sec: 5802.6, 300 sec: 8150.3). Total num frames: 78778368. Throughput: 0: 1296.3. Samples: 2194036. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:29:29,305][25130] Avg episode reward: [(0, '53.843')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:29:34,300][25130] Fps is (10 sec: 2457.6, 60 sec: 5324.7, 300 sec: 8039.2). Total num frames: 78790656. Throughput: 0: 1093.3. Samples: 2198016. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:29:34,304][25130] Avg episode reward: [(0, '53.783')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:29:39,300][25130] Fps is (10 sec: 2867.2, 60 sec: 4846.9, 300 sec: 7955.9). Total num frames: 78807040. Throughput: 0: 991.5. Samples: 2200002. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:29:39,304][25130] Avg episode reward: [(0, '53.671')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:29:40,700][25742] Updated weights for policy 0, policy_version 19241 (0.0080)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:29:44,300][25130] Fps is (10 sec: 2867.2, 60 sec: 4369.0, 300 sec: 7844.9). Total num frames: 78819328. Throughput: 0: 784.2. Samples: 2203940. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:29:44,305][25130] Avg episode reward: [(0, '53.431')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:29:49,300][25130] Fps is (10 sec: 2457.6, 60 sec: 3822.9, 300 sec: 7733.8). Total num frames: 78831616. Throughput: 0: 674.0. Samples: 2207902. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:29:49,305][25130] Avg episode reward: [(0, '53.471')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:29:54,300][25130] Fps is (10 sec: 2457.5, 60 sec: 3276.7, 300 sec: 7622.7). Total num frames: 78843904. Throughput: 0: 667.5. Samples: 2209894. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:29:54,305][25130] Avg episode reward: [(0, '53.886')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:29:56,047][25742] Updated weights for policy 0, policy_version 19251 (0.0083)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:29:59,300][25130] Fps is (10 sec: 2867.2, 60 sec: 2867.2, 300 sec: 7539.4). Total num frames: 78860288. Throughput: 0: 665.9. Samples: 2213926. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:29:59,305][25130] Avg episode reward: [(0, '54.698')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:30:04,300][25130] Fps is (10 sec: 2867.3, 60 sec: 2662.4, 300 sec: 7428.3). Total num frames: 78872576. Throughput: 0: 664.2. Samples: 2217930. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:30:04,304][25130] Avg episode reward: [(0, '53.206')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:30:09,300][25130] Fps is (10 sec: 2457.6, 60 sec: 2662.4, 300 sec: 7317.2). Total num frames: 78884864. Throughput: 0: 664.9. Samples: 2219960. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:30:09,305][25130] Avg episode reward: [(0, '53.264')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:30:10,378][25742] Updated weights for policy 0, policy_version 19261 (0.0080)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:30:14,299][25130] Fps is (10 sec: 3277.0, 60 sec: 2799.0, 300 sec: 7234.0). Total num frames: 78905344. Throughput: 0: 678.8. Samples: 2224582. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:30:14,300][25130] Avg episode reward: [(0, '52.723')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:30:17,134][25742] Updated weights for policy 0, policy_version 19271 (0.0036)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:30:19,300][25130] Fps is (10 sec: 6553.9, 60 sec: 3276.8, 300 sec: 7233.9). Total num frames: 78950400. Throughput: 0: 843.8. Samples: 2235986. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:30:19,301][25130] Avg episode reward: [(0, '52.044')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:30:21,844][25742] Updated weights for policy 0, policy_version 19281 (0.0016)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:30:24,299][25130] Fps is (10 sec: 9011.3, 60 sec: 3823.0, 300 sec: 7234.0). Total num frames: 78995456. Throughput: 0: 941.7. Samples: 2242376. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:30:24,300][25130] Avg episode reward: [(0, '50.555')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:30:24,614][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000019287_78999552.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:30:24,719][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000018884_77348864.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:30:26,475][25742] Updated weights for policy 0, policy_version 19291 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:30:29,299][25130] Fps is (10 sec: 8602.0, 60 sec: 4300.9, 300 sec: 7234.0). Total num frames: 79036416. Throughput: 0: 1149.3. Samples: 2255656. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:30:29,301][25130] Avg episode reward: [(0, '50.110')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:30:31,360][25742] Updated weights for policy 0, policy_version 19301 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:30:34,299][25130] Fps is (10 sec: 8601.5, 60 sec: 4847.0, 300 sec: 7234.0). Total num frames: 79081472. Throughput: 0: 1347.2. Samples: 2268524. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:30:34,300][25130] Avg episode reward: [(0, '51.534')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:30:36,061][25742] Updated weights for policy 0, policy_version 19311 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:30:39,299][25130] Fps is (10 sec: 8601.6, 60 sec: 5256.6, 300 sec: 7234.0). Total num frames: 79122432. Throughput: 0: 1446.1. Samples: 2274966. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:30:39,300][25130] Avg episode reward: [(0, '52.333')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:30:40,687][25742] Updated weights for policy 0, policy_version 19321 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:30:44,300][25130] Fps is (10 sec: 8601.1, 60 sec: 5802.7, 300 sec: 7233.9). Total num frames: 79167488. Throughput: 0: 1652.4. Samples: 2288282. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:30:44,301][25130] Avg episode reward: [(0, '53.235')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:30:45,295][25742] Updated weights for policy 0, policy_version 19331 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:30:49,299][25130] Fps is (10 sec: 9011.3, 60 sec: 6348.9, 300 sec: 7234.0). Total num frames: 79212544. Throughput: 0: 1858.4. Samples: 2301558. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:30:49,300][25130] Avg episode reward: [(0, '54.259')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:30:50,000][25742] Updated weights for policy 0, policy_version 19341 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:30:54,299][25130] Fps is (10 sec: 9011.7, 60 sec: 6895.0, 300 sec: 7234.0). Total num frames: 79257600. Throughput: 0: 1955.8. Samples: 2307968. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:30:54,301][25130] Avg episode reward: [(0, '55.188')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:30:54,650][25742] Updated weights for policy 0, policy_version 19351 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:30:59,299][25130] Fps is (10 sec: 8601.6, 60 sec: 7304.6, 300 sec: 7220.1). Total num frames: 79298560. Throughput: 0: 2143.8. Samples: 2321052. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:30:59,301][25130] Avg episode reward: [(0, '54.118')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:30:59,471][25742] Updated weights for policy 0, policy_version 19361 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:31:04,128][25742] Updated weights for policy 0, policy_version 19371 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:31:04,300][25130] Fps is (10 sec: 8601.1, 60 sec: 7850.7, 300 sec: 7220.1). Total num frames: 79343616. Throughput: 0: 2181.2. Samples: 2334142. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:31:04,301][25130] Avg episode reward: [(0, '54.116')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:31:08,853][25742] Updated weights for policy 0, policy_version 19381 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:31:09,299][25130] Fps is (10 sec: 8601.6, 60 sec: 8328.7, 300 sec: 7220.1). Total num frames: 79384576. Throughput: 0: 2183.1. Samples: 2340616. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:31:09,301][25130] Avg episode reward: [(0, '53.357')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:31:13,617][25742] Updated weights for policy 0, policy_version 19391 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:31:14,299][25130] Fps is (10 sec: 8602.2, 60 sec: 8738.2, 300 sec: 7220.1). Total num frames: 79429632. Throughput: 0: 2177.5. Samples: 2353642. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:31:14,300][25130] Avg episode reward: [(0, '51.799')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:31:18,283][25742] Updated weights for policy 0, policy_version 19401 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:31:19,299][25130] Fps is (10 sec: 9011.3, 60 sec: 8738.2, 300 sec: 7220.1). Total num frames: 79474688. Throughput: 0: 2183.8. Samples: 2366794. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:31:19,300][25130] Avg episode reward: [(0, '53.016')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:31:23,046][25742] Updated weights for policy 0, policy_version 19411 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:31:24,299][25130] Fps is (10 sec: 8601.5, 60 sec: 8669.9, 300 sec: 7206.2). Total num frames: 79515648. Throughput: 0: 2181.1. Samples: 2373114. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:31:24,300][25130] Avg episode reward: [(0, '53.917')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:31:27,752][25742] Updated weights for policy 0, policy_version 19421 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:31:29,299][25130] Fps is (10 sec: 8601.6, 60 sec: 8738.1, 300 sec: 7220.1). Total num frames: 79560704. Throughput: 0: 2175.6. Samples: 2386182. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:31:29,300][25130] Avg episode reward: [(0, '54.352')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:31:32,469][25742] Updated weights for policy 0, policy_version 19431 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:31:34,299][25130] Fps is (10 sec: 8601.6, 60 sec: 8669.9, 300 sec: 7206.2). Total num frames: 79601664. Throughput: 0: 2166.4. Samples: 2399046. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:31:34,300][25130] Avg episode reward: [(0, '54.761')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:31:37,249][25742] Updated weights for policy 0, policy_version 19441 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:31:39,299][25130] Fps is (10 sec: 8601.6, 60 sec: 8738.1, 300 sec: 7206.2). Total num frames: 79646720. Throughput: 0: 2167.9. Samples: 2405522. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:31:39,300][25130] Avg episode reward: [(0, '53.144')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:31:41,925][25742] Updated weights for policy 0, policy_version 19451 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:31:44,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8738.2, 300 sec: 7206.2). Total num frames: 79691776. Throughput: 0: 2165.9. Samples: 2418516. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:31:44,300][25130] Avg episode reward: [(0, '52.493')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:31:46,612][25742] Updated weights for policy 0, policy_version 19461 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:31:49,300][25130] Fps is (10 sec: 8601.2, 60 sec: 8669.8, 300 sec: 7192.3). Total num frames: 79732736. Throughput: 0: 2167.7. Samples: 2431688. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:31:49,301][25130] Avg episode reward: [(0, '51.849')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:31:51,365][25742] Updated weights for policy 0, policy_version 19471 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:31:54,299][25130] Fps is (10 sec: 8601.6, 60 sec: 8669.9, 300 sec: 7192.3). Total num frames: 79777792. Throughput: 0: 2164.7. Samples: 2438026. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:31:54,300][25130] Avg episode reward: [(0, '51.420')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:31:56,518][25742] Updated weights for policy 0, policy_version 19481 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:31:59,299][25130] Fps is (10 sec: 8192.3, 60 sec: 8601.6, 300 sec: 7178.4). Total num frames: 79814656. Throughput: 0: 2145.6. Samples: 2450196. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:31:59,300][25130] Avg episode reward: [(0, '53.237')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:32:01,256][25742] Updated weights for policy 0, policy_version 19491 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:32:04,299][25130] Fps is (10 sec: 8191.9, 60 sec: 8601.7, 300 sec: 7178.4). Total num frames: 79859712. Throughput: 0: 2144.0. Samples: 2463276. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:32:04,301][25130] Avg episode reward: [(0, '53.179')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:32:05,946][25742] Updated weights for policy 0, policy_version 19501 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:32:09,299][25130] Fps is (10 sec: 9011.3, 60 sec: 8669.9, 300 sec: 7192.3). Total num frames: 79904768. Throughput: 0: 2147.0. Samples: 2469728. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:32:09,300][25130] Avg episode reward: [(0, '53.064')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:32:10,705][25742] Updated weights for policy 0, policy_version 19511 (0.0023)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:32:14,301][25130] Fps is (10 sec: 6962.6, 60 sec: 8328.4, 300 sec: 7122.9). Total num frames: 79929344. Throughput: 0: 2098.3. Samples: 2480608. Policy #0 lag: (min: 0.0, avg: 0.7, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:32:14,305][25130] Avg episode reward: [(0, '53.064')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:32:19,300][25130] Fps is (10 sec: 3686.1, 60 sec: 7782.3, 300 sec: 7011.8). Total num frames: 79941632. Throughput: 0: 1911.0. Samples: 2485042. Policy #0 lag: (min: 0.0, avg: 0.7, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:32:19,304][25130] Avg episode reward: [(0, '52.738')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:32:24,300][25130] Fps is (10 sec: 2457.6, 60 sec: 7304.4, 300 sec: 6900.7). Total num frames: 79953920. Throughput: 0: 1810.9. Samples: 2487014. Policy #0 lag: (min: 0.0, avg: 0.7, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:32:24,304][25130] Avg episode reward: [(0, '52.648')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:32:25,214][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000019521_79958016.pth...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:32:25,221][25742] Updated weights for policy 0, policy_version 19521 (0.0084)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:32:25,787][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000019138_78389248.pth\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:32:29,300][25130] Fps is (10 sec: 2457.6, 60 sec: 6758.3, 300 sec: 6803.5). Total num frames: 79966208. Throughput: 0: 1611.4. Samples: 2491030. Policy #0 lag: (min: 1.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:32:29,304][25130] Avg episode reward: [(0, '52.807')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:32:34,300][25130] Fps is (10 sec: 2457.6, 60 sec: 6280.4, 300 sec: 6692.4). Total num frames: 79978496. Throughput: 0: 1404.3. Samples: 2494882. Policy #0 lag: (min: 1.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:32:34,304][25130] Avg episode reward: [(0, '53.296')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:32:39,300][25130] Fps is (10 sec: 2867.2, 60 sec: 5802.6, 300 sec: 6609.1). Total num frames: 79994880. Throughput: 0: 1307.5. Samples: 2496864. Policy #0 lag: (min: 1.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:32:39,305][25130] Avg episode reward: [(0, '53.220')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:32:40,804][25742] Updated weights for policy 0, policy_version 19531 (0.0084)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:32:44,301][25130] Fps is (10 sec: 2867.1, 60 sec: 5256.4, 300 sec: 6498.0). Total num frames: 80007168. Throughput: 0: 1124.4. Samples: 2500796. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:32:44,306][25130] Avg episode reward: [(0, '52.963')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:32:49,300][25130] Fps is (10 sec: 2457.6, 60 sec: 4778.6, 300 sec: 6400.9). Total num frames: 80019456. Throughput: 0: 921.9. Samples: 2504764. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:32:49,305][25130] Avg episode reward: [(0, '53.468')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:32:54,300][25130] Fps is (10 sec: 2457.7, 60 sec: 4232.5, 300 sec: 6303.7). Total num frames: 80031744. Throughput: 0: 822.9. Samples: 2506758. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:32:54,304][25130] Avg episode reward: [(0, '53.093')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:32:56,301][25742] Updated weights for policy 0, policy_version 19541 (0.0085)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:32:59,300][25130] Fps is (10 sec: 2457.6, 60 sec: 3822.9, 300 sec: 6192.6). Total num frames: 80044032. Throughput: 0: 669.8. Samples: 2510748. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:32:59,305][25130] Avg episode reward: [(0, '52.923')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:33:04,301][25130] Fps is (10 sec: 2867.2, 60 sec: 3345.0, 300 sec: 6109.3). Total num frames: 80060416. Throughput: 0: 660.2. Samples: 2514752. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:33:04,305][25130] Avg episode reward: [(0, '53.891')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:33:09,300][25130] Fps is (10 sec: 2867.2, 60 sec: 2798.9, 300 sec: 5998.2). Total num frames: 80072704. Throughput: 0: 660.5. Samples: 2516738. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:33:09,305][25130] Avg episode reward: [(0, '54.430')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:33:11,763][25742] Updated weights for policy 0, policy_version 19551 (0.0082)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:33:14,300][25130] Fps is (10 sec: 2457.7, 60 sec: 2594.1, 300 sec: 5901.0). Total num frames: 80084992. Throughput: 0: 659.3. Samples: 2520700. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:33:14,305][25130] Avg episode reward: [(0, '54.420')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:33:19,300][25130] Fps is (10 sec: 2457.6, 60 sec: 2594.1, 300 sec: 5803.8). Total num frames: 80097280. Throughput: 0: 662.2. Samples: 2524682. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:33:19,304][25130] Avg episode reward: [(0, '53.310')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:33:24,300][25130] Fps is (10 sec: 2867.2, 60 sec: 2662.4, 300 sec: 5706.6). Total num frames: 80113664. Throughput: 0: 662.4. Samples: 2526670. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:33:24,305][25130] Avg episode reward: [(0, '53.366')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:33:27,227][25742] Updated weights for policy 0, policy_version 19561 (0.0084)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:33:29,300][25130] Fps is (10 sec: 2867.2, 60 sec: 2662.4, 300 sec: 5609.4). Total num frames: 80125952. Throughput: 0: 663.2. Samples: 2530638. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:33:29,305][25130] Avg episode reward: [(0, '53.526')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:33:34,300][25130] Fps is (10 sec: 2867.2, 60 sec: 2730.7, 300 sec: 5512.2). Total num frames: 80142336. Throughput: 0: 667.2. Samples: 2534790. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:33:34,306][25130] Avg episode reward: [(0, '54.321')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:33:37,678][25742] Updated weights for policy 0, policy_version 19571 (0.0065)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:33:39,300][25130] Fps is (10 sec: 4915.5, 60 sec: 3003.8, 300 sec: 5484.5). Total num frames: 80175104. Throughput: 0: 688.4. Samples: 2537736. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:33:39,301][25130] Avg episode reward: [(0, '53.784')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:33:42,398][25742] Updated weights for policy 0, policy_version 19581 (0.0017)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:33:44,299][25130] Fps is (10 sec: 7783.2, 60 sec: 3549.9, 300 sec: 5484.5). Total num frames: 80220160. Throughput: 0: 886.1. Samples: 2550622. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:33:44,300][25130] Avg episode reward: [(0, '52.652')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:33:47,059][25742] Updated weights for policy 0, policy_version 19591 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:33:49,300][25130] Fps is (10 sec: 8601.3, 60 sec: 4027.7, 300 sec: 5470.6). Total num frames: 80261120. Throughput: 0: 1091.7. Samples: 2563878. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:33:49,301][25130] Avg episode reward: [(0, '52.399')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:33:51,711][25742] Updated weights for policy 0, policy_version 19601 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:33:54,299][25130] Fps is (10 sec: 8601.5, 60 sec: 4573.9, 300 sec: 5484.5). Total num frames: 80306176. Throughput: 0: 1197.3. Samples: 2570616. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:33:54,300][25130] Avg episode reward: [(0, '52.335')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:33:56,389][25742] Updated weights for policy 0, policy_version 19611 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:33:59,300][25130] Fps is (10 sec: 8601.7, 60 sec: 5051.8, 300 sec: 5540.0). Total num frames: 80347136. Throughput: 0: 1392.5. Samples: 2583364. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:33:59,301][25130] Avg episode reward: [(0, '51.318')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:34:01,198][25742] Updated weights for policy 0, policy_version 19621 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:34:04,300][25130] Fps is (10 sec: 8601.0, 60 sec: 5529.6, 300 sec: 5651.1). Total num frames: 80392192. Throughput: 0: 1595.3. Samples: 2596472. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:34:04,304][25130] Avg episode reward: [(0, '53.491')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:34:05,922][25742] Updated weights for policy 0, policy_version 19631 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:34:09,300][25130] Fps is (10 sec: 9010.8, 60 sec: 6075.7, 300 sec: 5762.2). Total num frames: 80437248. Throughput: 0: 1693.2. Samples: 2602864. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:34:09,304][25130] Avg episode reward: [(0, '53.562')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:34:10,622][25742] Updated weights for policy 0, policy_version 19641 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:34:14,299][25130] Fps is (10 sec: 8602.2, 60 sec: 6553.7, 300 sec: 5845.5). Total num frames: 80478208. Throughput: 0: 1897.2. Samples: 2616012. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:34:14,300][25130] Avg episode reward: [(0, '53.055')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:34:15,315][25742] Updated weights for policy 0, policy_version 19651 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:34:19,300][25130] Fps is (10 sec: 8601.8, 60 sec: 7099.8, 300 sec: 5956.6). Total num frames: 80523264. Throughput: 0: 2097.3. Samples: 2629170. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:34:19,305][25130] Avg episode reward: [(0, '53.441')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:34:20,046][25742] Updated weights for policy 0, policy_version 19661 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:34:24,299][25130] Fps is (10 sec: 9011.2, 60 sec: 7577.7, 300 sec: 6067.6). Total num frames: 80568320. Throughput: 0: 2174.0. Samples: 2635564. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:34:24,300][25130] Avg episode reward: [(0, '53.106')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:34:24,303][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000019670_80568320.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:34:24,462][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000019287_78999552.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:34:24,926][25742] Updated weights for policy 0, policy_version 19671 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:34:29,300][25130] Fps is (10 sec: 8601.6, 60 sec: 8055.5, 300 sec: 6164.8). Total num frames: 80609280. Throughput: 0: 2174.8. Samples: 2648488. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:34:29,305][25130] Avg episode reward: [(0, '51.895')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:34:29,638][25742] Updated weights for policy 0, policy_version 19681 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:34:34,299][25130] Fps is (10 sec: 8192.0, 60 sec: 8465.2, 300 sec: 6248.2). Total num frames: 80650240. Throughput: 0: 2162.6. Samples: 2661194. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:34:34,300][25130] Avg episode reward: [(0, '52.729')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:34:34,391][25742] Updated weights for policy 0, policy_version 19691 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:34:39,142][25742] Updated weights for policy 0, policy_version 19701 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:34:39,299][25130] Fps is (10 sec: 8602.2, 60 sec: 8669.9, 300 sec: 6359.2). Total num frames: 80695296. Throughput: 0: 2160.8. Samples: 2667854. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:34:39,300][25130] Avg episode reward: [(0, '53.219')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:34:43,910][25742] Updated weights for policy 0, policy_version 19711 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:34:44,299][25130] Fps is (10 sec: 8601.6, 60 sec: 8601.6, 300 sec: 6456.4). Total num frames: 80736256. Throughput: 0: 2159.7. Samples: 2680550. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:34:44,300][25130] Avg episode reward: [(0, '53.592')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:34:48,682][25742] Updated weights for policy 0, policy_version 19721 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:34:49,299][25130] Fps is (10 sec: 8601.5, 60 sec: 8670.0, 300 sec: 6567.5). Total num frames: 80781312. Throughput: 0: 2158.7. Samples: 2693614. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:34:49,300][25130] Avg episode reward: [(0, '52.876')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:34:53,421][25742] Updated weights for policy 0, policy_version 19731 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:34:54,299][25130] Fps is (10 sec: 8601.5, 60 sec: 8601.6, 300 sec: 6650.8). Total num frames: 80822272. Throughput: 0: 2159.4. Samples: 2700036. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:34:54,300][25130] Avg episode reward: [(0, '51.538')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:34:58,265][25742] Updated weights for policy 0, policy_version 19741 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:34:59,299][25130] Fps is (10 sec: 8192.0, 60 sec: 8601.7, 300 sec: 6748.0). Total num frames: 80863232. Throughput: 0: 2149.0. Samples: 2712718. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:34:59,300][25130] Avg episode reward: [(0, '50.616')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:35:03,108][25742] Updated weights for policy 0, policy_version 19751 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:35:04,300][25130] Fps is (10 sec: 8601.0, 60 sec: 8601.6, 300 sec: 6859.1). Total num frames: 80908288. Throughput: 0: 2141.0. Samples: 2725514. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:35:04,301][25130] Avg episode reward: [(0, '50.893')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:35:08,000][25742] Updated weights for policy 0, policy_version 19761 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:35:09,300][25130] Fps is (10 sec: 8601.0, 60 sec: 8533.4, 300 sec: 6928.5). Total num frames: 80949248. Throughput: 0: 2136.7. Samples: 2731718. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:35:09,305][25130] Avg episode reward: [(0, '51.186')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:35:12,719][25742] Updated weights for policy 0, policy_version 19771 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:35:14,299][25130] Fps is (10 sec: 8602.3, 60 sec: 8601.6, 300 sec: 6928.5). Total num frames: 80994304. Throughput: 0: 2139.8. Samples: 2744776. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:35:14,300][25130] Avg episode reward: [(0, '53.140')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:35:17,430][25742] Updated weights for policy 0, policy_version 19781 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:35:19,299][25130] Fps is (10 sec: 8602.2, 60 sec: 8533.4, 300 sec: 6914.6). Total num frames: 81035264. Throughput: 0: 2141.8. Samples: 2757574. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:35:19,301][25130] Avg episode reward: [(0, '53.742')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:35:23,425][25742] Updated weights for policy 0, policy_version 19791 (0.0038)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:35:24,300][25130] Fps is (10 sec: 6962.6, 60 sec: 8260.1, 300 sec: 6872.9). Total num frames: 81063936. Throughput: 0: 2130.2. Samples: 2763714. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:35:24,305][25130] Avg episode reward: [(0, '53.388')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:35:29,300][25130] Fps is (10 sec: 4095.6, 60 sec: 7782.4, 300 sec: 6761.8). Total num frames: 81076224. Throughput: 0: 1956.5. Samples: 2768596. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:35:29,304][25130] Avg episode reward: [(0, '53.320')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:35:34,300][25130] Fps is (10 sec: 2457.6, 60 sec: 7304.4, 300 sec: 6664.7). Total num frames: 81088512. Throughput: 0: 1754.4. Samples: 2772564. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:35:34,305][25130] Avg episode reward: [(0, '52.677')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:35:39,204][25742] Updated weights for policy 0, policy_version 19801 (0.0083)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:35:39,300][25130] Fps is (10 sec: 2867.3, 60 sec: 6826.6, 300 sec: 6567.5). Total num frames: 81104896. Throughput: 0: 1655.8. Samples: 2774550. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:35:39,305][25130] Avg episode reward: [(0, '52.496')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:35:44,300][25130] Fps is (10 sec: 2867.2, 60 sec: 6348.7, 300 sec: 6456.4). Total num frames: 81117184. Throughput: 0: 1462.3. Samples: 2778524. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:35:44,304][25130] Avg episode reward: [(0, '52.634')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:35:49,300][25130] Fps is (10 sec: 2457.6, 60 sec: 5802.6, 300 sec: 6345.3). Total num frames: 81129472. Throughput: 0: 1265.9. Samples: 2782478. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:35:49,304][25130] Avg episode reward: [(0, '52.311')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:35:54,300][25130] Fps is (10 sec: 2457.5, 60 sec: 5324.7, 300 sec: 6248.1). Total num frames: 81141760. Throughput: 0: 1172.2. Samples: 2784466. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:35:54,304][25130] Avg episode reward: [(0, '51.716')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:35:54,685][25742] Updated weights for policy 0, policy_version 19811 (0.0078)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:35:59,301][25130] Fps is (10 sec: 2457.5, 60 sec: 4846.8, 300 sec: 6137.0). Total num frames: 81154048. Throughput: 0: 969.4. Samples: 2788398. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:35:59,305][25130] Avg episode reward: [(0, '51.550')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:36:04,300][25130] Fps is (10 sec: 2867.2, 60 sec: 4369.1, 300 sec: 6053.7). Total num frames: 81170432. Throughput: 0: 773.7. Samples: 2792392. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:36:04,305][25130] Avg episode reward: [(0, '51.121')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:36:09,300][25130] Fps is (10 sec: 2867.3, 60 sec: 3891.2, 300 sec: 5942.7). Total num frames: 81182720. Throughput: 0: 681.7. Samples: 2794390. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:36:09,305][25130] Avg episode reward: [(0, '51.654')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:36:10,191][25742] Updated weights for policy 0, policy_version 19821 (0.0083)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:36:14,300][25130] Fps is (10 sec: 2457.6, 60 sec: 3345.0, 300 sec: 5831.6). Total num frames: 81195008. Throughput: 0: 661.6. Samples: 2798368. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:36:14,306][25130] Avg episode reward: [(0, '51.591')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:36:19,300][25130] Fps is (10 sec: 2457.6, 60 sec: 2867.2, 300 sec: 5734.4). Total num frames: 81207296. Throughput: 0: 662.6. Samples: 2802380. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:36:19,305][25130] Avg episode reward: [(0, '51.640')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:36:24,301][25130] Fps is (10 sec: 2867.1, 60 sec: 2662.4, 300 sec: 5637.2). Total num frames: 81223680. Throughput: 0: 662.7. Samples: 2804372. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:36:24,306][25130] Avg episode reward: [(0, '50.704')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:36:25,626][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000019831_81227776.pth...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:36:25,640][25742] Updated weights for policy 0, policy_version 19831 (0.0083)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:36:26,181][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000019521_79958016.pth\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:36:29,300][25130] Fps is (10 sec: 2867.3, 60 sec: 2662.4, 300 sec: 5540.0). Total num frames: 81235968. Throughput: 0: 661.4. Samples: 2808286. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:36:29,305][25130] Avg episode reward: [(0, '50.752')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:36:34,300][25130] Fps is (10 sec: 2457.7, 60 sec: 2662.4, 300 sec: 5428.9). Total num frames: 81248256. Throughput: 0: 660.0. Samples: 2812178. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:36:34,304][25130] Avg episode reward: [(0, '50.788')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:36:39,300][25130] Fps is (10 sec: 2457.6, 60 sec: 2594.1, 300 sec: 5317.8). Total num frames: 81260544. Throughput: 0: 660.6. Samples: 2814192. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:36:39,304][25130] Avg episode reward: [(0, '50.377')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:36:41,167][25742] Updated weights for policy 0, policy_version 19841 (0.0082)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:36:44,300][25130] Fps is (10 sec: 2867.2, 60 sec: 2662.4, 300 sec: 5234.5). Total num frames: 81276928. Throughput: 0: 662.3. Samples: 2818202. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:36:44,305][25130] Avg episode reward: [(0, '51.058')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:36:49,301][25130] Fps is (10 sec: 3276.7, 60 sec: 2730.7, 300 sec: 5137.3). Total num frames: 81293312. Throughput: 0: 668.8. Samples: 2822486. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:36:49,305][25130] Avg episode reward: [(0, '51.831')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:36:52,005][25742] Updated weights for policy 0, policy_version 19851 (0.0065)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:36:54,299][25130] Fps is (10 sec: 4915.6, 60 sec: 3072.0, 300 sec: 5123.5). Total num frames: 81326080. Throughput: 0: 703.0. Samples: 2826026. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:36:54,301][25130] Avg episode reward: [(0, '52.819')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:36:56,780][25742] Updated weights for policy 0, policy_version 19861 (0.0017)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:36:59,299][25130] Fps is (10 sec: 7783.1, 60 sec: 3618.2, 300 sec: 5123.5). Total num frames: 81371136. Throughput: 0: 903.5. Samples: 2839026. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:36:59,300][25130] Avg episode reward: [(0, '52.682')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:37:01,436][25742] Updated weights for policy 0, policy_version 19871 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:37:04,299][25130] Fps is (10 sec: 9011.3, 60 sec: 4096.1, 300 sec: 5123.5). Total num frames: 81416192. Throughput: 0: 1107.3. Samples: 2852206. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:37:04,300][25130] Avg episode reward: [(0, '52.132')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:37:06,103][25742] Updated weights for policy 0, policy_version 19881 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:37:09,299][25130] Fps is (10 sec: 8601.5, 60 sec: 4573.9, 300 sec: 5179.0). Total num frames: 81457152. Throughput: 0: 1208.4. Samples: 2858750. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:37:09,300][25130] Avg episode reward: [(0, '53.016')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:37:10,699][25742] Updated weights for policy 0, policy_version 19891 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:37:14,300][25130] Fps is (10 sec: 8601.2, 60 sec: 5120.0, 300 sec: 5290.1). Total num frames: 81502208. Throughput: 0: 1417.8. Samples: 2872086. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:37:14,301][25130] Avg episode reward: [(0, '52.476')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:37:15,352][25742] Updated weights for policy 0, policy_version 19901 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:37:19,299][25130] Fps is (10 sec: 9011.3, 60 sec: 5666.2, 300 sec: 5401.2). Total num frames: 81547264. Throughput: 0: 1623.7. Samples: 2885242. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:37:19,300][25130] Avg episode reward: [(0, '53.553')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:37:20,027][25742] Updated weights for policy 0, policy_version 19911 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:37:24,299][25130] Fps is (10 sec: 9011.5, 60 sec: 6144.1, 300 sec: 5512.3). Total num frames: 81592320. Throughput: 0: 1723.5. Samples: 2891750. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:37:24,300][25130] Avg episode reward: [(0, '54.934')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:37:24,628][25742] Updated weights for policy 0, policy_version 19921 (0.0013)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:37:29,295][25742] Updated weights for policy 0, policy_version 19931 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:37:29,299][25130] Fps is (10 sec: 9011.3, 60 sec: 6690.2, 300 sec: 5623.3). Total num frames: 81637376. Throughput: 0: 1929.2. Samples: 2905016. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:37:29,300][25130] Avg episode reward: [(0, '55.230')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:37:33,953][25742] Updated weights for policy 0, policy_version 19941 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:37:34,299][25130] Fps is (10 sec: 8601.6, 60 sec: 7168.1, 300 sec: 5706.6). Total num frames: 81678336. Throughput: 0: 2128.1. Samples: 2918248. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:37:34,300][25130] Avg episode reward: [(0, '53.395')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:37:38,692][25742] Updated weights for policy 0, policy_version 19951 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:37:39,299][25130] Fps is (10 sec: 8601.5, 60 sec: 7714.2, 300 sec: 5817.7). Total num frames: 81723392. Throughput: 0: 2190.1. Samples: 2924582. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:37:39,300][25130] Avg episode reward: [(0, '53.072')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:37:43,329][25742] Updated weights for policy 0, policy_version 19961 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:37:44,299][25130] Fps is (10 sec: 9011.3, 60 sec: 8192.1, 300 sec: 5928.8). Total num frames: 81768448. Throughput: 0: 2195.2. Samples: 2937812. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:37:44,300][25130] Avg episode reward: [(0, '52.836')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:37:47,974][25742] Updated weights for policy 0, policy_version 19971 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:37:49,299][25130] Fps is (10 sec: 8601.7, 60 sec: 8601.7, 300 sec: 6026.0). Total num frames: 81809408. Throughput: 0: 2197.2. Samples: 2951080. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:37:49,300][25130] Avg episode reward: [(0, '51.883')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:37:52,600][25742] Updated weights for policy 0, policy_version 19981 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:37:54,299][25130] Fps is (10 sec: 8601.5, 60 sec: 8806.4, 300 sec: 6137.1). Total num frames: 81854464. Throughput: 0: 2203.0. Samples: 2957886. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:37:54,300][25130] Avg episode reward: [(0, '53.059')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:37:57,248][25742] Updated weights for policy 0, policy_version 19991 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:37:59,299][25130] Fps is (10 sec: 9011.1, 60 sec: 8806.4, 300 sec: 6234.3). Total num frames: 81899520. Throughput: 0: 2201.4. Samples: 2971146. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:37:59,300][25130] Avg episode reward: [(0, '53.771')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:38:01,892][25742] Updated weights for policy 0, policy_version 20001 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:38:04,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8806.4, 300 sec: 6345.3). Total num frames: 81944576. Throughput: 0: 2202.8. Samples: 2984366. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:38:04,300][25130] Avg episode reward: [(0, '52.378')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:38:06,523][25742] Updated weights for policy 0, policy_version 20011 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:38:09,299][25130] Fps is (10 sec: 8601.7, 60 sec: 8806.4, 300 sec: 6442.5). Total num frames: 81985536. Throughput: 0: 2203.3. Samples: 2990896. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:38:09,300][25130] Avg episode reward: [(0, '52.920')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:38:11,242][25742] Updated weights for policy 0, policy_version 20021 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:38:14,299][25130] Fps is (10 sec: 8601.7, 60 sec: 8806.5, 300 sec: 6553.6). Total num frames: 82030592. Throughput: 0: 2198.2. Samples: 3003934. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:38:14,301][25130] Avg episode reward: [(0, '51.983')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:38:15,874][25742] Updated weights for policy 0, policy_version 20031 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:38:19,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8806.4, 300 sec: 6650.8). Total num frames: 82075648. Throughput: 0: 2198.0. Samples: 3017158. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:38:19,300][25130] Avg episode reward: [(0, '52.520')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:38:20,555][25742] Updated weights for policy 0, policy_version 20041 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:38:24,299][25130] Fps is (10 sec: 9011.2, 60 sec: 8806.4, 300 sec: 6761.9). Total num frames: 82120704. Throughput: 0: 2201.4. Samples: 3023646. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:38:24,300][25130] Avg episode reward: [(0, '53.124')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:38:24,304][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000020049_82120704.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:38:24,406][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000019670_80568320.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:38:25,236][25742] Updated weights for policy 0, policy_version 20051 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:38:29,299][25130] Fps is (10 sec: 8601.6, 60 sec: 8738.1, 300 sec: 6845.2). Total num frames: 82161664. Throughput: 0: 2200.2. Samples: 3036822. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:38:29,300][25130] Avg episode reward: [(0, '52.750')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:38:29,906][25742] Updated weights for policy 0, policy_version 20061 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:38:34,299][25130] Fps is (10 sec: 8601.5, 60 sec: 8806.4, 300 sec: 6886.8). Total num frames: 82206720. Throughput: 0: 2198.9. Samples: 3050030. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:38:34,300][25130] Avg episode reward: [(0, '53.120')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:38:34,542][25742] Updated weights for policy 0, policy_version 20071 (0.0013)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:38:39,211][25742] Updated weights for policy 0, policy_version 20081 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:38:39,299][25130] Fps is (10 sec: 9011.0, 60 sec: 8806.4, 300 sec: 6886.8). Total num frames: 82251776. Throughput: 0: 2191.5. Samples: 3056504. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:38:39,301][25130] Avg episode reward: [(0, '53.492')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:38:43,905][25742] Updated weights for policy 0, policy_version 20091 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:38:44,300][25130] Fps is (10 sec: 8601.3, 60 sec: 8738.1, 300 sec: 6886.8). Total num frames: 82292736. Throughput: 0: 2190.4. Samples: 3069714. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:38:44,301][25130] Avg episode reward: [(0, '53.343')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:38:48,657][25742] Updated weights for policy 0, policy_version 20101 (0.0016)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:38:49,299][25130] Fps is (10 sec: 8601.7, 60 sec: 8806.4, 300 sec: 6886.8). Total num frames: 82337792. Throughput: 0: 2187.5. Samples: 3082802. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:38:49,300][25130] Avg episode reward: [(0, '54.168')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:38:54,300][25130] Fps is (10 sec: 7372.4, 60 sec: 8533.2, 300 sec: 6845.2). Total num frames: 82366464. Throughput: 0: 2182.7. Samples: 3089118. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:38:54,304][25130] Avg episode reward: [(0, '53.579')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:38:57,081][25742] Updated weights for policy 0, policy_version 20111 (0.0048)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:38:59,300][25130] Fps is (10 sec: 4095.7, 60 sec: 7987.1, 300 sec: 6734.1). Total num frames: 82378752. Throughput: 0: 2003.2. Samples: 3094078. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:38:59,306][25130] Avg episode reward: [(0, '53.569')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:39:04,300][25130] Fps is (10 sec: 2457.6, 60 sec: 7441.0, 300 sec: 6623.0). Total num frames: 82391040. Throughput: 0: 1796.1. Samples: 3097986. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:39:04,305][25130] Avg episode reward: [(0, '53.952')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:39:09,300][25130] Fps is (10 sec: 2457.5, 60 sec: 6963.1, 300 sec: 6525.8). Total num frames: 82403328. Throughput: 0: 1695.6. Samples: 3099948. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:39:09,305][25130] Avg episode reward: [(0, '54.017')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:39:12,661][25742] Updated weights for policy 0, policy_version 20121 (0.0083)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:39:14,300][25130] Fps is (10 sec: 2867.2, 60 sec: 6485.2, 300 sec: 6428.6). Total num frames: 82419712. Throughput: 0: 1490.9. Samples: 3103914. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:39:14,304][25130] Avg episode reward: [(0, '54.026')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:39:19,300][25130] Fps is (10 sec: 2867.3, 60 sec: 5939.1, 300 sec: 6317.5). Total num frames: 82432000. Throughput: 0: 1285.7. Samples: 3107886. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:39:19,305][25130] Avg episode reward: [(0, '54.027')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:39:24,300][25130] Fps is (10 sec: 2457.6, 60 sec: 5393.0, 300 sec: 6220.4). Total num frames: 82444288. Throughput: 0: 1185.6. Samples: 3109856. Policy #0 lag: (min: 0.0, avg: 1.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:39:24,305][25130] Avg episode reward: [(0, '53.976')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:39:28,235][25742] Updated weights for policy 0, policy_version 20131 (0.0081)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:39:29,300][25130] Fps is (10 sec: 2457.6, 60 sec: 4915.1, 300 sec: 6123.2). Total num frames: 82456576. Throughput: 0: 980.9. Samples: 3113856. Policy #0 lag: (min: 0.0, avg: 1.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:39:29,305][25130] Avg episode reward: [(0, '53.788')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:39:34,300][25130] Fps is (10 sec: 2457.6, 60 sec: 4369.0, 300 sec: 6012.1). Total num frames: 82468864. Throughput: 0: 778.7. Samples: 3117844. Policy #0 lag: (min: 0.0, avg: 1.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:39:34,305][25130] Avg episode reward: [(0, '53.650')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:39:39,300][25130] Fps is (10 sec: 2867.2, 60 sec: 3891.1, 300 sec: 5928.8). Total num frames: 82485248. Throughput: 0: 682.2. Samples: 3119818. Policy #0 lag: (min: 0.0, avg: 1.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:39:39,304][25130] Avg episode reward: [(0, '53.377')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:39:43,816][25742] Updated weights for policy 0, policy_version 20141 (0.0083)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:39:44,301][25130] Fps is (10 sec: 2867.1, 60 sec: 3413.3, 300 sec: 5817.7). Total num frames: 82497536. Throughput: 0: 658.7. Samples: 3123720. Policy #0 lag: (min: 0.0, avg: 1.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:39:44,305][25130] Avg episode reward: [(0, '52.346')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:39:49,300][25130] Fps is (10 sec: 2457.6, 60 sec: 2867.2, 300 sec: 5720.5). Total num frames: 82509824. Throughput: 0: 661.9. Samples: 3127772. Policy #0 lag: (min: 0.0, avg: 1.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:39:49,304][25130] Avg episode reward: [(0, '52.370')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:39:54,300][25130] Fps is (10 sec: 2457.7, 60 sec: 2594.1, 300 sec: 5623.3). Total num frames: 82522112. Throughput: 0: 663.5. Samples: 3129804. Policy #0 lag: (min: 0.0, avg: 1.5, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:39:54,304][25130] Avg episode reward: [(0, '52.273')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:39:58,879][25742] Updated weights for policy 0, policy_version 20151 (0.0079)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:39:59,300][25130] Fps is (10 sec: 2867.2, 60 sec: 2662.4, 300 sec: 5526.1). Total num frames: 82538496. Throughput: 0: 666.4. Samples: 3133900. Policy #0 lag: (min: 0.0, avg: 1.5, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:39:59,304][25130] Avg episode reward: [(0, '52.567')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:40:04,300][25130] Fps is (10 sec: 2867.1, 60 sec: 2662.4, 300 sec: 5428.9). Total num frames: 82550784. Throughput: 0: 669.1. Samples: 3137996. Policy #0 lag: (min: 0.0, avg: 1.5, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:40:04,305][25130] Avg episode reward: [(0, '52.646')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:40:09,300][25130] Fps is (10 sec: 2457.6, 60 sec: 2662.4, 300 sec: 5317.8). Total num frames: 82563072. Throughput: 0: 670.8. Samples: 3140040. Policy #0 lag: (min: 0.0, avg: 1.5, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:40:09,304][25130] Avg episode reward: [(0, '53.065')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:40:13,934][25742] Updated weights for policy 0, policy_version 20161 (0.0080)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:40:14,300][25130] Fps is (10 sec: 2867.3, 60 sec: 2662.4, 300 sec: 5234.5). Total num frames: 82579456. Throughput: 0: 672.1. Samples: 3144102. Policy #0 lag: (min: 0.0, avg: 1.5, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:40:14,304][25130] Avg episode reward: [(0, '53.145')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:40:19,300][25130] Fps is (10 sec: 2867.2, 60 sec: 2662.4, 300 sec: 5179.0). Total num frames: 82591744. Throughput: 0: 674.1. Samples: 3148180. Policy #0 lag: (min: 0.0, avg: 1.5, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:40:19,305][25130] Avg episode reward: [(0, '52.817')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:40:24,300][25130] Fps is (10 sec: 3276.8, 60 sec: 2798.9, 300 sec: 5206.8). Total num frames: 82612224. Throughput: 0: 678.8. Samples: 3150362. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:40:24,304][25130] Avg episode reward: [(0, '53.423')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:40:24,328][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000020169_82612224.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:40:24,551][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000019831_81227776.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:40:25,372][25742] Updated weights for policy 0, policy_version 20171 (0.0070)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:40:29,300][25130] Fps is (10 sec: 6143.9, 60 sec: 3276.8, 300 sec: 5304.0). Total num frames: 82653184. Throughput: 0: 795.2. Samples: 3159506. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:40:29,305][25130] Avg episode reward: [(0, '53.424')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:40:30,122][25742] Updated weights for policy 0, policy_version 20181 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:40:34,299][25130] Fps is (10 sec: 8602.5, 60 sec: 3823.0, 300 sec: 5401.2). Total num frames: 82698240. Throughput: 0: 992.8. Samples: 3172446. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:40:34,300][25130] Avg episode reward: [(0, '54.413')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:40:34,766][25742] Updated weights for policy 0, policy_version 20191 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:40:39,299][25130] Fps is (10 sec: 8602.3, 60 sec: 4232.6, 300 sec: 5498.4). Total num frames: 82739200. Throughput: 0: 1098.4. Samples: 3179232. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:40:39,300][25130] Avg episode reward: [(0, '53.551')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:40:39,491][25742] Updated weights for policy 0, policy_version 20201 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:40:44,226][25742] Updated weights for policy 0, policy_version 20211 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:40:44,299][25130] Fps is (10 sec: 8601.6, 60 sec: 4778.8, 300 sec: 5609.5). Total num frames: 82784256. Throughput: 0: 1289.7. Samples: 3191934. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:40:44,302][25130] Avg episode reward: [(0, '52.330')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:40:49,028][25742] Updated weights for policy 0, policy_version 20221 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:40:49,299][25130] Fps is (10 sec: 8601.7, 60 sec: 5256.6, 300 sec: 5706.6). Total num frames: 82825216. Throughput: 0: 1487.3. Samples: 3204922. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:40:49,300][25130] Avg episode reward: [(0, '52.047')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:40:53,789][25742] Updated weights for policy 0, policy_version 20231 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:40:54,300][25130] Fps is (10 sec: 8600.9, 60 sec: 5802.7, 300 sec: 5817.7). Total num frames: 82870272. Throughput: 0: 1583.6. Samples: 3211300. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:40:54,302][25130] Avg episode reward: [(0, '52.497')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:40:58,569][25742] Updated weights for policy 0, policy_version 20241 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:40:59,299][25130] Fps is (10 sec: 8601.6, 60 sec: 6212.4, 300 sec: 5901.0). Total num frames: 82911232. Throughput: 0: 1783.2. Samples: 3224346. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:40:59,300][25130] Avg episode reward: [(0, '53.384')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:41:03,705][25742] Updated weights for policy 0, policy_version 20251 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:41:04,299][25130] Fps is (10 sec: 8192.6, 60 sec: 6690.3, 300 sec: 5998.2). Total num frames: 82952192. Throughput: 0: 1961.1. Samples: 3236426. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:41:04,300][25130] Avg episode reward: [(0, '53.353')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:41:08,593][25742] Updated weights for policy 0, policy_version 20261 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:41:09,299][25130] Fps is (10 sec: 8191.8, 60 sec: 7168.1, 300 sec: 6095.4). Total num frames: 82993152. Throughput: 0: 2049.7. Samples: 3242596. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:41:09,300][25130] Avg episode reward: [(0, '53.689')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:41:13,336][25742] Updated weights for policy 0, policy_version 20271 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:41:14,299][25130] Fps is (10 sec: 8601.5, 60 sec: 7646.0, 300 sec: 6206.5). Total num frames: 83038208. Throughput: 0: 2133.0. Samples: 3255490. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:41:14,301][25130] Avg episode reward: [(0, '53.294')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:41:18,097][25742] Updated weights for policy 0, policy_version 20281 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:41:19,299][25130] Fps is (10 sec: 8601.7, 60 sec: 8123.8, 300 sec: 6289.8). Total num frames: 83079168. Throughput: 0: 2131.2. Samples: 3268350. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:41:19,300][25130] Avg episode reward: [(0, '53.663')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:41:22,861][25742] Updated weights for policy 0, policy_version 20291 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:41:24,299][25130] Fps is (10 sec: 8192.0, 60 sec: 8465.2, 300 sec: 6387.0). Total num frames: 83120128. Throughput: 0: 2121.7. Samples: 3274708. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:41:24,300][25130] Avg episode reward: [(0, '53.266')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:41:27,705][25742] Updated weights for policy 0, policy_version 20301 (0.0017)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:41:29,299][25130] Fps is (10 sec: 8601.5, 60 sec: 8533.4, 300 sec: 6498.1). Total num frames: 83165184. Throughput: 0: 2125.6. Samples: 3287588. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:41:29,301][25130] Avg episode reward: [(0, '52.914')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:41:32,665][25742] Updated weights for policy 0, policy_version 20311 (0.0018)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:41:34,299][25130] Fps is (10 sec: 8601.6, 60 sec: 8465.0, 300 sec: 6595.3). Total num frames: 83206144. Throughput: 0: 2112.2. Samples: 3299970. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:41:34,300][25130] Avg episode reward: [(0, '51.907')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:41:37,480][25742] Updated weights for policy 0, policy_version 20321 (0.0016)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:41:39,299][25130] Fps is (10 sec: 8192.1, 60 sec: 8465.1, 300 sec: 6678.6). Total num frames: 83247104. Throughput: 0: 2111.3. Samples: 3306306. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:41:39,300][25130] Avg episode reward: [(0, '49.984')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:41:42,482][25742] Updated weights for policy 0, policy_version 20331 (0.0018)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:41:44,299][25130] Fps is (10 sec: 8192.1, 60 sec: 8396.8, 300 sec: 6761.9). Total num frames: 83288064. Throughput: 0: 2095.1. Samples: 3318626. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:41:44,300][25130] Avg episode reward: [(0, '49.524')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:41:47,650][25742] Updated weights for policy 0, policy_version 20341 (0.0023)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:41:49,299][25130] Fps is (10 sec: 8192.0, 60 sec: 8396.8, 300 sec: 6789.6). Total num frames: 83329024. Throughput: 0: 2092.2. Samples: 3330576. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:41:49,300][25130] Avg episode reward: [(0, '49.611')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:41:52,797][25742] Updated weights for policy 0, policy_version 20351 (0.0021)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:41:54,299][25130] Fps is (10 sec: 8192.0, 60 sec: 8328.6, 300 sec: 6775.8). Total num frames: 83369984. Throughput: 0: 2086.4. Samples: 3336482. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:41:54,300][25130] Avg episode reward: [(0, '49.985')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:41:57,864][25742] Updated weights for policy 0, policy_version 20361 (0.0021)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:41:59,299][25130] Fps is (10 sec: 7782.4, 60 sec: 8260.3, 300 sec: 6748.0). Total num frames: 83406848. Throughput: 0: 2067.3. Samples: 3348520. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:41:59,300][25130] Avg episode reward: [(0, '51.172')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:42:02,753][25742] Updated weights for policy 0, policy_version 20371 (0.0016)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:42:04,299][25130] Fps is (10 sec: 8191.9, 60 sec: 8328.5, 300 sec: 6761.9). Total num frames: 83451904. Throughput: 0: 2064.8. Samples: 3361266. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:42:04,301][25130] Avg episode reward: [(0, '51.180')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:42:07,842][25742] Updated weights for policy 0, policy_version 20381 (0.0021)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:42:09,299][25130] Fps is (10 sec: 8191.9, 60 sec: 8260.3, 300 sec: 6734.1). Total num frames: 83488768. Throughput: 0: 2053.3. Samples: 3367108. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:42:09,300][25130] Avg episode reward: [(0, '51.774')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:42:12,946][25742] Updated weights for policy 0, policy_version 20391 (0.0020)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:42:14,299][25130] Fps is (10 sec: 7782.4, 60 sec: 8192.0, 300 sec: 6720.2). Total num frames: 83529728. Throughput: 0: 2033.2. Samples: 3379084. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:42:14,300][25130] Avg episode reward: [(0, '52.179')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:42:17,983][25742] Updated weights for policy 0, policy_version 20401 (0.0019)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:42:19,299][25130] Fps is (10 sec: 8192.0, 60 sec: 8192.0, 300 sec: 6706.3). Total num frames: 83570688. Throughput: 0: 2030.2. Samples: 3391328. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:42:19,300][25130] Avg episode reward: [(0, '53.278')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:42:22,958][25742] Updated weights for policy 0, policy_version 20411 (0.0019)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:42:24,299][25130] Fps is (10 sec: 8192.0, 60 sec: 8192.0, 300 sec: 6692.4). Total num frames: 83611648. Throughput: 0: 2026.0. Samples: 3397478. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:42:24,300][25130] Avg episode reward: [(0, '52.217')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:42:24,476][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000020414_83615744.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:42:24,628][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000020049_82120704.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:42:28,092][25742] Updated weights for policy 0, policy_version 20421 (0.0021)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:42:29,301][25130] Fps is (10 sec: 8191.1, 60 sec: 8123.6, 300 sec: 6692.4). Total num frames: 83652608. Throughput: 0: 2021.1. Samples: 3409576. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:42:29,305][25130] Avg episode reward: [(0, '52.892')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:42:34,301][25130] Fps is (10 sec: 5324.2, 60 sec: 7645.7, 300 sec: 6581.3). Total num frames: 83664896. Throughput: 0: 1898.7. Samples: 3416020. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:42:34,305][25130] Avg episode reward: [(0, '53.494')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:42:39,300][25130] Fps is (10 sec: 2457.7, 60 sec: 7167.9, 300 sec: 6470.3). Total num frames: 83677184. Throughput: 0: 1811.5. Samples: 3418000. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:42:39,305][25130] Avg episode reward: [(0, '53.985')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:42:41,787][25742] Updated weights for policy 0, policy_version 20431 (0.0074)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:42:44,300][25130] Fps is (10 sec: 2457.7, 60 sec: 6690.1, 300 sec: 6373.1). Total num frames: 83689472. Throughput: 0: 1630.3. Samples: 3421886. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:42:44,306][25130] Avg episode reward: [(0, '53.894')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:42:49,301][25130] Fps is (10 sec: 2457.5, 60 sec: 6212.2, 300 sec: 6262.0). Total num frames: 83701760. Throughput: 0: 1433.3. Samples: 3425764. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:42:49,305][25130] Avg episode reward: [(0, '53.795')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:42:54,301][25130] Fps is (10 sec: 2457.5, 60 sec: 5734.3, 300 sec: 6150.9). Total num frames: 83714048. Throughput: 0: 1346.5. Samples: 3427702. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:42:54,306][25130] Avg episode reward: [(0, '53.474')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:42:57,496][25742] Updated weights for policy 0, policy_version 20441 (0.0087)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:42:59,300][25130] Fps is (10 sec: 2867.3, 60 sec: 5393.0, 300 sec: 6053.7). Total num frames: 83730432. Throughput: 0: 1167.8. Samples: 3431634. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:42:59,305][25130] Avg episode reward: [(0, '53.358')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:43:04,300][25130] Fps is (10 sec: 2867.3, 60 sec: 4846.9, 300 sec: 5956.5). Total num frames: 83742720. Throughput: 0: 982.7. Samples: 3435550. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:43:04,306][25130] Avg episode reward: [(0, '53.350')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:43:09,301][25130] Fps is (10 sec: 2457.5, 60 sec: 4437.3, 300 sec: 5845.5). Total num frames: 83755008. Throughput: 0: 888.5. Samples: 3437462. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:43:09,304][25130] Avg episode reward: [(0, '53.689')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:43:13,323][25742] Updated weights for policy 0, policy_version 20451 (0.0085)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:43:14,301][25130] Fps is (10 sec: 2457.6, 60 sec: 3959.4, 300 sec: 5734.4). Total num frames: 83767296. Throughput: 0: 706.8. Samples: 3441382. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:43:14,306][25130] Avg episode reward: [(0, '54.007')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:43:19,300][25130] Fps is (10 sec: 2457.7, 60 sec: 3481.6, 300 sec: 5623.3). Total num frames: 83779584. Throughput: 0: 651.3. Samples: 3445328. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:43:19,304][25130] Avg episode reward: [(0, '54.192')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:43:24,301][25130] Fps is (10 sec: 2867.2, 60 sec: 3071.9, 300 sec: 5540.0). Total num frames: 83795968. Throughput: 0: 650.1. Samples: 3447254. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:43:24,305][25130] Avg episode reward: [(0, '54.646')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:43:28,949][25742] Updated weights for policy 0, policy_version 20461 (0.0085)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:43:29,300][25130] Fps is (10 sec: 2867.2, 60 sec: 2594.1, 300 sec: 5428.9). Total num frames: 83808256. Throughput: 0: 650.5. Samples: 3451160. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:43:29,306][25130] Avg episode reward: [(0, '54.286')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:43:34,300][25130] Fps is (10 sec: 2457.7, 60 sec: 2594.1, 300 sec: 5317.8). Total num frames: 83820544. Throughput: 0: 652.1. Samples: 3455108. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:43:34,305][25130] Avg episode reward: [(0, '53.806')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:43:39,300][25130] Fps is (10 sec: 2457.6, 60 sec: 2594.1, 300 sec: 5220.7). Total num frames: 83832832. Throughput: 0: 652.3. Samples: 3457056. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:43:39,305][25130] Avg episode reward: [(0, '53.443')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:43:44,300][25130] Fps is (10 sec: 2457.6, 60 sec: 2594.1, 300 sec: 5109.6). Total num frames: 83845120. Throughput: 0: 652.0. Samples: 3460974. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:43:44,306][25130] Avg episode reward: [(0, '53.470')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:43:44,583][25742] Updated weights for policy 0, policy_version 20471 (0.0086)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:43:49,300][25130] Fps is (10 sec: 2867.1, 60 sec: 2662.4, 300 sec: 5067.9). Total num frames: 83861504. Throughput: 0: 652.9. Samples: 3464930. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:43:49,305][25130] Avg episode reward: [(0, '53.661')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:43:54,300][25130] Fps is (10 sec: 2867.2, 60 sec: 2662.4, 300 sec: 5067.9). Total num frames: 83873792. Throughput: 0: 653.9. Samples: 3466886. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:43:54,306][25130] Avg episode reward: [(0, '53.511')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:43:59,300][25130] Fps is (10 sec: 2457.6, 60 sec: 2594.1, 300 sec: 5067.9). Total num frames: 83886080. Throughput: 0: 654.8. Samples: 3470846. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:43:59,306][25130] Avg episode reward: [(0, '53.872')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:44:00,306][25742] Updated weights for policy 0, policy_version 20481 (0.0086)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:44:04,300][25130] Fps is (10 sec: 2457.6, 60 sec: 2594.1, 300 sec: 5067.9). Total num frames: 83898368. Throughput: 0: 653.0. Samples: 3474712. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:44:04,307][25130] Avg episode reward: [(0, '54.236')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:44:09,300][25130] Fps is (10 sec: 2457.6, 60 sec: 2594.1, 300 sec: 5054.0). Total num frames: 83910656. Throughput: 0: 654.1. Samples: 3476688. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:44:09,305][25130] Avg episode reward: [(0, '54.039')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:44:14,181][25742] Updated weights for policy 0, policy_version 20491 (0.0079)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:44:14,299][25130] Fps is (10 sec: 3277.0, 60 sec: 2730.7, 300 sec: 5081.8). Total num frames: 83931136. Throughput: 0: 661.7. Samples: 3480936. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:44:14,301][25130] Avg episode reward: [(0, '54.069')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:44:19,299][25130] Fps is (10 sec: 5734.9, 60 sec: 3140.3, 300 sec: 5165.1). Total num frames: 83968000. Throughput: 0: 790.2. Samples: 3490668. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:44:19,300][25130] Avg episode reward: [(0, '54.113')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:44:19,483][25742] Updated weights for policy 0, policy_version 20501 (0.0021)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:44:24,299][25130] Fps is (10 sec: 7782.6, 60 sec: 3549.9, 300 sec: 5262.3). Total num frames: 84008960. Throughput: 0: 879.9. Samples: 3496650. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:44:24,300][25130] Avg episode reward: [(0, '53.928')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:44:24,499][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000020511_84013056.pth...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:44:24,503][25742] Updated weights for policy 0, policy_version 20511 (0.0017)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:44:24,611][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000020169_82612224.pth\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:44:29,299][25130] Fps is (10 sec: 8192.1, 60 sec: 4027.8, 300 sec: 5359.5). Total num frames: 84049920. Throughput: 0: 1070.2. Samples: 3509134. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:44:29,300][25130] Avg episode reward: [(0, '52.297')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:44:29,373][25742] Updated weights for policy 0, policy_version 20521 (0.0015)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:44:34,169][25742] Updated weights for policy 0, policy_version 20531 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:44:34,299][25130] Fps is (10 sec: 8601.6, 60 sec: 4573.9, 300 sec: 5456.7). Total num frames: 84094976. Throughput: 0: 1269.2. Samples: 3522042. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:44:34,300][25130] Avg episode reward: [(0, '50.425')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:44:38,963][25742] Updated weights for policy 0, policy_version 20541 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:44:39,299][25130] Fps is (10 sec: 8601.5, 60 sec: 5051.8, 300 sec: 5553.9). Total num frames: 84135936. Throughput: 0: 1366.4. Samples: 3528374. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:44:39,301][25130] Avg episode reward: [(0, '50.565')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:44:43,770][25742] Updated weights for policy 0, policy_version 20551 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:44:44,299][25130] Fps is (10 sec: 8601.6, 60 sec: 5598.0, 300 sec: 5665.0). Total num frames: 84180992. Throughput: 0: 1561.4. Samples: 3541106. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:44:44,300][25130] Avg episode reward: [(0, '50.666')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:44:48,593][25742] Updated weights for policy 0, policy_version 20561 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:44:49,299][25130] Fps is (10 sec: 8601.7, 60 sec: 6007.6, 300 sec: 5762.2). Total num frames: 84221952. Throughput: 0: 1760.6. Samples: 3553936. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:44:49,300][25130] Avg episode reward: [(0, '52.423')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:44:53,441][25742] Updated weights for policy 0, policy_version 20571 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:44:54,299][25130] Fps is (10 sec: 8191.9, 60 sec: 6485.4, 300 sec: 5845.5). Total num frames: 84262912. Throughput: 0: 1855.9. Samples: 3560200. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:44:54,301][25130] Avg episode reward: [(0, '53.300')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:44:58,253][25742] Updated weights for policy 0, policy_version 20581 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:44:59,299][25130] Fps is (10 sec: 8601.6, 60 sec: 7031.6, 300 sec: 5956.6). Total num frames: 84307968. Throughput: 0: 2046.0. Samples: 3573006. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:44:59,300][25130] Avg episode reward: [(0, '53.634')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:45:03,069][25742] Updated weights for policy 0, policy_version 20591 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:45:04,299][25130] Fps is (10 sec: 8601.7, 60 sec: 7509.4, 300 sec: 6053.8). Total num frames: 84348928. Throughput: 0: 2113.0. Samples: 3585752. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:45:04,300][25130] Avg episode reward: [(0, '54.194')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:45:07,909][25742] Updated weights for policy 0, policy_version 20601 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:45:09,299][25130] Fps is (10 sec: 8191.9, 60 sec: 7987.3, 300 sec: 6137.1). Total num frames: 84389888. Throughput: 0: 2119.2. Samples: 3592014. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:45:09,300][25130] Avg episode reward: [(0, '54.018')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:45:12,737][25742] Updated weights for policy 0, policy_version 20611 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:45:14,300][25130] Fps is (10 sec: 8600.8, 60 sec: 8396.7, 300 sec: 6248.1). Total num frames: 84434944. Throughput: 0: 2127.8. Samples: 3604886. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:45:14,305][25130] Avg episode reward: [(0, '54.787')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:45:17,556][25742] Updated weights for policy 0, policy_version 20621 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:45:19,299][25130] Fps is (10 sec: 8601.7, 60 sec: 8465.1, 300 sec: 6317.6). Total num frames: 84475904. Throughput: 0: 2121.6. Samples: 3617514. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:45:19,300][25130] Avg episode reward: [(0, '54.442')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:45:22,392][25742] Updated weights for policy 0, policy_version 20631 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:45:24,299][25130] Fps is (10 sec: 8192.7, 60 sec: 8465.1, 300 sec: 6317.6). Total num frames: 84516864. Throughput: 0: 2121.0. Samples: 3623820. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:45:24,300][25130] Avg episode reward: [(0, '54.221')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:45:27,227][25742] Updated weights for policy 0, policy_version 20641 (0.0016)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:45:29,300][25130] Fps is (10 sec: 8600.9, 60 sec: 8533.2, 300 sec: 6317.5). Total num frames: 84561920. Throughput: 0: 2124.8. Samples: 3636724. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:45:29,305][25130] Avg episode reward: [(0, '53.966')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:45:32,065][25742] Updated weights for policy 0, policy_version 20651 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:45:34,299][25130] Fps is (10 sec: 8601.6, 60 sec: 8465.0, 300 sec: 6317.6). Total num frames: 84602880. Throughput: 0: 2118.9. Samples: 3649288. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:45:34,300][25130] Avg episode reward: [(0, '54.076')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:45:36,896][25742] Updated weights for policy 0, policy_version 20661 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:45:39,299][25130] Fps is (10 sec: 8192.6, 60 sec: 8465.1, 300 sec: 6303.7). Total num frames: 84643840. Throughput: 0: 2120.0. Samples: 3655600. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:45:39,300][25130] Avg episode reward: [(0, '54.219')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:45:41,741][25742] Updated weights for policy 0, policy_version 20671 (0.0015)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:45:44,299][25130] Fps is (10 sec: 8601.7, 60 sec: 8465.1, 300 sec: 6317.6). Total num frames: 84688896. Throughput: 0: 2121.7. Samples: 3668482. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:45:44,301][25130] Avg episode reward: [(0, '53.230')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:45:46,660][25742] Updated weights for policy 0, policy_version 20681 (0.0018)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:45:49,300][25130] Fps is (10 sec: 8601.3, 60 sec: 8465.0, 300 sec: 6303.7). Total num frames: 84729856. Throughput: 0: 2111.9. Samples: 3680788. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:45:49,301][25130] Avg episode reward: [(0, '53.799')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:45:52,164][25742] Updated weights for policy 0, policy_version 20691 (0.0028)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:45:54,300][25130] Fps is (10 sec: 6553.1, 60 sec: 8191.9, 300 sec: 6248.1). Total num frames: 84754432. Throughput: 0: 2095.4. Samples: 3686308. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:45:54,305][25130] Avg episode reward: [(0, '53.907')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:45:59,300][25130] Fps is (10 sec: 3686.2, 60 sec: 7645.8, 300 sec: 6150.9). Total num frames: 84766720. Throughput: 0: 1918.3. Samples: 3691208. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:45:59,305][25130] Avg episode reward: [(0, '53.181')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:46:04,300][25130] Fps is (10 sec: 2457.6, 60 sec: 7167.9, 300 sec: 6053.7). Total num frames: 84779008. Throughput: 0: 1724.2. Samples: 3695104. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:46:04,305][25130] Avg episode reward: [(0, '53.291')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:46:07,817][25742] Updated weights for policy 0, policy_version 20701 (0.0084)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:46:09,300][25130] Fps is (10 sec: 2457.6, 60 sec: 6690.0, 300 sec: 5942.7). Total num frames: 84791296. Throughput: 0: 1626.3. Samples: 3697004. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:46:09,305][25130] Avg episode reward: [(0, '53.427')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:46:14,301][25130] Fps is (10 sec: 2867.0, 60 sec: 6212.2, 300 sec: 5859.3). Total num frames: 84807680. Throughput: 0: 1425.2. Samples: 3700858. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:46:14,307][25130] Avg episode reward: [(0, '53.171')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:46:19,300][25130] Fps is (10 sec: 2867.2, 60 sec: 5734.3, 300 sec: 5762.2). Total num frames: 84819968. Throughput: 0: 1233.1. Samples: 3704778. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:46:19,305][25130] Avg episode reward: [(0, '52.572')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:46:23,637][25742] Updated weights for policy 0, policy_version 20711 (0.0085)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:46:24,301][25130] Fps is (10 sec: 2457.7, 60 sec: 5256.4, 300 sec: 5651.1). Total num frames: 84832256. Throughput: 0: 1135.5. Samples: 3706698. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:46:24,305][25130] Avg episode reward: [(0, '52.426')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:46:25,168][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000020712_84836352.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:46:25,771][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000020414_83615744.pth\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:46:29,300][25130] Fps is (10 sec: 2457.6, 60 sec: 4710.4, 300 sec: 5553.9). Total num frames: 84844544. Throughput: 0: 938.5. Samples: 3710714. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:46:29,306][25130] Avg episode reward: [(0, '52.834')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:46:34,301][25130] Fps is (10 sec: 2457.6, 60 sec: 4232.5, 300 sec: 5456.7). Total num frames: 84856832. Throughput: 0: 749.6. Samples: 3714522. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:46:34,305][25130] Avg episode reward: [(0, '52.716')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:46:39,300][25130] Fps is (10 sec: 2457.6, 60 sec: 3754.6, 300 sec: 5359.5). Total num frames: 84869120. Throughput: 0: 670.0. Samples: 3716460. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:46:39,304][25130] Avg episode reward: [(0, '52.646')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:46:39,359][25742] Updated weights for policy 0, policy_version 20721 (0.0089)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:46:44,301][25130] Fps is (10 sec: 2867.2, 60 sec: 3276.7, 300 sec: 5276.2). Total num frames: 84885504. Throughput: 0: 647.6. Samples: 3720348. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:46:44,306][25130] Avg episode reward: [(0, '53.192')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:46:49,300][25130] Fps is (10 sec: 2867.2, 60 sec: 2798.9, 300 sec: 5179.0). Total num frames: 84897792. Throughput: 0: 647.3. Samples: 3724234. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:46:49,305][25130] Avg episode reward: [(0, '53.118')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:46:54,300][25130] Fps is (10 sec: 2457.7, 60 sec: 2594.1, 300 sec: 5095.7). Total num frames: 84910080. Throughput: 0: 647.9. Samples: 3726160. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:46:54,306][25130] Avg episode reward: [(0, '53.108')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:46:55,123][25742] Updated weights for policy 0, policy_version 20731 (0.0086)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:46:59,300][25130] Fps is (10 sec: 2457.6, 60 sec: 2594.1, 300 sec: 4984.6). Total num frames: 84922368. Throughput: 0: 649.4. Samples: 3730080. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:46:59,305][25130] Avg episode reward: [(0, '52.988')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:47:04,300][25130] Fps is (10 sec: 2457.6, 60 sec: 2594.1, 300 sec: 4901.3). Total num frames: 84934656. Throughput: 0: 647.8. Samples: 3733928. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:47:04,305][25130] Avg episode reward: [(0, '52.787')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:47:09,300][25130] Fps is (10 sec: 2457.7, 60 sec: 2594.1, 300 sec: 4804.1). Total num frames: 84946944. Throughput: 0: 648.2. Samples: 3735866. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:47:09,306][25130] Avg episode reward: [(0, '52.723')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:47:11,082][25742] Updated weights for policy 0, policy_version 20741 (0.0087)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:47:14,300][25130] Fps is (10 sec: 2867.1, 60 sec: 2594.2, 300 sec: 4720.8). Total num frames: 84963328. Throughput: 0: 644.8. Samples: 3739728. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:47:14,307][25130] Avg episode reward: [(0, '52.911')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:47:19,300][25130] Fps is (10 sec: 2867.1, 60 sec: 2594.1, 300 sec: 4623.6). Total num frames: 84975616. Throughput: 0: 648.0. Samples: 3743682. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:47:19,305][25130] Avg episode reward: [(0, '52.590')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:47:24,301][25130] Fps is (10 sec: 2457.6, 60 sec: 2594.1, 300 sec: 4526.4). Total num frames: 84987904. Throughput: 0: 648.4. Samples: 3745636. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:47:24,307][25130] Avg episode reward: [(0, '52.241')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:47:26,952][25742] Updated weights for policy 0, policy_version 20751 (0.0086)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:47:29,301][25130] Fps is (10 sec: 2457.6, 60 sec: 2594.1, 300 sec: 4526.4). Total num frames: 85000192. Throughput: 0: 648.6. Samples: 3749534. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:47:29,305][25130] Avg episode reward: [(0, '52.415')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:47:34,300][25130] Fps is (10 sec: 2457.6, 60 sec: 2594.1, 300 sec: 4526.4). Total num frames: 85012480. Throughput: 0: 649.1. Samples: 3753442. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:47:34,306][25130] Avg episode reward: [(0, '52.426')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:47:39,301][25130] Fps is (10 sec: 2457.6, 60 sec: 2594.1, 300 sec: 4526.4). Total num frames: 85024768. Throughput: 0: 649.4. Samples: 3755382. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:47:39,305][25130] Avg episode reward: [(0, '53.172')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:47:42,793][25742] Updated weights for policy 0, policy_version 20761 (0.0085)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:47:44,301][25130] Fps is (10 sec: 2457.5, 60 sec: 2525.9, 300 sec: 4526.4). Total num frames: 85037056. Throughput: 0: 649.8. Samples: 3759322. Policy #0 lag: (min: 0.0, avg: 1.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:47:44,307][25130] Avg episode reward: [(0, '53.300')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:47:49,301][25130] Fps is (10 sec: 2867.1, 60 sec: 2594.1, 300 sec: 4540.3). Total num frames: 85053440. Throughput: 0: 650.9. Samples: 3763218. Policy #0 lag: (min: 0.0, avg: 1.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:47:49,306][25130] Avg episode reward: [(0, '53.659')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:47:53,627][25742] Updated weights for policy 0, policy_version 20771 (0.0067)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:47:54,300][25130] Fps is (10 sec: 4505.8, 60 sec: 2867.2, 300 sec: 4582.0). Total num frames: 85082112. Throughput: 0: 662.6. Samples: 3765682. Policy #0 lag: (min: 0.0, avg: 1.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:47:54,306][25130] Avg episode reward: [(0, '53.711')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:47:58,748][25742] Updated weights for policy 0, policy_version 20781 (0.0022)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:47:59,299][25130] Fps is (10 sec: 6964.1, 60 sec: 3345.1, 300 sec: 4679.2). Total num frames: 85123072. Throughput: 0: 831.5. Samples: 3777144. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:47:59,301][25130] Avg episode reward: [(0, '54.751')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:48:03,541][25742] Updated weights for policy 0, policy_version 20791 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:48:04,299][25130] Fps is (10 sec: 8192.7, 60 sec: 3823.0, 300 sec: 4776.4). Total num frames: 85164032. Throughput: 0: 1024.1. Samples: 3789766. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:48:04,300][25130] Avg episode reward: [(0, '55.713')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:48:04,484][25727] Saving new best policy, reward=55.713!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:48:08,233][25742] Updated weights for policy 0, policy_version 20801 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:48:09,300][25130] Fps is (10 sec: 8601.3, 60 sec: 4369.1, 300 sec: 4887.4). Total num frames: 85209088. Throughput: 0: 1123.3. Samples: 3796182. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:48:09,300][25130] Avg episode reward: [(0, '55.341')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:48:12,904][25742] Updated weights for policy 0, policy_version 20811 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:48:14,300][25130] Fps is (10 sec: 8601.2, 60 sec: 4778.7, 300 sec: 4984.6). Total num frames: 85250048. Throughput: 0: 1329.1. Samples: 3809344. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:48:14,301][25130] Avg episode reward: [(0, '55.407')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:48:17,639][25742] Updated weights for policy 0, policy_version 20821 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:48:19,299][25130] Fps is (10 sec: 8602.0, 60 sec: 5324.9, 300 sec: 5081.8). Total num frames: 85295104. Throughput: 0: 1532.9. Samples: 3822422. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:48:19,300][25130] Avg episode reward: [(0, '55.314')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:48:22,351][25742] Updated weights for policy 0, policy_version 20831 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:48:24,299][25130] Fps is (10 sec: 9011.8, 60 sec: 5871.0, 300 sec: 5192.9). Total num frames: 85340160. Throughput: 0: 1632.6. Samples: 3828848. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:48:24,300][25130] Avg episode reward: [(0, '56.147')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:48:24,304][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000020835_85340160.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:48:24,402][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000020511_84013056.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:48:24,413][25727] Saving new best policy, reward=56.147!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:48:27,118][25742] Updated weights for policy 0, policy_version 20841 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:48:29,299][25130] Fps is (10 sec: 8601.6, 60 sec: 6348.9, 300 sec: 5290.1). Total num frames: 85381120. Throughput: 0: 1831.9. Samples: 3841754. Policy #0 lag: (min: 1.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:48:29,300][25130] Avg episode reward: [(0, '55.552')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:48:31,860][25742] Updated weights for policy 0, policy_version 20851 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:48:34,300][25130] Fps is (10 sec: 8600.8, 60 sec: 6894.9, 300 sec: 5401.2). Total num frames: 85426176. Throughput: 0: 2031.7. Samples: 3854644. Policy #0 lag: (min: 1.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:48:34,305][25130] Avg episode reward: [(0, '55.872')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:48:36,635][25742] Updated weights for policy 0, policy_version 20861 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:48:39,299][25130] Fps is (10 sec: 8601.6, 60 sec: 7372.9, 300 sec: 5498.4). Total num frames: 85467136. Throughput: 0: 2122.5. Samples: 3861192. Policy #0 lag: (min: 1.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:48:39,300][25130] Avg episode reward: [(0, '55.495')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:48:41,390][25742] Updated weights for policy 0, policy_version 20871 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:48:44,299][25130] Fps is (10 sec: 8602.4, 60 sec: 7919.1, 300 sec: 5595.6). Total num frames: 85512192. Throughput: 0: 2151.2. Samples: 3873950. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:48:44,300][25130] Avg episode reward: [(0, '54.677')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:48:46,124][25742] Updated weights for policy 0, policy_version 20881 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:48:49,299][25130] Fps is (10 sec: 8601.7, 60 sec: 8328.7, 300 sec: 5692.8). Total num frames: 85553152. Throughput: 0: 2159.9. Samples: 3886960. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:48:49,300][25130] Avg episode reward: [(0, '53.898')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:48:50,856][25742] Updated weights for policy 0, policy_version 20891 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:48:54,299][25130] Fps is (10 sec: 8601.6, 60 sec: 8601.7, 300 sec: 5803.8). Total num frames: 85598208. Throughput: 0: 2159.6. Samples: 3893362. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:48:54,300][25130] Avg episode reward: [(0, '53.199')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:48:55,567][25742] Updated weights for policy 0, policy_version 20901 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:48:59,299][25130] Fps is (10 sec: 8601.5, 60 sec: 8601.6, 300 sec: 5901.0). Total num frames: 85639168. Throughput: 0: 2158.3. Samples: 3906466. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:48:59,300][25130] Avg episode reward: [(0, '53.251')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:49:00,327][25742] Updated weights for policy 0, policy_version 20911 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:49:04,300][25130] Fps is (10 sec: 8601.1, 60 sec: 8669.8, 300 sec: 6012.1). Total num frames: 85684224. Throughput: 0: 2157.3. Samples: 3919500. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:49:04,304][25130] Avg episode reward: [(0, '53.029')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:49:05,050][25742] Updated weights for policy 0, policy_version 20921 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:49:09,299][25130] Fps is (10 sec: 8601.6, 60 sec: 8601.7, 300 sec: 6081.5). Total num frames: 85725184. Throughput: 0: 2155.5. Samples: 3925844. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:49:09,300][25130] Avg episode reward: [(0, '52.577')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:49:09,786][25742] Updated weights for policy 0, policy_version 20931 (0.0014)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:49:14,299][25130] Fps is (10 sec: 8602.1, 60 sec: 8670.0, 300 sec: 6109.3). Total num frames: 85770240. Throughput: 0: 2159.6. Samples: 3938938. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:49:14,300][25130] Avg episode reward: [(0, '52.131')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:49:14,509][25742] Updated weights for policy 0, policy_version 20941 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:49:19,299][25130] Fps is (10 sec: 8601.5, 60 sec: 8601.6, 300 sec: 6109.3). Total num frames: 85811200. Throughput: 0: 2154.1. Samples: 3951578. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:49:19,301][25130] Avg episode reward: [(0, '52.525')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:49:19,359][25742] Updated weights for policy 0, policy_version 20951 (0.0017)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:49:24,300][25130] Fps is (10 sec: 6962.6, 60 sec: 8328.4, 300 sec: 6067.6). Total num frames: 85839872. Throughput: 0: 2146.5. Samples: 3957784. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:49:24,304][25130] Avg episode reward: [(0, '52.613')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:49:29,300][25130] Fps is (10 sec: 4095.7, 60 sec: 7850.6, 300 sec: 5956.5). Total num frames: 85852160. Throughput: 0: 1973.1. Samples: 3962742. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:49:29,304][25130] Avg episode reward: [(0, '52.506')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:49:29,735][25742] Updated weights for policy 0, policy_version 20961 (0.0064)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:49:34,301][25130] Fps is (10 sec: 2867.1, 60 sec: 7372.8, 300 sec: 5873.2). Total num frames: 85868544. Throughput: 0: 1774.9. Samples: 3966834. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:49:34,305][25130] Avg episode reward: [(0, '53.124')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:49:39,300][25130] Fps is (10 sec: 2867.2, 60 sec: 6894.9, 300 sec: 5762.2). Total num frames: 85880832. Throughput: 0: 1676.6. Samples: 3968810. Policy #0 lag: (min: 0.0, avg: 0.7, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:49:39,304][25130] Avg episode reward: [(0, '53.149')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:49:44,300][25130] Fps is (10 sec: 2457.7, 60 sec: 6348.7, 300 sec: 5665.0). Total num frames: 85893120. Throughput: 0: 1476.4. Samples: 3972906. Policy #0 lag: (min: 0.0, avg: 1.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:49:44,304][25130] Avg episode reward: [(0, '53.246')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:49:44,935][25742] Updated weights for policy 0, policy_version 20971 (0.0081)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:49:49,300][25130] Fps is (10 sec: 2457.6, 60 sec: 5870.8, 300 sec: 5567.8). Total num frames: 85905408. Throughput: 0: 1277.5. Samples: 3976990. Policy #0 lag: (min: 0.0, avg: 1.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:49:49,305][25130] Avg episode reward: [(0, '53.583')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:49:54,300][25130] Fps is (10 sec: 2867.2, 60 sec: 5393.0, 300 sec: 5470.6). Total num frames: 85921792. Throughput: 0: 1181.4. Samples: 3979008. Policy #0 lag: (min: 0.0, avg: 1.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:49:54,305][25130] Avg episode reward: [(0, '54.099')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:49:59,300][25130] Fps is (10 sec: 2867.2, 60 sec: 4915.1, 300 sec: 5373.4). Total num frames: 85934080. Throughput: 0: 981.0. Samples: 3983084. Policy #0 lag: (min: 1.0, avg: 1.5, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:49:59,305][25130] Avg episode reward: [(0, '54.711')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:50:00,036][25742] Updated weights for policy 0, policy_version 20981 (0.0083)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:50:04,300][25130] Fps is (10 sec: 2457.5, 60 sec: 4369.0, 300 sec: 5276.2). Total num frames: 85946368. Throughput: 0: 790.7. Samples: 3987158. Policy #0 lag: (min: 1.0, avg: 1.5, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:50:04,304][25130] Avg episode reward: [(0, '54.646')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:50:09,300][25130] Fps is (10 sec: 2867.2, 60 sec: 3959.4, 300 sec: 5179.0). Total num frames: 85962752. Throughput: 0: 696.8. Samples: 3989140. Policy #0 lag: (min: 1.0, avg: 1.5, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:50:09,305][25130] Avg episode reward: [(0, '54.932')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:50:14,300][25130] Fps is (10 sec: 2867.2, 60 sec: 3413.3, 300 sec: 5081.8). Total num frames: 85975040. Throughput: 0: 675.3. Samples: 3993130. Policy #0 lag: (min: 0.0, avg: 1.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:50:14,305][25130] Avg episode reward: [(0, '54.333')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:50:15,339][25742] Updated weights for policy 0, policy_version 20991 (0.0085)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:50:19,300][25130] Fps is (10 sec: 2457.6, 60 sec: 2935.4, 300 sec: 4984.6). Total num frames: 85987328. Throughput: 0: 671.4. Samples: 3997048. Policy #0 lag: (min: 0.0, avg: 1.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:50:19,305][25130] Avg episode reward: [(0, '54.421')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:50:24,300][25130] Fps is (10 sec: 2457.6, 60 sec: 2662.4, 300 sec: 4873.5). Total num frames: 85999616. Throughput: 0: 672.2. Samples: 3999060. Policy #0 lag: (min: 0.0, avg: 1.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:50:24,306][25130] Avg episode reward: [(0, '53.560')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:50:24,605][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000020997_86003712.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:50:25,206][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000020712_84836352.pth\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:50:29,300][25130] Fps is (10 sec: 2457.6, 60 sec: 2662.4, 300 sec: 4776.3). Total num frames: 86011904. Throughput: 0: 670.3. Samples: 4003068. Policy #0 lag: (min: 0.0, avg: 1.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:50:29,305][25130] Avg episode reward: [(0, '53.530')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:50:31,119][25742] Updated weights for policy 0, policy_version 21001 (0.0078)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:50:34,300][25130] Fps is (10 sec: 2867.2, 60 sec: 2662.4, 300 sec: 4693.0). Total num frames: 86028288. Throughput: 0: 666.3. Samples: 4006972. Policy #0 lag: (min: 1.0, avg: 1.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:50:34,306][25130] Avg episode reward: [(0, '53.610')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:50:39,300][25130] Fps is (10 sec: 2867.2, 60 sec: 2662.4, 300 sec: 4582.0). Total num frames: 86040576. Throughput: 0: 664.9. Samples: 4008928. Policy #0 lag: (min: 1.0, avg: 1.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:50:39,304][25130] Avg episode reward: [(0, '53.902')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:50:44,300][25130] Fps is (10 sec: 2457.6, 60 sec: 2662.4, 300 sec: 4484.8). Total num frames: 86052864. Throughput: 0: 662.3. Samples: 4012886. Policy #0 lag: (min: 1.0, avg: 1.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:50:44,305][25130] Avg episode reward: [(0, '53.922')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:50:46,385][25742] Updated weights for policy 0, policy_version 21011 (0.0080)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:50:49,301][25130] Fps is (10 sec: 2457.6, 60 sec: 2662.4, 300 sec: 4443.1). Total num frames: 86065152. Throughput: 0: 658.3. Samples: 4016780. Policy #0 lag: (min: 0.0, avg: 1.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:50:49,306][25130] Avg episode reward: [(0, '53.965')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:50:54,300][25130] Fps is (10 sec: 2867.2, 60 sec: 2662.4, 300 sec: 4457.0). Total num frames: 86081536. Throughput: 0: 659.3. Samples: 4018808. Policy #0 lag: (min: 0.0, avg: 1.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:50:54,305][25130] Avg episode reward: [(0, '53.631')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:50:59,300][25130] Fps is (10 sec: 2867.2, 60 sec: 2662.4, 300 sec: 4457.0). Total num frames: 86093824. Throughput: 0: 656.9. Samples: 4022692. Policy #0 lag: (min: 0.0, avg: 1.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:50:59,304][25130] Avg episode reward: [(0, '54.012')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:51:02,085][25742] Updated weights for policy 0, policy_version 21021 (0.0084)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:51:04,300][25130] Fps is (10 sec: 2457.6, 60 sec: 2662.4, 300 sec: 4457.0). Total num frames: 86106112. Throughput: 0: 658.4. Samples: 4026674. Policy #0 lag: (min: 0.0, avg: 1.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:51:04,305][25130] Avg episode reward: [(0, '54.052')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:51:09,300][25130] Fps is (10 sec: 2457.6, 60 sec: 2594.1, 300 sec: 4443.1). Total num frames: 86118400. Throughput: 0: 657.6. Samples: 4028654. Policy #0 lag: (min: 0.0, avg: 1.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:51:09,305][25130] Avg episode reward: [(0, '53.220')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:51:14,300][25130] Fps is (10 sec: 2457.6, 60 sec: 2594.1, 300 sec: 4443.1). Total num frames: 86130688. Throughput: 0: 657.6. Samples: 4032660. Policy #0 lag: (min: 0.0, avg: 1.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:51:14,305][25130] Avg episode reward: [(0, '53.777')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:51:17,619][25742] Updated weights for policy 0, policy_version 21031 (0.0083)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:51:19,300][25130] Fps is (10 sec: 3276.9, 60 sec: 2730.7, 300 sec: 4470.9). Total num frames: 86151168. Throughput: 0: 659.9. Samples: 4036666. Policy #0 lag: (min: 0.0, avg: 1.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:51:19,305][25130] Avg episode reward: [(0, '53.090')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:51:24,299][25130] Fps is (10 sec: 4505.9, 60 sec: 2935.5, 300 sec: 4512.6). Total num frames: 86175744. Throughput: 0: 676.1. Samples: 4039354. Policy #0 lag: (min: 0.0, avg: 1.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:51:24,301][25130] Avg episode reward: [(0, '53.857')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:51:24,831][25742] Updated weights for policy 0, policy_version 21041 (0.0043)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:51:29,299][25130] Fps is (10 sec: 6963.8, 60 sec: 3481.6, 300 sec: 4623.6). Total num frames: 86220800. Throughput: 0: 860.3. Samples: 4051598. Policy #0 lag: (min: 0.0, avg: 1.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:51:29,300][25130] Avg episode reward: [(0, '53.701')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:51:29,628][25742] Updated weights for policy 0, policy_version 21051 (0.0016)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:51:34,299][25130] Fps is (10 sec: 8601.7, 60 sec: 3891.3, 300 sec: 4720.8). Total num frames: 86261760. Throughput: 0: 1059.5. Samples: 4064456. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:51:34,300][25130] Avg episode reward: [(0, '52.793')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:51:34,322][25742] Updated weights for policy 0, policy_version 21061 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:51:39,299][25130] Fps is (10 sec: 8192.0, 60 sec: 4369.1, 300 sec: 4804.1). Total num frames: 86302720. Throughput: 0: 1150.6. Samples: 4070584. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:51:39,300][25130] Avg episode reward: [(0, '53.568')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:51:39,401][25742] Updated weights for policy 0, policy_version 21071 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:51:44,299][25130] Fps is (10 sec: 8192.1, 60 sec: 4847.0, 300 sec: 4901.3). Total num frames: 86343680. Throughput: 0: 1336.1. Samples: 4082816. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:51:44,300][25130] Avg episode reward: [(0, '53.462')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:51:44,365][25742] Updated weights for policy 0, policy_version 21081 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:51:49,299][25130] Fps is (10 sec: 8192.0, 60 sec: 5324.9, 300 sec: 4998.5). Total num frames: 86384640. Throughput: 0: 1514.1. Samples: 4094808. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:51:49,300][25130] Avg episode reward: [(0, '53.840')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:51:49,605][25742] Updated weights for policy 0, policy_version 21091 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:51:54,299][25130] Fps is (10 sec: 8191.9, 60 sec: 5734.5, 300 sec: 5095.7). Total num frames: 86425600. Throughput: 0: 1606.1. Samples: 4100926. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:51:54,300][25130] Avg episode reward: [(0, '54.610')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:51:54,533][25742] Updated weights for policy 0, policy_version 21101 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:51:59,299][25130] Fps is (10 sec: 7782.4, 60 sec: 6144.1, 300 sec: 5179.0). Total num frames: 86462464. Throughput: 0: 1785.0. Samples: 4112982. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:51:59,300][25130] Avg episode reward: [(0, '55.145')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:51:59,863][25742] Updated weights for policy 0, policy_version 21111 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:52:04,299][25130] Fps is (10 sec: 7782.4, 60 sec: 6622.0, 300 sec: 5276.2). Total num frames: 86503424. Throughput: 0: 1953.8. Samples: 4124586. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:52:04,300][25130] Avg episode reward: [(0, '53.477')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:52:05,050][25742] Updated weights for policy 0, policy_version 21121 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:52:09,300][25130] Fps is (10 sec: 8191.4, 60 sec: 7099.8, 300 sec: 5359.5). Total num frames: 86544384. Throughput: 0: 2026.8. Samples: 4130560. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:52:09,305][25130] Avg episode reward: [(0, '53.998')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:52:10,266][25742] Updated weights for policy 0, policy_version 21131 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:52:14,299][25130] Fps is (10 sec: 8192.1, 60 sec: 7577.7, 300 sec: 5456.7). Total num frames: 86585344. Throughput: 0: 2024.4. Samples: 4142698. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:52:14,300][25130] Avg episode reward: [(0, '53.384')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:52:14,942][25742] Updated weights for policy 0, policy_version 21141 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:52:18,444][25742] Updated weights for policy 0, policy_version 21151 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:52:19,299][25130] Fps is (10 sec: 9831.2, 60 sec: 8192.1, 300 sec: 5609.5). Total num frames: 86642688. Throughput: 0: 2084.9. Samples: 4158278. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:52:19,300][25130] Avg episode reward: [(0, '52.784')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:52:21,905][25742] Updated weights for policy 0, policy_version 21161 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:52:24,299][25130] Fps is (10 sec: 11468.7, 60 sec: 8738.2, 300 sec: 5762.2). Total num frames: 86700032. Throughput: 0: 2148.0. Samples: 4167246. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:52:24,300][25130] Avg episode reward: [(0, '52.696')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:52:24,324][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000021168_86704128.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:52:24,401][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000020835_85340160.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:52:25,368][25742] Updated weights for policy 0, policy_version 21171 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:52:28,825][25742] Updated weights for policy 0, policy_version 21181 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:52:29,299][25130] Fps is (10 sec: 11878.3, 60 sec: 9011.2, 300 sec: 5928.8). Total num frames: 86761472. Throughput: 0: 2267.5. Samples: 4184856. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:52:29,300][25130] Avg episode reward: [(0, '53.935')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:52:32,298][25742] Updated weights for policy 0, policy_version 21191 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:52:34,299][25130] Fps is (10 sec: 11878.4, 60 sec: 9284.3, 300 sec: 6081.5). Total num frames: 86818816. Throughput: 0: 2392.5. Samples: 4202472. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:52:34,300][25130] Avg episode reward: [(0, '53.888')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:52:35,756][25742] Updated weights for policy 0, policy_version 21201 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:52:39,259][25742] Updated weights for policy 0, policy_version 21211 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:52:39,299][25130] Fps is (10 sec: 11878.5, 60 sec: 9625.6, 300 sec: 6248.2). Total num frames: 86880256. Throughput: 0: 2456.0. Samples: 4211446. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:52:39,300][25130] Avg episode reward: [(0, '53.880')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:52:42,743][25742] Updated weights for policy 0, policy_version 21221 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:52:44,299][25130] Fps is (10 sec: 11878.4, 60 sec: 9898.6, 300 sec: 6387.0). Total num frames: 86937600. Throughput: 0: 2577.8. Samples: 4228982. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:52:44,300][25130] Avg episode reward: [(0, '51.955')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:52:46,280][25742] Updated weights for policy 0, policy_version 21231 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:52:49,299][25130] Fps is (10 sec: 11468.7, 60 sec: 10171.7, 300 sec: 6484.2). Total num frames: 86994944. Throughput: 0: 2708.3. Samples: 4246460. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:52:49,300][25130] Avg episode reward: [(0, '51.741')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:52:49,750][25742] Updated weights for policy 0, policy_version 21241 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:52:53,244][25742] Updated weights for policy 0, policy_version 21251 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:52:54,299][25130] Fps is (10 sec: 11878.4, 60 sec: 10513.1, 300 sec: 6553.6). Total num frames: 87056384. Throughput: 0: 2775.9. Samples: 4255472. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:52:54,300][25130] Avg episode reward: [(0, '50.549')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:52:56,725][25742] Updated weights for policy 0, policy_version 21261 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:52:59,299][25130] Fps is (10 sec: 11878.6, 60 sec: 10854.4, 300 sec: 6609.1). Total num frames: 87113728. Throughput: 0: 2894.5. Samples: 4272952. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:52:59,300][25130] Avg episode reward: [(0, '50.697')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:53:00,271][25742] Updated weights for policy 0, policy_version 21271 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:53:03,817][25742] Updated weights for policy 0, policy_version 21281 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:53:04,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11127.5, 300 sec: 6650.8). Total num frames: 87171072. Throughput: 0: 2934.3. Samples: 4290322. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:53:04,300][25130] Avg episode reward: [(0, '50.216')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:53:07,311][25742] Updated weights for policy 0, policy_version 21291 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:53:09,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11400.7, 300 sec: 6706.3). Total num frames: 87228416. Throughput: 0: 2933.9. Samples: 4299270. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:53:09,300][25130] Avg episode reward: [(0, '50.234')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:53:10,787][25742] Updated weights for policy 0, policy_version 21301 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:53:14,270][25742] Updated weights for policy 0, policy_version 21311 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:53:14,299][25130] Fps is (10 sec: 11878.5, 60 sec: 11741.9, 300 sec: 6761.9). Total num frames: 87289856. Throughput: 0: 2932.8. Samples: 4316832. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:53:14,300][25130] Avg episode reward: [(0, '50.270')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:53:17,754][25742] Updated weights for policy 0, policy_version 21321 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:53:19,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11741.9, 300 sec: 6803.5). Total num frames: 87347200. Throughput: 0: 2931.0. Samples: 4334368. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:53:19,300][25130] Avg episode reward: [(0, '49.930')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:53:21,226][25742] Updated weights for policy 0, policy_version 21331 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:53:24,299][25130] Fps is (10 sec: 11468.6, 60 sec: 11741.8, 300 sec: 6859.1). Total num frames: 87404544. Throughput: 0: 2930.9. Samples: 4343338. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:53:24,300][25130] Avg episode reward: [(0, '51.959')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:53:24,719][25742] Updated weights for policy 0, policy_version 21341 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:53:28,227][25742] Updated weights for policy 0, policy_version 21351 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:53:29,299][25130] Fps is (10 sec: 11878.5, 60 sec: 11741.9, 300 sec: 6914.6). Total num frames: 87465984. Throughput: 0: 2930.6. Samples: 4360858. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:53:29,300][25130] Avg episode reward: [(0, '52.962')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:53:31,727][25742] Updated weights for policy 0, policy_version 21361 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:53:34,299][25130] Fps is (10 sec: 11878.6, 60 sec: 11741.9, 300 sec: 6970.1). Total num frames: 87523328. Throughput: 0: 2928.8. Samples: 4378256. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:53:34,300][25130] Avg episode reward: [(0, '54.674')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:53:35,267][25742] Updated weights for policy 0, policy_version 21371 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:53:38,793][25742] Updated weights for policy 0, policy_version 21381 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:53:39,299][25130] Fps is (10 sec: 11468.6, 60 sec: 11673.6, 300 sec: 7011.8). Total num frames: 87580672. Throughput: 0: 2921.2. Samples: 4386926. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:53:39,300][25130] Avg episode reward: [(0, '53.492')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:53:42,251][25742] Updated weights for policy 0, policy_version 21391 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:53:44,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11673.6, 300 sec: 7067.3). Total num frames: 87638016. Throughput: 0: 2928.5. Samples: 4404734. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:53:44,300][25130] Avg episode reward: [(0, '53.656')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:53:45,719][25742] Updated weights for policy 0, policy_version 21401 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:53:49,178][25742] Updated weights for policy 0, policy_version 21411 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:53:49,299][25130] Fps is (10 sec: 11878.6, 60 sec: 11741.9, 300 sec: 7122.9). Total num frames: 87699456. Throughput: 0: 2933.4. Samples: 4422324. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:53:49,300][25130] Avg episode reward: [(0, '52.795')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:53:52,621][25742] Updated weights for policy 0, policy_version 21421 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:53:54,299][25130] Fps is (10 sec: 11878.3, 60 sec: 11673.6, 300 sec: 7178.4). Total num frames: 87756800. Throughput: 0: 2935.6. Samples: 4431370. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:53:54,300][25130] Avg episode reward: [(0, '52.966')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:53:56,071][25742] Updated weights for policy 0, policy_version 21431 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:53:59,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11741.9, 300 sec: 7234.0). Total num frames: 87818240. Throughput: 0: 2935.7. Samples: 4448940. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:53:59,300][25130] Avg episode reward: [(0, '51.708')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:53:59,623][25742] Updated weights for policy 0, policy_version 21441 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:54:03,142][25742] Updated weights for policy 0, policy_version 21451 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:54:04,299][25130] Fps is (10 sec: 11878.2, 60 sec: 11741.9, 300 sec: 7289.5). Total num frames: 87875584. Throughput: 0: 2933.0. Samples: 4466352. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:54:04,300][25130] Avg episode reward: [(0, '50.166')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:54:06,624][25742] Updated weights for policy 0, policy_version 21461 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:54:09,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11741.9, 300 sec: 7331.1). Total num frames: 87932928. Throughput: 0: 2930.2. Samples: 4475198. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:54:09,300][25130] Avg episode reward: [(0, '51.024')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:54:10,098][25742] Updated weights for policy 0, policy_version 21471 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:54:13,646][25742] Updated weights for policy 0, policy_version 21481 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:54:14,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11673.6, 300 sec: 7386.7). Total num frames: 87990272. Throughput: 0: 2932.9. Samples: 4492838. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:54:14,300][25130] Avg episode reward: [(0, '52.285')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:54:17,150][25742] Updated weights for policy 0, policy_version 21491 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:54:19,299][25130] Fps is (10 sec: 11878.3, 60 sec: 11741.9, 300 sec: 7497.8). Total num frames: 88051712. Throughput: 0: 2934.4. Samples: 4510304. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:54:19,300][25130] Avg episode reward: [(0, '53.665')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:54:20,602][25742] Updated weights for policy 0, policy_version 21501 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:54:24,052][25742] Updated weights for policy 0, policy_version 21511 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:54:24,299][25130] Fps is (10 sec: 11878.5, 60 sec: 11741.9, 300 sec: 7650.5). Total num frames: 88109056. Throughput: 0: 2938.2. Samples: 4519146. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:54:24,300][25130] Avg episode reward: [(0, '53.200')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:54:24,397][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000021512_88113152.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:54:24,471][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000020997_86003712.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:54:27,581][25742] Updated weights for policy 0, policy_version 21521 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:54:29,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11673.6, 300 sec: 7789.4). Total num frames: 88166400. Throughput: 0: 2935.5. Samples: 4536830. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:54:29,300][25130] Avg episode reward: [(0, '53.241')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:54:31,163][25742] Updated weights for policy 0, policy_version 21531 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:54:34,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11673.6, 300 sec: 7942.1). Total num frames: 88223744. Throughput: 0: 2917.3. Samples: 4553602. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:54:34,300][25130] Avg episode reward: [(0, '53.554')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:54:34,744][25742] Updated weights for policy 0, policy_version 21541 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:54:38,294][25742] Updated weights for policy 0, policy_version 21551 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:54:39,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11673.6, 300 sec: 8094.8). Total num frames: 88281088. Throughput: 0: 2915.8. Samples: 4562582. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:54:39,300][25130] Avg episode reward: [(0, '53.246')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:54:41,971][25742] Updated weights for policy 0, policy_version 21561 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:54:44,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11673.6, 300 sec: 8247.6). Total num frames: 88338432. Throughput: 0: 2895.5. Samples: 4579236. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:54:44,300][25130] Avg episode reward: [(0, '51.346')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:54:45,591][25742] Updated weights for policy 0, policy_version 21571 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:54:49,169][25742] Updated weights for policy 0, policy_version 21581 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:54:49,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11605.3, 300 sec: 8386.4). Total num frames: 88395776. Throughput: 0: 2890.2. Samples: 4596410. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:54:49,300][25130] Avg episode reward: [(0, '48.527')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:54:52,675][25742] Updated weights for policy 0, policy_version 21591 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:54:54,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11605.3, 300 sec: 8539.1). Total num frames: 88453120. Throughput: 0: 2890.8. Samples: 4605286. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:54:54,300][25130] Avg episode reward: [(0, '49.479')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:54:56,248][25742] Updated weights for policy 0, policy_version 21601 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:54:59,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11537.1, 300 sec: 8691.9). Total num frames: 88510464. Throughput: 0: 2876.6. Samples: 4622284. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:54:59,300][25130] Avg episode reward: [(0, '50.894')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:54:59,832][25742] Updated weights for policy 0, policy_version 21611 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:55:03,316][25742] Updated weights for policy 0, policy_version 21621 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:55:04,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11537.1, 300 sec: 8830.7). Total num frames: 88567808. Throughput: 0: 2876.8. Samples: 4639758. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:55:04,300][25130] Avg episode reward: [(0, '51.602')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:55:06,874][25742] Updated weights for policy 0, policy_version 21631 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:55:09,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11537.1, 300 sec: 8983.5). Total num frames: 88625152. Throughput: 0: 2874.7. Samples: 4648508. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:55:09,300][25130] Avg episode reward: [(0, '51.868')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:55:10,463][25742] Updated weights for policy 0, policy_version 21641 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:55:13,982][25742] Updated weights for policy 0, policy_version 21651 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:55:14,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11537.1, 300 sec: 9136.2). Total num frames: 88682496. Throughput: 0: 2866.3. Samples: 4665814. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:55:14,300][25130] Avg episode reward: [(0, '51.300')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:55:17,561][25742] Updated weights for policy 0, policy_version 21661 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:55:19,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11468.8, 300 sec: 9288.9). Total num frames: 88739840. Throughput: 0: 2874.2. Samples: 4682942. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:55:19,300][25130] Avg episode reward: [(0, '50.950')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:55:21,104][25742] Updated weights for policy 0, policy_version 21671 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:55:24,299][25130] Fps is (10 sec: 11878.5, 60 sec: 11537.1, 300 sec: 9455.5). Total num frames: 88801280. Throughput: 0: 2867.7. Samples: 4691626. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:55:24,300][25130] Avg episode reward: [(0, '51.657')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:55:24,591][25742] Updated weights for policy 0, policy_version 21681 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:55:28,184][25742] Updated weights for policy 0, policy_version 21691 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:55:29,299][25130] Fps is (10 sec: 11878.3, 60 sec: 11537.1, 300 sec: 9594.4). Total num frames: 88858624. Throughput: 0: 2884.5. Samples: 4709038. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:55:29,300][25130] Avg episode reward: [(0, '51.967')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:55:31,757][25742] Updated weights for policy 0, policy_version 21701 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:55:34,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11537.1, 300 sec: 9747.1). Total num frames: 88915968. Throughput: 0: 2887.8. Samples: 4726362. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:55:34,300][25130] Avg episode reward: [(0, '52.380')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:55:35,276][25742] Updated weights for policy 0, policy_version 21711 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:55:38,861][25742] Updated weights for policy 0, policy_version 21721 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:55:39,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11537.1, 300 sec: 9899.9). Total num frames: 88973312. Throughput: 0: 2879.3. Samples: 4734856. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:55:39,300][25130] Avg episode reward: [(0, '53.811')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:55:42,371][25742] Updated weights for policy 0, policy_version 21731 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:55:44,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11537.1, 300 sec: 10052.6). Total num frames: 89030656. Throughput: 0: 2887.2. Samples: 4752210. Policy #0 lag: (min: 0.0, avg: 0.8, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:55:44,300][25130] Avg episode reward: [(0, '52.777')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:55:45,900][25742] Updated weights for policy 0, policy_version 21741 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:55:49,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11537.1, 300 sec: 10191.4). Total num frames: 89088000. Throughput: 0: 2884.4. Samples: 4769554. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:55:49,300][25130] Avg episode reward: [(0, '53.344')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:55:49,483][25742] Updated weights for policy 0, policy_version 21751 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:55:52,973][25742] Updated weights for policy 0, policy_version 21761 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:55:54,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11537.1, 300 sec: 10344.2). Total num frames: 89145344. Throughput: 0: 2888.3. Samples: 4778480. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:55:54,300][25130] Avg episode reward: [(0, '52.876')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:55:56,525][25742] Updated weights for policy 0, policy_version 21771 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:55:59,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11537.1, 300 sec: 10496.9). Total num frames: 89202688. Throughput: 0: 2890.4. Samples: 4795880. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:55:59,300][25130] Avg episode reward: [(0, '53.003')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:56:00,117][25742] Updated weights for policy 0, policy_version 21781 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:56:03,595][25742] Updated weights for policy 0, policy_version 21791 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:56:04,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11605.3, 300 sec: 10663.5). Total num frames: 89264128. Throughput: 0: 2896.3. Samples: 4813274. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:56:04,300][25130] Avg episode reward: [(0, '54.062')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:56:07,179][25742] Updated weights for policy 0, policy_version 21801 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:56:09,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11537.1, 300 sec: 10802.4). Total num frames: 89317376. Throughput: 0: 2890.5. Samples: 4821698. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:56:09,300][25130] Avg episode reward: [(0, '55.287')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:56:10,747][25742] Updated weights for policy 0, policy_version 21811 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:56:14,239][25742] Updated weights for policy 0, policy_version 21821 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:56:14,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11605.3, 300 sec: 10941.2). Total num frames: 89378816. Throughput: 0: 2889.6. Samples: 4839072. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:56:14,300][25130] Avg episode reward: [(0, '54.796')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:56:17,799][25742] Updated weights for policy 0, policy_version 21831 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:56:19,299][25130] Fps is (10 sec: 11878.3, 60 sec: 11605.3, 300 sec: 11052.3). Total num frames: 89436160. Throughput: 0: 2890.6. Samples: 4856438. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:56:19,300][25130] Avg episode reward: [(0, '53.942')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:56:21,347][25742] Updated weights for policy 0, policy_version 21841 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:56:24,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11537.1, 300 sec: 11093.9). Total num frames: 89493504. Throughput: 0: 2890.6. Samples: 4864932. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:56:24,300][25130] Avg episode reward: [(0, '53.625')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:56:24,500][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000021850_89497600.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:56:24,575][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000021168_86704128.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:56:24,859][25742] Updated weights for policy 0, policy_version 21851 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:56:28,438][25742] Updated weights for policy 0, policy_version 21861 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:56:29,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11537.1, 300 sec: 11149.5). Total num frames: 89550848. Throughput: 0: 2892.1. Samples: 4882356. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:56:29,300][25130] Avg episode reward: [(0, '52.869')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:56:31,987][25742] Updated weights for policy 0, policy_version 21871 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:56:34,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11537.0, 300 sec: 11205.0). Total num frames: 89608192. Throughput: 0: 2893.0. Samples: 4899738. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:56:34,300][25130] Avg episode reward: [(0, '53.003')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:56:35,495][25742] Updated weights for policy 0, policy_version 21881 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:56:39,082][25742] Updated weights for policy 0, policy_version 21891 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:56:39,299][25130] Fps is (10 sec: 11468.5, 60 sec: 11537.0, 300 sec: 11260.5). Total num frames: 89665536. Throughput: 0: 2891.3. Samples: 4908588. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:56:39,300][25130] Avg episode reward: [(0, '52.495')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:56:42,687][25742] Updated weights for policy 0, policy_version 21901 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:56:44,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11537.1, 300 sec: 11316.1). Total num frames: 89722880. Throughput: 0: 2878.2. Samples: 4925400. Policy #0 lag: (min: 0.0, avg: 0.9, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:56:44,300][25130] Avg episode reward: [(0, '52.555')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:56:46,211][25742] Updated weights for policy 0, policy_version 21911 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:56:49,299][25130] Fps is (10 sec: 11469.0, 60 sec: 11537.1, 300 sec: 11371.6). Total num frames: 89780224. Throughput: 0: 2876.1. Samples: 4942700. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:56:49,301][25130] Avg episode reward: [(0, '53.582')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:56:49,828][25742] Updated weights for policy 0, policy_version 21921 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:56:53,381][25742] Updated weights for policy 0, policy_version 21931 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:56:54,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11537.1, 300 sec: 11441.0). Total num frames: 89837568. Throughput: 0: 2885.3. Samples: 4951538. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:56:54,300][25130] Avg episode reward: [(0, '52.964')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:56:56,915][25742] Updated weights for policy 0, policy_version 21941 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:56:59,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11537.1, 300 sec: 11496.6). Total num frames: 89894912. Throughput: 0: 2881.7. Samples: 4968750. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:56:59,300][25130] Avg episode reward: [(0, '53.311')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:57:00,529][25742] Updated weights for policy 0, policy_version 21951 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:57:04,026][25742] Updated weights for policy 0, policy_version 21961 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:57:04,299][25130] Fps is (10 sec: 11468.6, 60 sec: 11468.8, 300 sec: 11552.1). Total num frames: 89952256. Throughput: 0: 2875.6. Samples: 4985840. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:57:04,300][25130] Avg episode reward: [(0, '52.465')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:57:07,588][25742] Updated weights for policy 0, policy_version 21971 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:57:09,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11537.1, 300 sec: 11607.6). Total num frames: 90009600. Throughput: 0: 2884.0. Samples: 4994714. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:57:09,301][25130] Avg episode reward: [(0, '51.879')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:57:11,170][25742] Updated weights for policy 0, policy_version 21981 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:57:14,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11468.8, 300 sec: 11607.6). Total num frames: 90066944. Throughput: 0: 2882.2. Samples: 5012056. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:57:14,300][25130] Avg episode reward: [(0, '52.972')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:57:14,680][25742] Updated weights for policy 0, policy_version 21991 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:57:18,250][25742] Updated weights for policy 0, policy_version 22001 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:57:19,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11468.8, 300 sec: 11607.7). Total num frames: 90124288. Throughput: 0: 2881.0. Samples: 5029384. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:57:19,300][25130] Avg episode reward: [(0, '52.656')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:57:21,824][25742] Updated weights for policy 0, policy_version 22011 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:57:24,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11537.1, 300 sec: 11607.7). Total num frames: 90185728. Throughput: 0: 2872.9. Samples: 5037866. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:57:24,300][25130] Avg episode reward: [(0, '54.308')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:57:25,327][25742] Updated weights for policy 0, policy_version 22021 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:57:28,910][25742] Updated weights for policy 0, policy_version 22031 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:57:29,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11537.1, 300 sec: 11607.7). Total num frames: 90243072. Throughput: 0: 2885.9. Samples: 5055264. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:57:29,300][25130] Avg episode reward: [(0, '52.643')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:57:32,500][25742] Updated weights for policy 0, policy_version 22041 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:57:34,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11537.1, 300 sec: 11593.8). Total num frames: 90300416. Throughput: 0: 2885.3. Samples: 5072538. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:57:34,300][25130] Avg episode reward: [(0, '51.213')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:57:36,006][25742] Updated weights for policy 0, policy_version 22051 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:57:39,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11537.1, 300 sec: 11593.8). Total num frames: 90357760. Throughput: 0: 2877.8. Samples: 5081040. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:57:39,300][25130] Avg episode reward: [(0, '50.582')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:57:39,635][25742] Updated weights for policy 0, policy_version 22061 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:57:43,178][25742] Updated weights for policy 0, policy_version 22071 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:57:44,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11537.1, 300 sec: 11593.8). Total num frames: 90415104. Throughput: 0: 2877.8. Samples: 5098250. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:57:44,300][25130] Avg episode reward: [(0, '51.269')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:57:46,746][25742] Updated weights for policy 0, policy_version 22081 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:57:49,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11537.1, 300 sec: 11579.9). Total num frames: 90472448. Throughput: 0: 2882.6. Samples: 5115558. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:57:49,300][25130] Avg episode reward: [(0, '51.867')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:57:50,342][25742] Updated weights for policy 0, policy_version 22091 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:57:53,851][25742] Updated weights for policy 0, policy_version 22101 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:57:54,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11537.1, 300 sec: 11579.9). Total num frames: 90529792. Throughput: 0: 2873.0. Samples: 5123998. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:57:54,300][25130] Avg episode reward: [(0, '51.855')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:57:57,384][25742] Updated weights for policy 0, policy_version 22111 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:57:59,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11537.1, 300 sec: 11579.9). Total num frames: 90587136. Throughput: 0: 2874.8. Samples: 5141420. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:57:59,300][25130] Avg episode reward: [(0, '52.695')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:58:01,023][25742] Updated weights for policy 0, policy_version 22121 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:58:04,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11537.1, 300 sec: 11579.9). Total num frames: 90644480. Throughput: 0: 2875.2. Samples: 5158768. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:58:04,300][25130] Avg episode reward: [(0, '51.976')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:58:04,529][25742] Updated weights for policy 0, policy_version 22131 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:58:08,064][25742] Updated weights for policy 0, policy_version 22141 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:58:09,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11537.1, 300 sec: 11566.0). Total num frames: 90701824. Throughput: 0: 2882.7. Samples: 5167586. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:58:09,300][25130] Avg episode reward: [(0, '51.967')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:58:11,632][25742] Updated weights for policy 0, policy_version 22151 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:58:14,299][25130] Fps is (10 sec: 11468.6, 60 sec: 11537.1, 300 sec: 11566.0). Total num frames: 90759168. Throughput: 0: 2874.5. Samples: 5184616. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:58:14,301][25130] Avg episode reward: [(0, '52.550')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:58:15,156][25742] Updated weights for policy 0, policy_version 22161 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:58:18,713][25742] Updated weights for policy 0, policy_version 22171 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:58:19,299][25130] Fps is (10 sec: 11468.6, 60 sec: 11537.0, 300 sec: 11566.0). Total num frames: 90816512. Throughput: 0: 2876.7. Samples: 5201990. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:58:19,300][25130] Avg episode reward: [(0, '51.269')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:58:22,280][25742] Updated weights for policy 0, policy_version 22181 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:58:24,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11468.8, 300 sec: 11552.1). Total num frames: 90873856. Throughput: 0: 2885.0. Samples: 5210864. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:58:24,300][25130] Avg episode reward: [(0, '51.184')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:58:24,379][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000022187_90877952.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:58:24,452][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000021512_88113152.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:58:25,786][25742] Updated weights for policy 0, policy_version 22191 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:58:29,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11468.8, 300 sec: 11552.1). Total num frames: 90931200. Throughput: 0: 2889.2. Samples: 5228266. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:58:29,300][25130] Avg episode reward: [(0, '51.489')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:58:29,386][25742] Updated weights for policy 0, policy_version 22201 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:58:32,931][25742] Updated weights for policy 0, policy_version 22211 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:58:34,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11468.8, 300 sec: 11552.1). Total num frames: 90988544. Throughput: 0: 2886.0. Samples: 5245430. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:58:34,301][25130] Avg episode reward: [(0, '53.055')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:58:36,441][25742] Updated weights for policy 0, policy_version 22221 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:58:39,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11468.8, 300 sec: 11552.1). Total num frames: 91045888. Throughput: 0: 2890.6. Samples: 5254074. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:58:39,300][25130] Avg episode reward: [(0, '53.571')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:58:40,039][25742] Updated weights for policy 0, policy_version 22231 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:58:43,570][25742] Updated weights for policy 0, policy_version 22241 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:58:44,299][25130] Fps is (10 sec: 11878.5, 60 sec: 11537.1, 300 sec: 11552.1). Total num frames: 91107328. Throughput: 0: 2887.9. Samples: 5271376. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:58:44,300][25130] Avg episode reward: [(0, '53.627')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:58:47,079][25742] Updated weights for policy 0, policy_version 22251 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:58:49,299][25130] Fps is (10 sec: 11878.3, 60 sec: 11537.1, 300 sec: 11552.1). Total num frames: 91164672. Throughput: 0: 2888.8. Samples: 5288766. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:58:49,300][25130] Avg episode reward: [(0, '53.308')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:58:50,667][25742] Updated weights for policy 0, policy_version 22261 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:58:54,196][25742] Updated weights for policy 0, policy_version 22271 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:58:54,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11537.1, 300 sec: 11538.2). Total num frames: 91222016. Throughput: 0: 2879.7. Samples: 5297172. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:58:54,300][25130] Avg episode reward: [(0, '53.107')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:58:57,717][25742] Updated weights for policy 0, policy_version 22281 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:58:59,299][25130] Fps is (10 sec: 11468.6, 60 sec: 11537.0, 300 sec: 11538.2). Total num frames: 91279360. Throughput: 0: 2889.1. Samples: 5314628. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:58:59,301][25130] Avg episode reward: [(0, '53.310')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:59:01,326][25742] Updated weights for policy 0, policy_version 22291 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:59:04,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11537.1, 300 sec: 11538.2). Total num frames: 91336704. Throughput: 0: 2886.7. Samples: 5331892. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:59:04,300][25130] Avg episode reward: [(0, '53.588')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:59:04,843][25742] Updated weights for policy 0, policy_version 22301 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:59:08,384][25742] Updated weights for policy 0, policy_version 22311 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:59:09,299][25130] Fps is (10 sec: 11469.0, 60 sec: 11537.1, 300 sec: 11538.2). Total num frames: 91394048. Throughput: 0: 2888.5. Samples: 5340848. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:59:09,300][25130] Avg episode reward: [(0, '52.475')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:59:11,965][25742] Updated weights for policy 0, policy_version 22321 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:59:14,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11537.1, 300 sec: 11524.3). Total num frames: 91451392. Throughput: 0: 2878.9. Samples: 5357816. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:59:14,300][25130] Avg episode reward: [(0, '51.588')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:59:15,485][25742] Updated weights for policy 0, policy_version 22331 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:59:19,046][25742] Updated weights for policy 0, policy_version 22341 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:59:19,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11537.1, 300 sec: 11524.3). Total num frames: 91508736. Throughput: 0: 2882.6. Samples: 5375146. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:59:19,300][25130] Avg episode reward: [(0, '50.704')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:59:22,641][25742] Updated weights for policy 0, policy_version 22351 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:59:24,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11537.1, 300 sec: 11524.3). Total num frames: 91566080. Throughput: 0: 2885.3. Samples: 5383912. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:59:24,301][25130] Avg episode reward: [(0, '51.936')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:59:26,177][25742] Updated weights for policy 0, policy_version 22361 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:59:29,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11537.1, 300 sec: 11524.3). Total num frames: 91623424. Throughput: 0: 2888.0. Samples: 5401338. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:59:29,300][25130] Avg episode reward: [(0, '52.615')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:59:29,783][25742] Updated weights for policy 0, policy_version 22371 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:59:33,346][25742] Updated weights for policy 0, policy_version 22381 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:59:34,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11537.1, 300 sec: 11524.3). Total num frames: 91680768. Throughput: 0: 2875.5. Samples: 5418164. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:59:34,300][25130] Avg episode reward: [(0, '54.242')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:59:36,861][25742] Updated weights for policy 0, policy_version 22391 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:59:39,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11537.1, 300 sec: 11524.3). Total num frames: 91738112. Throughput: 0: 2887.6. Samples: 5427116. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:59:39,300][25130] Avg episode reward: [(0, '54.490')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:59:40,448][25742] Updated weights for policy 0, policy_version 22401 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:59:43,998][25742] Updated weights for policy 0, policy_version 22411 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:59:44,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11468.8, 300 sec: 11524.3). Total num frames: 91795456. Throughput: 0: 2883.7. Samples: 5444396. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:59:44,300][25130] Avg episode reward: [(0, '54.018')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:59:47,530][25742] Updated weights for policy 0, policy_version 22421 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:59:49,299][25130] Fps is (10 sec: 11878.3, 60 sec: 11537.1, 300 sec: 11538.2). Total num frames: 91856896. Throughput: 0: 2887.6. Samples: 5461832. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:59:49,300][25130] Avg episode reward: [(0, '53.265')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:59:50,997][25742] Updated weights for policy 0, policy_version 22431 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:59:54,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11537.1, 300 sec: 11538.2). Total num frames: 91914240. Throughput: 0: 2879.6. Samples: 5470432. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:59:54,300][25130] Avg episode reward: [(0, '53.255')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:59:54,515][25742] Updated weights for policy 0, policy_version 22441 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 20:59:58,064][25742] Updated weights for policy 0, policy_version 22451 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:59:59,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11537.1, 300 sec: 11538.2). Total num frames: 91971584. Throughput: 0: 2889.7. Samples: 5487854. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 20:59:59,300][25130] Avg episode reward: [(0, '53.333')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:00:01,552][25742] Updated weights for policy 0, policy_version 22461 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:00:04,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11537.1, 300 sec: 11538.2). Total num frames: 92028928. Throughput: 0: 2900.9. Samples: 5505684. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:00:04,300][25130] Avg episode reward: [(0, '54.213')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:00:05,012][25742] Updated weights for policy 0, policy_version 22471 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:00:08,530][25742] Updated weights for policy 0, policy_version 22481 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:00:09,299][25130] Fps is (10 sec: 11878.3, 60 sec: 11605.3, 300 sec: 11552.1). Total num frames: 92090368. Throughput: 0: 2898.8. Samples: 5514356. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:00:09,300][25130] Avg episode reward: [(0, '54.059')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:00:12,062][25742] Updated weights for policy 0, policy_version 22491 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:00:14,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11605.3, 300 sec: 11552.1). Total num frames: 92147712. Throughput: 0: 2897.9. Samples: 5531744. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:00:14,300][25130] Avg episode reward: [(0, '52.618')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:00:15,544][25742] Updated weights for policy 0, policy_version 22501 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:00:19,067][25742] Updated weights for policy 0, policy_version 22511 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:00:19,299][25130] Fps is (10 sec: 11468.6, 60 sec: 11605.3, 300 sec: 11538.2). Total num frames: 92205056. Throughput: 0: 2913.0. Samples: 5549250. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:00:19,301][25130] Avg episode reward: [(0, '51.058')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:00:22,539][25742] Updated weights for policy 0, policy_version 22521 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:00:24,299][25130] Fps is (10 sec: 11878.3, 60 sec: 11673.6, 300 sec: 11552.1). Total num frames: 92266496. Throughput: 0: 2914.3. Samples: 5558262. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:00:24,300][25130] Avg episode reward: [(0, '51.715')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:00:24,303][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000022526_92266496.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:00:24,382][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000021850_89497600.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:00:26,007][25742] Updated weights for policy 0, policy_version 22531 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:00:29,299][25130] Fps is (10 sec: 11878.7, 60 sec: 11673.6, 300 sec: 11552.1). Total num frames: 92323840. Throughput: 0: 2920.4. Samples: 5575812. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:00:29,300][25130] Avg episode reward: [(0, '51.856')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:00:29,504][25742] Updated weights for policy 0, policy_version 22541 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:00:33,005][25742] Updated weights for policy 0, policy_version 22551 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:00:34,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11673.6, 300 sec: 11552.1). Total num frames: 92381184. Throughput: 0: 2922.0. Samples: 5593322. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:00:34,300][25130] Avg episode reward: [(0, '52.810')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:00:36,524][25742] Updated weights for policy 0, policy_version 22561 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:00:39,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11673.6, 300 sec: 11552.1). Total num frames: 92438528. Throughput: 0: 2929.8. Samples: 5602274. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:00:39,300][25130] Avg episode reward: [(0, '53.102')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:00:40,048][25742] Updated weights for policy 0, policy_version 22571 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:00:43,567][25742] Updated weights for policy 0, policy_version 22581 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:00:44,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11741.9, 300 sec: 11566.0). Total num frames: 92499968. Throughput: 0: 2930.3. Samples: 5619716. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:00:44,300][25130] Avg episode reward: [(0, '54.140')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:00:47,101][25742] Updated weights for policy 0, policy_version 22591 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:00:49,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11673.6, 300 sec: 11566.0). Total num frames: 92557312. Throughput: 0: 2920.4. Samples: 5637100. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:00:49,300][25130] Avg episode reward: [(0, '53.945')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:00:50,592][25742] Updated weights for policy 0, policy_version 22601 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:00:54,096][25742] Updated weights for policy 0, policy_version 22611 (0.0013)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:00:54,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11673.6, 300 sec: 11566.0). Total num frames: 92614656. Throughput: 0: 2922.1. Samples: 5645850. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:00:54,300][25130] Avg episode reward: [(0, '54.084')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:00:57,610][25742] Updated weights for policy 0, policy_version 22621 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:00:59,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11673.6, 300 sec: 11552.1). Total num frames: 92672000. Throughput: 0: 2928.9. Samples: 5663546. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:00:59,300][25130] Avg episode reward: [(0, '54.167')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:01:01,129][25742] Updated weights for policy 0, policy_version 22631 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:01:04,299][25130] Fps is (10 sec: 11878.6, 60 sec: 11741.9, 300 sec: 11579.9). Total num frames: 92733440. Throughput: 0: 2929.1. Samples: 5681058. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:01:04,300][25130] Avg episode reward: [(0, '54.863')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:01:04,595][25742] Updated weights for policy 0, policy_version 22641 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:01:08,064][25742] Updated weights for policy 0, policy_version 22651 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:01:09,300][25130] Fps is (10 sec: 11877.9, 60 sec: 11673.5, 300 sec: 11566.0). Total num frames: 92790784. Throughput: 0: 2920.8. Samples: 5689698. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:01:09,301][25130] Avg episode reward: [(0, '54.713')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:01:11,544][25742] Updated weights for policy 0, policy_version 22661 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:01:14,299][25130] Fps is (10 sec: 11468.6, 60 sec: 11673.6, 300 sec: 11566.0). Total num frames: 92848128. Throughput: 0: 2929.6. Samples: 5707644. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:01:14,300][25130] Avg episode reward: [(0, '54.735')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:01:15,012][25742] Updated weights for policy 0, policy_version 22671 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:01:18,483][25742] Updated weights for policy 0, policy_version 22681 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:01:19,299][25130] Fps is (10 sec: 11878.9, 60 sec: 11741.9, 300 sec: 11579.9). Total num frames: 92909568. Throughput: 0: 2931.2. Samples: 5725228. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:01:19,300][25130] Avg episode reward: [(0, '52.847')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:01:21,945][25742] Updated weights for policy 0, policy_version 22691 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:01:24,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11673.6, 300 sec: 11579.9). Total num frames: 92966912. Throughput: 0: 2932.4. Samples: 5734232. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:01:24,300][25130] Avg episode reward: [(0, '52.859')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:01:25,424][25742] Updated weights for policy 0, policy_version 22701 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:01:28,936][25742] Updated weights for policy 0, policy_version 22711 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:01:29,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11741.9, 300 sec: 11593.8). Total num frames: 93028352. Throughput: 0: 2933.8. Samples: 5751736. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:01:29,300][25130] Avg episode reward: [(0, '52.519')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:01:32,519][25742] Updated weights for policy 0, policy_version 22721 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:01:34,299][25130] Fps is (10 sec: 11878.5, 60 sec: 11741.9, 300 sec: 11593.8). Total num frames: 93085696. Throughput: 0: 2932.8. Samples: 5769076. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:01:34,300][25130] Avg episode reward: [(0, '52.733')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:01:36,091][25742] Updated weights for policy 0, policy_version 22731 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:01:39,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11741.9, 300 sec: 11593.8). Total num frames: 93143040. Throughput: 0: 2926.6. Samples: 5777546. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:01:39,300][25130] Avg episode reward: [(0, '53.177')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:01:39,599][25742] Updated weights for policy 0, policy_version 22741 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:01:43,077][25742] Updated weights for policy 0, policy_version 22751 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:01:44,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11673.6, 300 sec: 11593.8). Total num frames: 93200384. Throughput: 0: 2922.0. Samples: 5795038. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:01:44,301][25130] Avg episode reward: [(0, '53.678')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:01:46,552][25742] Updated weights for policy 0, policy_version 22761 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:01:49,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11673.6, 300 sec: 11593.8). Total num frames: 93257728. Throughput: 0: 2928.8. Samples: 5812854. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:01:49,300][25130] Avg episode reward: [(0, '54.367')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:01:50,055][25742] Updated weights for policy 0, policy_version 22771 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:01:53,542][25742] Updated weights for policy 0, policy_version 22781 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:01:54,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11741.9, 300 sec: 11607.6). Total num frames: 93319168. Throughput: 0: 2930.0. Samples: 5821546. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:01:54,300][25130] Avg episode reward: [(0, '53.353')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:01:57,019][25742] Updated weights for policy 0, policy_version 22791 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:01:59,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11741.9, 300 sec: 11607.7). Total num frames: 93376512. Throughput: 0: 2921.5. Samples: 5839112. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:01:59,300][25130] Avg episode reward: [(0, '53.970')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:02:00,526][25742] Updated weights for policy 0, policy_version 22801 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:02:03,998][25742] Updated weights for policy 0, policy_version 22811 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:02:04,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11673.6, 300 sec: 11607.6). Total num frames: 93433856. Throughput: 0: 2923.8. Samples: 5856798. Policy #0 lag: (min: 0.0, avg: 1.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:02:04,300][25130] Avg episode reward: [(0, '52.754')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:02:07,502][25742] Updated weights for policy 0, policy_version 22821 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:02:09,299][25130] Fps is (10 sec: 11878.3, 60 sec: 11741.9, 300 sec: 11621.5). Total num frames: 93495296. Throughput: 0: 2918.2. Samples: 5865552. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:02:09,300][25130] Avg episode reward: [(0, '51.213')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:02:11,020][25742] Updated weights for policy 0, policy_version 22831 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:02:14,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11741.9, 300 sec: 11621.5). Total num frames: 93552640. Throughput: 0: 2917.5. Samples: 5883024. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:02:14,300][25130] Avg episode reward: [(0, '51.142')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:02:14,528][25742] Updated weights for policy 0, policy_version 22841 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:02:18,006][25742] Updated weights for policy 0, policy_version 22851 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:02:19,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11673.6, 300 sec: 11607.6). Total num frames: 93609984. Throughput: 0: 2921.2. Samples: 5900530. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:02:19,300][25130] Avg episode reward: [(0, '52.438')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:02:21,474][25742] Updated weights for policy 0, policy_version 22861 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:02:24,299][25130] Fps is (10 sec: 11878.3, 60 sec: 11741.9, 300 sec: 11621.5). Total num frames: 93671424. Throughput: 0: 2933.1. Samples: 5909536. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:02:24,300][25130] Avg episode reward: [(0, '53.708')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:02:24,303][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000022869_93671424.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:02:24,388][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000022187_90877952.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:02:24,980][25742] Updated weights for policy 0, policy_version 22871 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:02:28,492][25742] Updated weights for policy 0, policy_version 22881 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:02:29,300][25130] Fps is (10 sec: 11877.9, 60 sec: 11673.5, 300 sec: 11621.5). Total num frames: 93728768. Throughput: 0: 2931.6. Samples: 5926960. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:02:29,301][25130] Avg episode reward: [(0, '54.993')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:02:32,014][25742] Updated weights for policy 0, policy_version 22891 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:02:34,299][25130] Fps is (10 sec: 11469.0, 60 sec: 11673.6, 300 sec: 11621.5). Total num frames: 93786112. Throughput: 0: 2923.3. Samples: 5944402. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:02:34,300][25130] Avg episode reward: [(0, '54.522')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:02:35,513][25742] Updated weights for policy 0, policy_version 22901 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:02:39,025][25742] Updated weights for policy 0, policy_version 22911 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:02:39,300][25130] Fps is (10 sec: 11468.8, 60 sec: 11673.5, 300 sec: 11621.5). Total num frames: 93843456. Throughput: 0: 2928.7. Samples: 5953340. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:02:39,301][25130] Avg episode reward: [(0, '53.308')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:02:42,518][25742] Updated weights for policy 0, policy_version 22921 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:02:44,299][25130] Fps is (10 sec: 11878.3, 60 sec: 11741.9, 300 sec: 11635.4). Total num frames: 93904896. Throughput: 0: 2927.9. Samples: 5970868. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:02:44,300][25130] Avg episode reward: [(0, '54.733')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:02:46,019][25742] Updated weights for policy 0, policy_version 22931 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:02:49,299][25130] Fps is (10 sec: 11878.9, 60 sec: 11741.9, 300 sec: 11635.4). Total num frames: 93962240. Throughput: 0: 2922.7. Samples: 5988320. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:02:49,300][25130] Avg episode reward: [(0, '54.240')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:02:49,549][25742] Updated weights for policy 0, policy_version 22941 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:02:53,064][25742] Updated weights for policy 0, policy_version 22951 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:02:54,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11673.6, 300 sec: 11635.4). Total num frames: 94019584. Throughput: 0: 2925.1. Samples: 5997182. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:02:54,300][25130] Avg episode reward: [(0, '53.466')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:02:56,561][25742] Updated weights for policy 0, policy_version 22961 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:02:59,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11673.6, 300 sec: 11635.4). Total num frames: 94076928. Throughput: 0: 2924.6. Samples: 6014630. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:02:59,300][25130] Avg episode reward: [(0, '53.973')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:03:00,079][25742] Updated weights for policy 0, policy_version 22971 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:03:03,552][25742] Updated weights for policy 0, policy_version 22981 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:03:04,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11741.9, 300 sec: 11649.3). Total num frames: 94138368. Throughput: 0: 2926.5. Samples: 6032222. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:03:04,300][25130] Avg episode reward: [(0, '54.048')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:03:07,030][25742] Updated weights for policy 0, policy_version 22991 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:03:09,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11673.6, 300 sec: 11649.3). Total num frames: 94195712. Throughput: 0: 2916.7. Samples: 6040786. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:03:09,300][25130] Avg episode reward: [(0, '53.900')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:03:10,529][25742] Updated weights for policy 0, policy_version 23001 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:03:14,044][25742] Updated weights for policy 0, policy_version 23011 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:03:14,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11673.6, 300 sec: 11649.3). Total num frames: 94253056. Throughput: 0: 2922.7. Samples: 6058482. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:03:14,300][25130] Avg episode reward: [(0, '54.817')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:03:17,546][25742] Updated weights for policy 0, policy_version 23021 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:03:19,299][25130] Fps is (10 sec: 11878.2, 60 sec: 11741.8, 300 sec: 11663.2). Total num frames: 94314496. Throughput: 0: 2928.4. Samples: 6076180. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:03:19,300][25130] Avg episode reward: [(0, '53.403')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:03:21,048][25742] Updated weights for policy 0, policy_version 23031 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:03:24,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11673.6, 300 sec: 11663.2). Total num frames: 94371840. Throughput: 0: 2918.6. Samples: 6084674. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:03:24,300][25130] Avg episode reward: [(0, '53.739')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:03:24,538][25742] Updated weights for policy 0, policy_version 23041 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:03:28,003][25742] Updated weights for policy 0, policy_version 23051 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:03:29,299][25130] Fps is (10 sec: 11469.1, 60 sec: 11673.7, 300 sec: 11663.2). Total num frames: 94429184. Throughput: 0: 2923.5. Samples: 6102426. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:03:29,300][25130] Avg episode reward: [(0, '53.300')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:03:31,502][25742] Updated weights for policy 0, policy_version 23061 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:03:34,299][25130] Fps is (10 sec: 11878.5, 60 sec: 11741.9, 300 sec: 11677.1). Total num frames: 94490624. Throughput: 0: 2930.9. Samples: 6120212. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:03:34,300][25130] Avg episode reward: [(0, '52.407')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:03:34,976][25742] Updated weights for policy 0, policy_version 23071 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:03:38,485][25742] Updated weights for policy 0, policy_version 23081 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:03:39,299][25130] Fps is (10 sec: 11878.2, 60 sec: 11741.9, 300 sec: 11663.2). Total num frames: 94547968. Throughput: 0: 2924.7. Samples: 6128792. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:03:39,300][25130] Avg episode reward: [(0, '52.509')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:03:41,956][25742] Updated weights for policy 0, policy_version 23091 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:03:44,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11673.6, 300 sec: 11663.2). Total num frames: 94605312. Throughput: 0: 2930.7. Samples: 6146512. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:03:44,300][25130] Avg episode reward: [(0, '52.235')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:03:45,442][25742] Updated weights for policy 0, policy_version 23101 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:03:48,935][25742] Updated weights for policy 0, policy_version 23111 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:03:49,299][25130] Fps is (10 sec: 11878.5, 60 sec: 11741.9, 300 sec: 11677.1). Total num frames: 94666752. Throughput: 0: 2933.8. Samples: 6164244. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:03:49,300][25130] Avg episode reward: [(0, '52.621')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:03:52,437][25742] Updated weights for policy 0, policy_version 23121 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:03:54,299][25130] Fps is (10 sec: 11878.5, 60 sec: 11741.9, 300 sec: 11677.1). Total num frames: 94724096. Throughput: 0: 2933.6. Samples: 6172800. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:03:54,300][25130] Avg episode reward: [(0, '52.685')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:03:55,913][25742] Updated weights for policy 0, policy_version 23131 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:03:59,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11741.9, 300 sec: 11677.1). Total num frames: 94781440. Throughput: 0: 2934.4. Samples: 6190530. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:03:59,301][25130] Avg episode reward: [(0, '51.037')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:03:59,395][25742] Updated weights for policy 0, policy_version 23141 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:04:02,879][25742] Updated weights for policy 0, policy_version 23151 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:04:04,299][25130] Fps is (10 sec: 11878.3, 60 sec: 11741.9, 300 sec: 11691.0). Total num frames: 94842880. Throughput: 0: 2935.6. Samples: 6208282. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:04:04,300][25130] Avg episode reward: [(0, '51.143')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:04:06,348][25742] Updated weights for policy 0, policy_version 23161 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:04:09,300][25130] Fps is (10 sec: 11877.9, 60 sec: 11741.8, 300 sec: 11690.9). Total num frames: 94900224. Throughput: 0: 2936.9. Samples: 6216834. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:04:09,301][25130] Avg episode reward: [(0, '49.931')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:04:09,833][25742] Updated weights for policy 0, policy_version 23171 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:04:13,304][25742] Updated weights for policy 0, policy_version 23181 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:04:14,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11741.9, 300 sec: 11691.0). Total num frames: 94957568. Throughput: 0: 2940.8. Samples: 6234762. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:04:14,300][25130] Avg episode reward: [(0, '49.766')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:04:16,781][25742] Updated weights for policy 0, policy_version 23191 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:04:19,299][25130] Fps is (10 sec: 11878.9, 60 sec: 11741.9, 300 sec: 11704.8). Total num frames: 95019008. Throughput: 0: 2937.1. Samples: 6252380. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:04:19,300][25130] Avg episode reward: [(0, '50.585')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:04:20,247][25742] Updated weights for policy 0, policy_version 23201 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:04:23,711][25742] Updated weights for policy 0, policy_version 23211 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:04:24,299][25130] Fps is (10 sec: 11878.2, 60 sec: 11741.9, 300 sec: 11704.8). Total num frames: 95076352. Throughput: 0: 2943.1. Samples: 6261232. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:04:24,300][25130] Avg episode reward: [(0, '51.741')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:04:24,403][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000023213_95080448.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:04:24,477][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000022526_92266496.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:04:27,191][25742] Updated weights for policy 0, policy_version 23221 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:04:29,299][25130] Fps is (10 sec: 11878.5, 60 sec: 11810.1, 300 sec: 11718.7). Total num frames: 95137792. Throughput: 0: 2943.3. Samples: 6278962. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:04:29,300][25130] Avg episode reward: [(0, '52.216')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:04:30,673][25742] Updated weights for policy 0, policy_version 23231 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:04:34,186][25742] Updated weights for policy 0, policy_version 23241 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:04:34,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11741.8, 300 sec: 11718.7). Total num frames: 95195136. Throughput: 0: 2938.7. Samples: 6296488. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:04:34,300][25130] Avg episode reward: [(0, '52.880')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:04:37,677][25742] Updated weights for policy 0, policy_version 23251 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:04:39,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11741.9, 300 sec: 11718.7). Total num frames: 95252480. Throughput: 0: 2944.5. Samples: 6305302. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:04:39,300][25130] Avg episode reward: [(0, '51.938')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:04:41,161][25742] Updated weights for policy 0, policy_version 23261 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:04:44,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11741.9, 300 sec: 11704.8). Total num frames: 95309824. Throughput: 0: 2943.0. Samples: 6322964. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:04:44,300][25130] Avg episode reward: [(0, '51.733')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:04:44,669][25742] Updated weights for policy 0, policy_version 23271 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:04:48,174][25742] Updated weights for policy 0, policy_version 23281 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:04:49,299][25130] Fps is (10 sec: 11878.5, 60 sec: 11741.9, 300 sec: 11718.7). Total num frames: 95371264. Throughput: 0: 2936.5. Samples: 6340426. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:04:49,300][25130] Avg episode reward: [(0, '51.270')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:04:51,643][25742] Updated weights for policy 0, policy_version 23291 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:04:54,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11741.9, 300 sec: 11718.7). Total num frames: 95428608. Throughput: 0: 2942.6. Samples: 6349248. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:04:54,300][25130] Avg episode reward: [(0, '53.299')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:04:55,117][25742] Updated weights for policy 0, policy_version 23301 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:04:58,581][25742] Updated weights for policy 0, policy_version 23311 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:04:59,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11810.2, 300 sec: 11732.6). Total num frames: 95490048. Throughput: 0: 2939.2. Samples: 6367026. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:04:59,300][25130] Avg episode reward: [(0, '54.328')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:05:02,090][25742] Updated weights for policy 0, policy_version 23321 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:05:04,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11741.9, 300 sec: 11718.7). Total num frames: 95547392. Throughput: 0: 2937.4. Samples: 6384562. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:05:04,300][25130] Avg episode reward: [(0, '54.737')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:05:05,565][25742] Updated weights for policy 0, policy_version 23331 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:05:09,047][25742] Updated weights for policy 0, policy_version 23341 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:05:09,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11742.0, 300 sec: 11718.7). Total num frames: 95604736. Throughput: 0: 2940.1. Samples: 6393538. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:05:09,301][25130] Avg episode reward: [(0, '55.120')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:05:12,539][25742] Updated weights for policy 0, policy_version 23351 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:05:14,299][25130] Fps is (10 sec: 11878.5, 60 sec: 11810.1, 300 sec: 11732.6). Total num frames: 95666176. Throughput: 0: 2935.2. Samples: 6411046. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:05:14,300][25130] Avg episode reward: [(0, '54.657')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:05:16,002][25742] Updated weights for policy 0, policy_version 23361 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:05:19,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11741.9, 300 sec: 11718.7). Total num frames: 95723520. Throughput: 0: 2936.6. Samples: 6428636. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:05:19,300][25130] Avg episode reward: [(0, '54.304')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:05:19,494][25742] Updated weights for policy 0, policy_version 23371 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:05:22,955][25742] Updated weights for policy 0, policy_version 23381 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:05:24,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11741.9, 300 sec: 11718.7). Total num frames: 95780864. Throughput: 0: 2940.7. Samples: 6437634. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:05:24,300][25130] Avg episode reward: [(0, '53.436')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:05:26,427][25742] Updated weights for policy 0, policy_version 23391 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:05:29,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11741.9, 300 sec: 11732.6). Total num frames: 95842304. Throughput: 0: 2939.9. Samples: 6455260. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:05:29,300][25130] Avg episode reward: [(0, '51.193')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:05:29,903][25742] Updated weights for policy 0, policy_version 23401 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:05:33,384][25742] Updated weights for policy 0, policy_version 23411 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:05:34,299][25130] Fps is (10 sec: 11878.3, 60 sec: 11741.9, 300 sec: 11732.6). Total num frames: 95899648. Throughput: 0: 2941.6. Samples: 6472796. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:05:34,300][25130] Avg episode reward: [(0, '50.195')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:05:36,900][25742] Updated weights for policy 0, policy_version 23421 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:05:39,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11741.9, 300 sec: 11718.7). Total num frames: 95956992. Throughput: 0: 2944.1. Samples: 6481732. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:05:39,300][25130] Avg episode reward: [(0, '50.542')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:05:40,390][25742] Updated weights for policy 0, policy_version 23431 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:05:43,881][25742] Updated weights for policy 0, policy_version 23441 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:05:44,299][25130] Fps is (10 sec: 11878.5, 60 sec: 11810.1, 300 sec: 11732.6). Total num frames: 96018432. Throughput: 0: 2938.5. Samples: 6499258. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:05:44,300][25130] Avg episode reward: [(0, '51.800')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:05:47,355][25742] Updated weights for policy 0, policy_version 23451 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:05:49,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11741.9, 300 sec: 11732.6). Total num frames: 96075776. Throughput: 0: 2937.8. Samples: 6516764. Policy #0 lag: (min: 0.0, avg: 1.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:05:49,300][25130] Avg episode reward: [(0, '52.416')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:05:50,845][25742] Updated weights for policy 0, policy_version 23461 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:05:54,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11741.9, 300 sec: 11732.6). Total num frames: 96133120. Throughput: 0: 2937.6. Samples: 6525732. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:05:54,300][25130] Avg episode reward: [(0, '50.817')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:05:54,308][25742] Updated weights for policy 0, policy_version 23471 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:05:57,819][25742] Updated weights for policy 0, policy_version 23481 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:05:59,299][25130] Fps is (10 sec: 11878.3, 60 sec: 11741.9, 300 sec: 11732.6). Total num frames: 96194560. Throughput: 0: 2938.7. Samples: 6543290. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:05:59,300][25130] Avg episode reward: [(0, '50.822')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:06:01,324][25742] Updated weights for policy 0, policy_version 23491 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:06:04,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11741.9, 300 sec: 11732.6). Total num frames: 96251904. Throughput: 0: 2935.7. Samples: 6560742. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:06:04,300][25130] Avg episode reward: [(0, '50.875')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:06:04,842][25742] Updated weights for policy 0, policy_version 23501 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:06:08,343][25742] Updated weights for policy 0, policy_version 23511 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:06:09,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11741.9, 300 sec: 11732.6). Total num frames: 96309248. Throughput: 0: 2934.3. Samples: 6569678. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:06:09,301][25130] Avg episode reward: [(0, '52.123')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:06:11,872][25742] Updated weights for policy 0, policy_version 23521 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:06:14,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11673.6, 300 sec: 11718.7). Total num frames: 96366592. Throughput: 0: 2930.6. Samples: 6587136. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:06:14,300][25130] Avg episode reward: [(0, '54.484')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:06:15,385][25742] Updated weights for policy 0, policy_version 23531 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:06:18,884][25742] Updated weights for policy 0, policy_version 23541 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:06:19,299][25130] Fps is (10 sec: 11878.6, 60 sec: 11741.9, 300 sec: 11732.6). Total num frames: 96428032. Throughput: 0: 2928.7. Samples: 6604586. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:06:19,300][25130] Avg episode reward: [(0, '54.645')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:06:22,385][25742] Updated weights for policy 0, policy_version 23551 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:06:24,299][25130] Fps is (10 sec: 11878.2, 60 sec: 11741.8, 300 sec: 11718.7). Total num frames: 96485376. Throughput: 0: 2920.1. Samples: 6613136. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:06:24,301][25130] Avg episode reward: [(0, '55.427')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:06:24,530][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000023557_96489472.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:06:24,607][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000022869_93671424.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:06:25,947][25742] Updated weights for policy 0, policy_version 23561 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:06:29,299][25130] Fps is (10 sec: 11468.5, 60 sec: 11673.6, 300 sec: 11718.7). Total num frames: 96542720. Throughput: 0: 2916.7. Samples: 6630512. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:06:29,300][25130] Avg episode reward: [(0, '52.526')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:06:29,452][25742] Updated weights for policy 0, policy_version 23571 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:06:32,985][25742] Updated weights for policy 0, policy_version 23581 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:06:34,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11673.6, 300 sec: 11718.7). Total num frames: 96600064. Throughput: 0: 2913.9. Samples: 6647888. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:06:34,300][25130] Avg episode reward: [(0, '50.548')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:06:36,500][25742] Updated weights for policy 0, policy_version 23591 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:06:39,299][25130] Fps is (10 sec: 11469.1, 60 sec: 11673.6, 300 sec: 11718.7). Total num frames: 96657408. Throughput: 0: 2913.9. Samples: 6656858. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:06:39,300][25130] Avg episode reward: [(0, '51.137')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:06:40,021][25742] Updated weights for policy 0, policy_version 23601 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:06:43,546][25742] Updated weights for policy 0, policy_version 23611 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:06:44,299][25130] Fps is (10 sec: 11878.5, 60 sec: 11673.6, 300 sec: 11732.6). Total num frames: 96718848. Throughput: 0: 2910.9. Samples: 6674282. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:06:44,300][25130] Avg episode reward: [(0, '52.226')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:06:47,049][25742] Updated weights for policy 0, policy_version 23621 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:06:49,299][25130] Fps is (10 sec: 11878.2, 60 sec: 11673.6, 300 sec: 11718.7). Total num frames: 96776192. Throughput: 0: 2911.0. Samples: 6691736. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:06:49,301][25130] Avg episode reward: [(0, '53.771')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:06:50,564][25742] Updated weights for policy 0, policy_version 23631 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:06:54,124][25742] Updated weights for policy 0, policy_version 23641 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:06:54,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11673.6, 300 sec: 11718.7). Total num frames: 96833536. Throughput: 0: 2906.1. Samples: 6700452. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:06:54,300][25130] Avg episode reward: [(0, '52.880')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:06:57,636][25742] Updated weights for policy 0, policy_version 23651 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:06:59,299][25130] Fps is (10 sec: 11469.0, 60 sec: 11605.4, 300 sec: 11718.7). Total num frames: 96890880. Throughput: 0: 2904.4. Samples: 6717832. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:06:59,300][25130] Avg episode reward: [(0, '51.770')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:07:01,135][25742] Updated weights for policy 0, policy_version 23661 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:07:04,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11673.6, 300 sec: 11718.7). Total num frames: 96952320. Throughput: 0: 2909.9. Samples: 6735532. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:07:04,300][25130] Avg episode reward: [(0, '51.215')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:07:04,605][25742] Updated weights for policy 0, policy_version 23671 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:07:08,080][25742] Updated weights for policy 0, policy_version 23681 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:07:09,299][25130] Fps is (10 sec: 11878.3, 60 sec: 11673.6, 300 sec: 11718.7). Total num frames: 97009664. Throughput: 0: 2910.7. Samples: 6744118. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:07:09,300][25130] Avg episode reward: [(0, '50.903')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:07:11,572][25742] Updated weights for policy 0, policy_version 23691 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:07:14,300][25130] Fps is (10 sec: 11468.0, 60 sec: 11673.5, 300 sec: 11718.7). Total num frames: 97067008. Throughput: 0: 2922.5. Samples: 6762024. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:07:14,301][25130] Avg episode reward: [(0, '50.659')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:07:15,043][25742] Updated weights for policy 0, policy_version 23701 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:07:18,524][25742] Updated weights for policy 0, policy_version 23711 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:07:19,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11673.6, 300 sec: 11718.7). Total num frames: 97128448. Throughput: 0: 2927.8. Samples: 6779638. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:07:19,300][25130] Avg episode reward: [(0, '49.698')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:07:22,073][25742] Updated weights for policy 0, policy_version 23721 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:07:24,299][25130] Fps is (10 sec: 11879.2, 60 sec: 11673.6, 300 sec: 11718.7). Total num frames: 97185792. Throughput: 0: 2916.4. Samples: 6788098. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:07:24,300][25130] Avg episode reward: [(0, '51.467')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:07:25,581][25742] Updated weights for policy 0, policy_version 23731 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:07:29,145][25742] Updated weights for policy 0, policy_version 23741 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:07:29,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11673.6, 300 sec: 11718.7). Total num frames: 97243136. Throughput: 0: 2917.1. Samples: 6805550. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:07:29,300][25130] Avg episode reward: [(0, '50.790')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:07:32,625][25742] Updated weights for policy 0, policy_version 23751 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:07:34,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11673.6, 300 sec: 11718.7). Total num frames: 97300480. Throughput: 0: 2917.3. Samples: 6823016. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:07:34,300][25130] Avg episode reward: [(0, '52.526')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:07:36,130][25742] Updated weights for policy 0, policy_version 23761 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:07:39,299][25130] Fps is (10 sec: 11878.5, 60 sec: 11741.9, 300 sec: 11718.7). Total num frames: 97361920. Throughput: 0: 2922.3. Samples: 6831956. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:07:39,300][25130] Avg episode reward: [(0, '51.991')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:07:39,649][25742] Updated weights for policy 0, policy_version 23771 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:07:43,158][25742] Updated weights for policy 0, policy_version 23781 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:07:44,299][25130] Fps is (10 sec: 11878.5, 60 sec: 11673.6, 300 sec: 11718.7). Total num frames: 97419264. Throughput: 0: 2924.4. Samples: 6849428. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:07:44,300][25130] Avg episode reward: [(0, '52.160')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:07:46,657][25742] Updated weights for policy 0, policy_version 23791 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:07:49,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11673.6, 300 sec: 11718.7). Total num frames: 97476608. Throughput: 0: 2919.6. Samples: 6866914. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:07:49,300][25130] Avg episode reward: [(0, '51.622')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:07:50,172][25742] Updated weights for policy 0, policy_version 23801 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:07:53,705][25742] Updated weights for policy 0, policy_version 23811 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:07:54,299][25130] Fps is (10 sec: 11468.6, 60 sec: 11673.6, 300 sec: 11718.7). Total num frames: 97533952. Throughput: 0: 2926.7. Samples: 6875818. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:07:54,300][25130] Avg episode reward: [(0, '52.930')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:07:57,235][25742] Updated weights for policy 0, policy_version 23821 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:07:59,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11673.6, 300 sec: 11704.8). Total num frames: 97591296. Throughput: 0: 2915.6. Samples: 6893224. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:07:59,300][25130] Avg episode reward: [(0, '52.017')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:08:00,784][25742] Updated weights for policy 0, policy_version 23831 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:08:04,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11605.3, 300 sec: 11704.8). Total num frames: 97648640. Throughput: 0: 2908.6. Samples: 6910526. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:08:04,301][25130] Avg episode reward: [(0, '51.360')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:08:04,345][25742] Updated weights for policy 0, policy_version 23841 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:08:07,847][25742] Updated weights for policy 0, policy_version 23851 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:08:09,299][25130] Fps is (10 sec: 11878.6, 60 sec: 11673.6, 300 sec: 11718.7). Total num frames: 97710080. Throughput: 0: 2909.6. Samples: 6919028. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:08:09,300][25130] Avg episode reward: [(0, '50.722')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:08:11,338][25742] Updated weights for policy 0, policy_version 23861 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:08:14,299][25130] Fps is (10 sec: 11878.5, 60 sec: 11673.7, 300 sec: 11704.8). Total num frames: 97767424. Throughput: 0: 2911.6. Samples: 6936572. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:08:14,300][25130] Avg episode reward: [(0, '50.690')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:08:14,819][25742] Updated weights for policy 0, policy_version 23871 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:08:18,301][25742] Updated weights for policy 0, policy_version 23881 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:08:19,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11605.3, 300 sec: 11704.8). Total num frames: 97824768. Throughput: 0: 2917.0. Samples: 6954282. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:08:19,300][25130] Avg episode reward: [(0, '52.231')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:08:21,816][25742] Updated weights for policy 0, policy_version 23891 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:08:24,299][25130] Fps is (10 sec: 11878.3, 60 sec: 11673.6, 300 sec: 11718.7). Total num frames: 97886208. Throughput: 0: 2914.1. Samples: 6963090. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:08:24,300][25130] Avg episode reward: [(0, '52.671')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:08:24,303][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000023898_97886208.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:08:24,375][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000023213_95080448.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:08:25,315][25742] Updated weights for policy 0, policy_version 23901 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:08:28,820][25742] Updated weights for policy 0, policy_version 23911 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:08:29,299][25130] Fps is (10 sec: 11878.5, 60 sec: 11673.6, 300 sec: 11704.8). Total num frames: 97943552. Throughput: 0: 2914.1. Samples: 6980562. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:08:29,300][25130] Avg episode reward: [(0, '53.328')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:08:32,380][25742] Updated weights for policy 0, policy_version 23921 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:08:34,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11673.6, 300 sec: 11704.8). Total num frames: 98000896. Throughput: 0: 2912.2. Samples: 6997962. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:08:34,300][25130] Avg episode reward: [(0, '53.894')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:08:35,886][25742] Updated weights for policy 0, policy_version 23931 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:08:39,300][25130] Fps is (10 sec: 11468.2, 60 sec: 11605.2, 300 sec: 11704.8). Total num frames: 98058240. Throughput: 0: 2912.5. Samples: 7006882. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:08:39,301][25130] Avg episode reward: [(0, '54.701')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:08:39,389][25742] Updated weights for policy 0, policy_version 23941 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:08:42,898][25742] Updated weights for policy 0, policy_version 23951 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:08:44,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11673.6, 300 sec: 11704.8). Total num frames: 98119680. Throughput: 0: 2914.8. Samples: 7024388. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:08:44,300][25130] Avg episode reward: [(0, '55.132')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:08:46,434][25742] Updated weights for policy 0, policy_version 23961 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:08:49,299][25130] Fps is (10 sec: 11879.0, 60 sec: 11673.6, 300 sec: 11704.8). Total num frames: 98177024. Throughput: 0: 2916.5. Samples: 7041770. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:08:49,300][25130] Avg episode reward: [(0, '54.870')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:08:49,943][25742] Updated weights for policy 0, policy_version 23971 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:08:53,409][25742] Updated weights for policy 0, policy_version 23981 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:08:54,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11673.6, 300 sec: 11704.8). Total num frames: 98234368. Throughput: 0: 2918.0. Samples: 7050338. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:08:54,300][25130] Avg episode reward: [(0, '54.690')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:08:56,902][25742] Updated weights for policy 0, policy_version 23991 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:08:59,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11673.6, 300 sec: 11691.0). Total num frames: 98291712. Throughput: 0: 2927.1. Samples: 7068292. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:08:59,300][25130] Avg episode reward: [(0, '53.317')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:09:00,429][25742] Updated weights for policy 0, policy_version 24001 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:09:03,910][25742] Updated weights for policy 0, policy_version 24011 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:09:04,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11741.9, 300 sec: 11704.9). Total num frames: 98353152. Throughput: 0: 2921.6. Samples: 7085752. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:09:04,300][25130] Avg episode reward: [(0, '52.707')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:09:07,424][25742] Updated weights for policy 0, policy_version 24021 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:09:09,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11673.6, 300 sec: 11704.8). Total num frames: 98410496. Throughput: 0: 2915.5. Samples: 7094290. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:09:09,301][25130] Avg episode reward: [(0, '52.523')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:09:10,919][25742] Updated weights for policy 0, policy_version 24031 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:09:14,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11673.6, 300 sec: 11691.0). Total num frames: 98467840. Throughput: 0: 2918.2. Samples: 7111882. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:09:14,300][25130] Avg episode reward: [(0, '53.406')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:09:14,416][25742] Updated weights for policy 0, policy_version 24041 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:09:17,904][25742] Updated weights for policy 0, policy_version 24051 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:09:19,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11673.6, 300 sec: 11691.0). Total num frames: 98525184. Throughput: 0: 2927.8. Samples: 7129712. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:09:19,300][25130] Avg episode reward: [(0, '54.374')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:09:21,432][25742] Updated weights for policy 0, policy_version 24061 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:09:24,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11673.6, 300 sec: 11691.0). Total num frames: 98586624. Throughput: 0: 2920.3. Samples: 7138294. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:09:24,300][25130] Avg episode reward: [(0, '54.378')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:09:24,898][25742] Updated weights for policy 0, policy_version 24071 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:09:28,370][25742] Updated weights for policy 0, policy_version 24081 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:09:29,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11673.6, 300 sec: 11691.0). Total num frames: 98643968. Throughput: 0: 2924.2. Samples: 7155976. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:09:29,300][25130] Avg episode reward: [(0, '53.874')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:09:31,900][25742] Updated weights for policy 0, policy_version 24091 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:09:34,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11673.6, 300 sec: 11691.0). Total num frames: 98701312. Throughput: 0: 2929.1. Samples: 7173580. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:09:34,300][25130] Avg episode reward: [(0, '53.593')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:09:35,367][25742] Updated weights for policy 0, policy_version 24101 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:09:38,843][25742] Updated weights for policy 0, policy_version 24111 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:09:39,299][25130] Fps is (10 sec: 11878.3, 60 sec: 11741.9, 300 sec: 11704.8). Total num frames: 98762752. Throughput: 0: 2933.3. Samples: 7182336. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:09:39,301][25130] Avg episode reward: [(0, '52.609')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:09:42,306][25742] Updated weights for policy 0, policy_version 24121 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:09:44,299][25130] Fps is (10 sec: 11878.3, 60 sec: 11673.6, 300 sec: 11691.0). Total num frames: 98820096. Throughput: 0: 2928.3. Samples: 7200066. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:09:44,301][25130] Avg episode reward: [(0, '52.847')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:09:45,781][25742] Updated weights for policy 0, policy_version 24131 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:09:49,286][25742] Updated weights for policy 0, policy_version 24141 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:09:49,299][25130] Fps is (10 sec: 11878.5, 60 sec: 11741.9, 300 sec: 11704.8). Total num frames: 98881536. Throughput: 0: 2934.7. Samples: 7217814. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:09:49,300][25130] Avg episode reward: [(0, '52.720')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:09:52,768][25742] Updated weights for policy 0, policy_version 24151 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:09:54,299][25130] Fps is (10 sec: 11878.5, 60 sec: 11741.9, 300 sec: 11691.0). Total num frames: 98938880. Throughput: 0: 2935.6. Samples: 7226390. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:09:54,300][25130] Avg episode reward: [(0, '52.421')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:09:56,263][25742] Updated weights for policy 0, policy_version 24161 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:09:59,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11741.9, 300 sec: 11691.0). Total num frames: 98996224. Throughput: 0: 2937.5. Samples: 7244070. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:09:59,300][25130] Avg episode reward: [(0, '53.584')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:09:59,747][25742] Updated weights for policy 0, policy_version 24171 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:10:03,215][25742] Updated weights for policy 0, policy_version 24181 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:10:04,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11741.9, 300 sec: 11704.8). Total num frames: 99057664. Throughput: 0: 2937.3. Samples: 7261890. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:10:04,300][25130] Avg episode reward: [(0, '53.195')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:10:06,711][25742] Updated weights for policy 0, policy_version 24191 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:10:09,299][25130] Fps is (10 sec: 11878.2, 60 sec: 11741.9, 300 sec: 11690.9). Total num frames: 99115008. Throughput: 0: 2935.4. Samples: 7270386. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:10:09,300][25130] Avg episode reward: [(0, '54.027')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:10:10,221][25742] Updated weights for policy 0, policy_version 24201 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:10:13,722][25742] Updated weights for policy 0, policy_version 24211 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:10:14,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11741.9, 300 sec: 11691.0). Total num frames: 99172352. Throughput: 0: 2936.0. Samples: 7288098. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:10:14,300][25130] Avg episode reward: [(0, '54.282')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:10:17,238][25742] Updated weights for policy 0, policy_version 24221 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:10:19,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11741.9, 300 sec: 11691.0). Total num frames: 99229696. Throughput: 0: 2933.8. Samples: 7305602. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:10:19,300][25130] Avg episode reward: [(0, '51.874')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:10:20,749][25742] Updated weights for policy 0, policy_version 24231 (0.0013)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:10:24,246][25742] Updated weights for policy 0, policy_version 24241 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:10:24,299][25130] Fps is (10 sec: 11878.3, 60 sec: 11741.9, 300 sec: 11691.0). Total num frames: 99291136. Throughput: 0: 2932.9. Samples: 7314318. Policy #0 lag: (min: 0.0, avg: 1.0, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:10:24,300][25130] Avg episode reward: [(0, '52.786')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:10:24,303][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000024241_99291136.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:10:24,378][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000023557_96489472.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:10:27,790][25742] Updated weights for policy 0, policy_version 24251 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:10:29,299][25130] Fps is (10 sec: 11878.5, 60 sec: 11741.9, 300 sec: 11691.0). Total num frames: 99348480. Throughput: 0: 2927.2. Samples: 7331788. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:10:29,300][25130] Avg episode reward: [(0, '52.417')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:10:31,287][25742] Updated weights for policy 0, policy_version 24261 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:10:34,299][25130] Fps is (10 sec: 11469.0, 60 sec: 11741.9, 300 sec: 11691.0). Total num frames: 99405824. Throughput: 0: 2920.5. Samples: 7349236. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:10:34,300][25130] Avg episode reward: [(0, '53.621')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:10:34,795][25742] Updated weights for policy 0, policy_version 24271 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:10:38,273][25742] Updated weights for policy 0, policy_version 24281 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:10:39,299][25130] Fps is (10 sec: 11468.7, 60 sec: 11673.6, 300 sec: 11677.1). Total num frames: 99463168. Throughput: 0: 2930.1. Samples: 7358244. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:10:39,300][25130] Avg episode reward: [(0, '53.231')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:10:41,760][25742] Updated weights for policy 0, policy_version 24291 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:10:44,299][25130] Fps is (10 sec: 11878.3, 60 sec: 11741.9, 300 sec: 11691.0). Total num frames: 99524608. Throughput: 0: 2927.5. Samples: 7375810. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:10:44,300][25130] Avg episode reward: [(0, '52.422')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:10:45,223][25742] Updated weights for policy 0, policy_version 24301 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:10:48,710][25742] Updated weights for policy 0, policy_version 24311 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:10:49,299][25130] Fps is (10 sec: 11878.3, 60 sec: 11673.6, 300 sec: 11691.0). Total num frames: 99581952. Throughput: 0: 2921.8. Samples: 7393372. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:10:49,300][25130] Avg episode reward: [(0, '52.471')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:10:52,204][25742] Updated weights for policy 0, policy_version 24321 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:10:54,299][25130] Fps is (10 sec: 11468.8, 60 sec: 11673.6, 300 sec: 11677.1). Total num frames: 99639296. Throughput: 0: 2932.7. Samples: 7402358. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:10:54,300][25130] Avg episode reward: [(0, '51.632')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:10:55,767][25742] Updated weights for policy 0, policy_version 24331 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:10:59,291][25742] Updated weights for policy 0, policy_version 24341 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:10:59,299][25130] Fps is (10 sec: 11878.5, 60 sec: 11741.9, 300 sec: 11691.0). Total num frames: 99700736. Throughput: 0: 2924.7. Samples: 7419708. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:10:59,300][25130] Avg episode reward: [(0, '53.251')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:11:02,821][25742] Updated weights for policy 0, policy_version 24351 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:04,299][25130] Fps is (10 sec: 11878.3, 60 sec: 11673.6, 300 sec: 11691.0). Total num frames: 99758080. Throughput: 0: 2922.4. Samples: 7437108. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:04,300][25130] Avg episode reward: [(0, '53.555')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:11:06,295][25742] Updated weights for policy 0, policy_version 24361 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:09,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11673.6, 300 sec: 11691.0). Total num frames: 99815424. Throughput: 0: 2924.1. Samples: 7445904. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:09,300][25130] Avg episode reward: [(0, '52.526')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:11:09,785][25742] Updated weights for policy 0, policy_version 24371 (0.0011)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:11:13,261][25742] Updated weights for policy 0, policy_version 24381 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:14,299][25130] Fps is (10 sec: 11468.9, 60 sec: 11673.6, 300 sec: 11677.1). Total num frames: 99872768. Throughput: 0: 2929.6. Samples: 7463622. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:14,300][25130] Avg episode reward: [(0, '52.138')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:11:16,759][25742] Updated weights for policy 0, policy_version 24391 (0.0012)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:19,299][25130] Fps is (10 sec: 11878.3, 60 sec: 11741.9, 300 sec: 11691.0). Total num frames: 99934208. Throughput: 0: 2931.1. Samples: 7481136. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:19,300][25130] Avg episode reward: [(0, '51.862')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:11:20,242][25742] Updated weights for policy 0, policy_version 24401 (0.0012)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:11:23,723][25742] Updated weights for policy 0, policy_version 24411 (0.0011)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:24,299][25130] Fps is (10 sec: 11878.4, 60 sec: 11673.6, 300 sec: 11691.0). Total num frames: 99991552. Throughput: 0: 2928.0. Samples: 7490002. Policy #0 lag: (min: 0.0, avg: 1.1, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:24,300][25130] Avg episode reward: [(0, '53.165')]\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,467][25130] Component Batcher_0 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,467][25727] Stopping Batcher_0...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,467][25727] Loop batcher_evt_loop terminating...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:11:25,468][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000024416_100007936.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,480][25130] Component RolloutWorker_w2 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,480][25743] Stopping RolloutWorker_w2...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,480][25741] Stopping RolloutWorker_w1...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,480][25747] Stopping RolloutWorker_w7...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,481][25743] Loop rollout_proc2_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,481][25741] Loop rollout_proc1_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,481][25745] Stopping RolloutWorker_w4...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,481][25747] Loop rollout_proc7_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,481][25740] Stopping RolloutWorker_w0...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,482][25130] Component RolloutWorker_w1 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,481][25745] Loop rollout_proc4_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,481][25748] Stopping RolloutWorker_w6...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,481][25740] Loop rollout_proc0_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,481][25748] Loop rollout_proc6_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,481][25746] Stopping RolloutWorker_w5...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,481][25746] Loop rollout_proc5_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,482][25744] Stopping RolloutWorker_w3...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,482][25744] Loop rollout_proc3_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,483][25130] Component RolloutWorker_w7 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,483][25130] Component RolloutWorker_w4 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,484][25130] Component RolloutWorker_w0 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,484][25130] Component RolloutWorker_w6 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,485][25130] Component RolloutWorker_w5 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,485][25130] Component RolloutWorker_w3 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,497][25742] Weights refcount: 2 0\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,498][25742] Stopping InferenceWorker_p0-w0...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,499][25742] Loop inference_proc0-0_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,503][25130] Component InferenceWorker_p0-w0 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,567][25727] Removing /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000023898_97886208.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:11:25,579][25727] Saving /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000024416_100007936.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,707][25130] Component LearnerWorker_p0 stopped!\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,706][25727] Stopping LearnerWorker_p0...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,707][25727] Loop learner_proc0_evt_loop terminating...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:25,708][25130] Waiting for process learner_proc0 to stop...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:26,593][25130] Waiting for process inference_proc0-0 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:26,594][25130] Waiting for process rollout_proc0 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:26,594][25130] Waiting for process rollout_proc1 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:26,595][25130] Waiting for process rollout_proc2 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:26,596][25130] Waiting for process rollout_proc3 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:26,596][25130] Waiting for process rollout_proc4 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:26,597][25130] Waiting for process rollout_proc5 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:26,597][25130] Waiting for process rollout_proc6 to join...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:26,597][25130] Waiting for process rollout_proc7 to join...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:11:26,598][25130] Batcher 0 profile tree view:\n",
      "batching: 59.7956, releasing_batches: 0.2405\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:11:26,598][25130] InferenceWorker_p0-w0 profile tree view:\n",
      "wait_policy: 0.0000\n",
      "  wait_policy_total: 17.0434\n",
      "update_model: 26.6694\n",
      "  weight_update: 0.0011\n",
      "one_step: 0.0026\n",
      "  handle_policy_step: 3420.4092\n",
      "    deserialize: 54.9042, stack: 7.3408, obs_to_device_normalize: 562.6929, forward: 2068.9029, send_messages: 74.6180\n",
      "    prepare_outputs: 599.8194\n",
      "      to_cpu: 533.3212\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:11:26,598][25130] Learner 0 profile tree view:\n",
      "misc: 0.0357, prepare_batch: 110.2995\n",
      "train: 2767.0231\n",
      "  epoch_init: 0.0685, minibatch_init: 0.0844, losses_postprocess: 2.3632, kl_divergence: 1.6554, after_optimizer: 7.0744\n",
      "  calculate_losses: 913.3665\n",
      "    losses_init: 0.0418, forward_head: 20.9282, bptt_initial: 870.4973, tail: 3.7315, advantages_returns: 0.9802, losses: 9.5816\n",
      "    bptt: 6.1521\n",
      "      bptt_forward_core: 5.8673\n",
      "  update: 1839.4362\n",
      "    clip: 7.4547\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:11:26,599][25130] RolloutWorker_w0 profile tree view:\n",
      "wait_for_trajectories: 0.8267, enqueue_policy_requests: 49.6830, env_step: 683.0266, overhead: 62.0049, complete_rollouts: 1.5404\n",
      "save_policy_outputs: 63.0357\n",
      "  split_output_tensors: 29.4261\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:11:26,599][25130] RolloutWorker_w7 profile tree view:\n",
      "wait_for_trajectories: 0.8489, enqueue_policy_requests: 52.9575, env_step: 696.2411, overhead: 63.7248, complete_rollouts: 1.5861\n",
      "save_policy_outputs: 64.2133\n",
      "  split_output_tensors: 29.9482\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:26,599][25130] Loop Runner_EvtLoop terminating...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:11:26,599][25130] Runner profile tree view:\n",
      "main_loop: 3537.7643\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:11:26,600][25130] Collected {0: 100007936}, FPS: 8480.8\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## Start the training, this should take around 15 minutes\n",
    "register_vizdoom_components()\n",
    "\n",
    "# The scenario we train on today is health gathering\n",
    "# other scenarios include \"doom_basic\", \"doom_two_colors_easy\", \"doom_dm\", \"doom_dwango5\", \"doom_my_way_home\", \"doom_deadly_corridor\", \"doom_defend_the_center\", \"doom_defend_the_line\"\n",
    "env = \"doom_health_gathering_supreme\"\n",
    "cfg = parse_vizdoom_cfg(\n",
    "    argv=[f\"--env={env}\", \n",
    "          \"--experiment=conv_resnet\",\n",
    "          \"--seed=200\",\n",
    "          \"--num_workers=8\",                    # Number of parallel environment workers.8\n",
    "          \"--num_envs_per_worker=4\",            # Number of envs on a single CPU actor.4\n",
    "          \"--batch_size=1024\",\n",
    "          \"--encoder_conv_architecture=resnet_impala\",\n",
    "          \"--train_for_env_steps=100000000\"]\n",
    ")\n",
    "\n",
    "# sample_size = num_workers * num_envs_per_worker * rollout \n",
    "# = 8 * 4 * 32 = 1024\n",
    "# = 16 * 8 * 32 = 4096\n",
    "# = 20 * 12 * 32 = 7680\n",
    "\n",
    "# batch_size = 2048\n",
    "\n",
    "status = run_rl(cfg)\n",
    "# run_rl(cfg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2024-07-05 21:11:59,040][25130] Loading existing experiment configuration from /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/config.json\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:59,041][25130] Overriding arg 'num_workers' with value 1 passed from command line\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:59,041][25130] Adding new argument 'no_render'=True that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:59,042][25130] Adding new argument 'save_video'=True that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:59,042][25130] Adding new argument 'video_frames'=1000000000.0 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:59,043][25130] Adding new argument 'video_name'=None that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:59,043][25130] Adding new argument 'max_num_frames'=1000000000.0 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:59,043][25130] Adding new argument 'max_num_episodes'=10 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:59,044][25130] Adding new argument 'push_to_hub'=False that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:59,044][25130] Adding new argument 'hf_repository'=None that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:59,045][25130] Adding new argument 'policy_index'=0 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:59,045][25130] Adding new argument 'eval_deterministic'=False that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:59,045][25130] Adding new argument 'train_script'=None that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:59,046][25130] Adding new argument 'enjoy_script'=None that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:59,046][25130] Using frameskip 1 and render_action_repeat=4 for evaluation\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:59,074][25130] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-05 21:11:59,077][25130] RunningMeanStd input shape: (3, 72, 128)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:59,078][25130] RunningMeanStd input shape: (1,)\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:59,087][25130] Num input channels: 3\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:59,096][25130] Convolutional layer output size: 4608\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:11:59,113][25130] Policy head output size: 512\u001b[0m\n",
      "\u001b[33m[2024-07-05 21:12:00,869][25130] Loading state from checkpoint /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000024416_100007936.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:01,723][25130] Num frames 100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:01,805][25130] Num frames 200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:01,896][25130] Num frames 300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:01,994][25130] Num frames 400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:02,070][25130] Num frames 500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:02,142][25130] Num frames 600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:02,215][25130] Num frames 700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:02,286][25130] Num frames 800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:02,358][25130] Num frames 900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:02,432][25130] Num frames 1000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:02,504][25130] Num frames 1100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:02,574][25130] Num frames 1200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:02,645][25130] Num frames 1300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:02,718][25130] Num frames 1400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:02,789][25130] Num frames 1500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:02,859][25130] Num frames 1600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:02,931][25130] Num frames 1700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:03,004][25130] Num frames 1800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:03,076][25130] Num frames 1900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:03,147][25130] Num frames 2000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:03,222][25130] Num frames 2100...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:12:03,273][25130] Avg episode rewards: #0: 55.999, true rewards: #0: 21.000\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:12:03,275][25130] Avg episode reward: 55.999, avg true_objective: 21.000\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:03,347][25130] Num frames 2200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:03,418][25130] Num frames 2300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:03,489][25130] Num frames 2400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:03,559][25130] Num frames 2500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:03,631][25130] Num frames 2600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:03,702][25130] Num frames 2700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:03,778][25130] Num frames 2800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:03,850][25130] Num frames 2900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:03,923][25130] Num frames 3000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:03,992][25130] Num frames 3100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:04,064][25130] Num frames 3200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:04,134][25130] Num frames 3300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:04,204][25130] Num frames 3400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:04,285][25130] Num frames 3500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:04,358][25130] Num frames 3600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:04,430][25130] Num frames 3700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:04,501][25130] Num frames 3800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:04,575][25130] Num frames 3900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:04,648][25130] Num frames 4000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:04,720][25130] Num frames 4100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:04,792][25130] Num frames 4200...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:12:04,844][25130] Avg episode rewards: #0: 58.999, true rewards: #0: 21.000\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:12:04,845][25130] Avg episode reward: 58.999, avg true_objective: 21.000\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:04,916][25130] Num frames 4300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:04,987][25130] Num frames 4400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:05,057][25130] Num frames 4500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:05,127][25130] Num frames 4600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:05,197][25130] Num frames 4700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:05,270][25130] Num frames 4800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:05,345][25130] Num frames 4900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:05,419][25130] Num frames 5000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:05,489][25130] Num frames 5100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:05,562][25130] Num frames 5200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:05,636][25130] Num frames 5300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:05,707][25130] Num frames 5400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:05,778][25130] Num frames 5500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:05,851][25130] Num frames 5600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:05,925][25130] Num frames 5700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:05,997][25130] Num frames 5800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:06,072][25130] Num frames 5900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:06,143][25130] Num frames 6000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:06,216][25130] Num frames 6100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:06,290][25130] Num frames 6200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:06,370][25130] Num frames 6300...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:12:06,422][25130] Avg episode rewards: #0: 59.332, true rewards: #0: 21.000\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:12:06,423][25130] Avg episode reward: 59.332, avg true_objective: 21.000\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:06,498][25130] Num frames 6400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:06,569][25130] Num frames 6500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:06,639][25130] Num frames 6600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:06,709][25130] Num frames 6700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:06,786][25130] Num frames 6800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:06,858][25130] Num frames 6900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:06,928][25130] Num frames 7000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:06,998][25130] Num frames 7100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:07,069][25130] Num frames 7200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:07,138][25130] Num frames 7300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:07,210][25130] Num frames 7400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:07,281][25130] Num frames 7500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:07,358][25130] Num frames 7600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:07,429][25130] Num frames 7700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:07,501][25130] Num frames 7800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:07,569][25130] Num frames 7900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:07,643][25130] Num frames 8000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:07,715][25130] Num frames 8100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:07,801][25130] Num frames 8200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:07,873][25130] Num frames 8300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:07,950][25130] Num frames 8400...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:12:08,002][25130] Avg episode rewards: #0: 58.999, true rewards: #0: 21.000\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:12:08,003][25130] Avg episode reward: 58.999, avg true_objective: 21.000\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:08,082][25130] Num frames 8500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:08,155][25130] Num frames 8600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:08,232][25130] Num frames 8700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:08,313][25130] Num frames 8800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:08,387][25130] Num frames 8900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:08,462][25130] Num frames 9000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:08,537][25130] Num frames 9100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:08,614][25130] Num frames 9200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:08,690][25130] Num frames 9300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:08,764][25130] Num frames 9400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:08,842][25130] Num frames 9500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:08,919][25130] Num frames 9600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:08,994][25130] Num frames 9700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:09,067][25130] Num frames 9800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:09,141][25130] Num frames 9900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:09,217][25130] Num frames 10000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:09,291][25130] Num frames 10100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:09,365][25130] Num frames 10200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:09,439][25130] Num frames 10300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:09,511][25130] Num frames 10400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:09,588][25130] Num frames 10500...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:12:09,640][25130] Avg episode rewards: #0: 58.399, true rewards: #0: 21.000\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:12:09,640][25130] Avg episode reward: 58.399, avg true_objective: 21.000\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:09,715][25130] Num frames 10600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:09,792][25130] Num frames 10700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:09,865][25130] Num frames 10800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:09,937][25130] Num frames 10900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:10,008][25130] Num frames 11000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:10,080][25130] Num frames 11100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:10,154][25130] Num frames 11200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:10,227][25130] Num frames 11300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:10,300][25130] Num frames 11400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:10,376][25130] Num frames 11500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:10,449][25130] Num frames 11600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:10,522][25130] Num frames 11700...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:12:10,592][25130] Avg episode rewards: #0: 53.197, true rewards: #0: 19.532\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:12:10,593][25130] Avg episode reward: 53.197, avg true_objective: 19.532\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:10,659][25130] Num frames 11800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:10,729][25130] Num frames 11900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:10,803][25130] Num frames 12000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:10,878][25130] Num frames 12100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:10,953][25130] Num frames 12200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:11,028][25130] Num frames 12300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:11,101][25130] Num frames 12400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:11,176][25130] Num frames 12500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:11,250][25130] Num frames 12600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:11,323][25130] Num frames 12700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:11,397][25130] Num frames 12800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:11,477][25130] Num frames 12900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:11,549][25130] Num frames 13000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:11,621][25130] Num frames 13100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:11,694][25130] Num frames 13200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:11,768][25130] Num frames 13300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:11,840][25130] Num frames 13400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:11,912][25130] Num frames 13500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:11,985][25130] Num frames 13600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:12,056][25130] Num frames 13700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:12,128][25130] Num frames 13800...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:12:12,197][25130] Avg episode rewards: #0: 53.883, true rewards: #0: 19.741\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:12:12,198][25130] Avg episode reward: 53.883, avg true_objective: 19.741\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:12,259][25130] Num frames 13900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:12,330][25130] Num frames 14000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:12,401][25130] Num frames 14100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:12,472][25130] Num frames 14200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:12,547][25130] Num frames 14300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:12,619][25130] Num frames 14400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:12,688][25130] Num frames 14500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:12,759][25130] Num frames 14600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:12,828][25130] Num frames 14700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:12,898][25130] Num frames 14800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:12,968][25130] Num frames 14900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:13,039][25130] Num frames 15000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:13,109][25130] Num frames 15100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:13,181][25130] Num frames 15200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:13,251][25130] Num frames 15300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:13,321][25130] Num frames 15400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:13,393][25130] Num frames 15500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:13,465][25130] Num frames 15600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:13,536][25130] Num frames 15700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:13,608][25130] Num frames 15800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:13,680][25130] Num frames 15900...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:12:13,748][25130] Avg episode rewards: #0: 55.023, true rewards: #0: 19.899\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:12:13,749][25130] Avg episode reward: 55.023, avg true_objective: 19.899\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:13,809][25130] Num frames 16000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:13,879][25130] Num frames 16100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:13,949][25130] Num frames 16200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:14,021][25130] Num frames 16300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:14,091][25130] Num frames 16400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:14,160][25130] Num frames 16500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:14,230][25130] Num frames 16600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:14,302][25130] Num frames 16700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:14,371][25130] Num frames 16800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:14,444][25130] Num frames 16900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:14,514][25130] Num frames 17000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:14,584][25130] Num frames 17100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:14,655][25130] Num frames 17200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:14,727][25130] Num frames 17300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:14,797][25130] Num frames 17400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:14,869][25130] Num frames 17500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:14,940][25130] Num frames 17600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:15,020][25130] Num frames 17700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:15,091][25130] Num frames 17800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:15,159][25130] Num frames 17900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:15,228][25130] Num frames 18000...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:12:15,296][25130] Avg episode rewards: #0: 55.353, true rewards: #0: 20.021\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:12:15,298][25130] Avg episode reward: 55.353, avg true_objective: 20.021\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:15,355][25130] Num frames 18100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:15,424][25130] Num frames 18200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:15,492][25130] Num frames 18300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:15,562][25130] Num frames 18400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:15,630][25130] Num frames 18500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:15,698][25130] Num frames 18600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:15,768][25130] Num frames 18700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:15,837][25130] Num frames 18800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:15,906][25130] Num frames 18900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:15,977][25130] Num frames 19000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:16,045][25130] Num frames 19100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:16,114][25130] Num frames 19200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:16,183][25130] Num frames 19300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:16,253][25130] Num frames 19400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:16,323][25130] Num frames 19500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:16,393][25130] Num frames 19600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:16,463][25130] Num frames 19700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:16,533][25130] Num frames 19800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:16,604][25130] Num frames 19900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:16,673][25130] Num frames 20000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 21:12:16,743][25130] Num frames 20100...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:12:16,810][25130] Avg episode rewards: #0: 56.518, true rewards: #0: 20.119\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 21:12:16,811][25130] Avg episode reward: 56.518, avg true_objective: 20.119\u001b[0m\n",
      "ffmpeg version 9c33b2f Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 9.3.0 (crosstool-NG 1.24.0.133_b0863d8_dirty)\n",
      "  configuration: --prefix=/home/conda/feedstock_root/build_artifacts/ffmpeg_1627813612080/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1627813612080/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-gnutls --enable-gpl --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-libx264 --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1627813612080/_build_env/bin/pkg-config\n",
      "  libavutil      56. 51.100 / 56. 51.100\n",
      "  libavcodec     58. 91.100 / 58. 91.100\n",
      "  libavformat    58. 45.100 / 58. 45.100\n",
      "  libavdevice    58. 10.100 / 58. 10.100\n",
      "  libavfilter     7. 85.100 /  7. 85.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  7.100 /  5.  7.100\n",
      "  libswresample   3.  7.100 /  3.  7.100\n",
      "  libpostproc    55.  7.100 / 55.  7.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/tmp/sf2_raghu/replay.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf59.27.100\n",
      "  Duration: 00:09:35.14, start: 0.000000, bitrate: 1834 kb/s\n",
      "    Stream #0:0(und): Video: mpeg4 (Simple Profile) (mp4v / 0x7634706D), yuv420p, 240x180 [SAR 1:1 DAR 4:3], 1833 kb/s, 35 fps, 35 tbr, 17920 tbn, 35 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mpeg4 (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x6241cc648d80] using SAR=1/1\n",
      "[libx264 @ 0x6241cc648d80] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
      "[libx264 @ 0x6241cc648d80] profile High, level 1.3, 4:2:0, 8-bit\n",
      "[libx264 @ 0x6241cc648d80] 264 - core 161 r3030M 8bd6d28 - H.264/MPEG-4 AVC codec - Copyleft 2003-2020 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to '/home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/replay.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf58.45.100\n",
      "    Stream #0:0(und): Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 240x180 [SAR 1:1 DAR 4:3], q=-1--1, 35 fps, 17920 tbn, 35 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      encoder         : Lavc58.91.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "frame=20130 fps=1378 q=-1.0 Lsize=   39080kB time=00:09:35.05 bitrate= 556.7kbits/s speed=39.4x    \n",
      "video:38859kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.568940%\n",
      "[libx264 @ 0x6241cc648d80] frame I:192   Avg QP:23.79  size:  5002\n",
      "[libx264 @ 0x6241cc648d80] frame P:7485  Avg QP:26.75  size:  2459\n",
      "[libx264 @ 0x6241cc648d80] frame B:12453 Avg QP:28.32  size:  1640\n",
      "[libx264 @ 0x6241cc648d80] consecutive B-frames: 14.6%  6.6%  6.0% 72.7%\n",
      "[libx264 @ 0x6241cc648d80] mb I  I16..4: 13.5% 73.1% 13.4%\n",
      "[libx264 @ 0x6241cc648d80] mb P  I16..4:  3.5% 18.0%  5.4%  P16..4: 34.0% 23.8% 10.4%  0.0%  0.0%    skip: 4.9%\n",
      "[libx264 @ 0x6241cc648d80] mb B  I16..4:  0.2%  5.1%  2.8%  B16..8: 38.2% 20.6%  6.6%  direct: 7.8%  skip:18.6%  L0:51.0% L1:34.8% BI:14.3%\n",
      "[libx264 @ 0x6241cc648d80] 8x8 transform intra:66.1% inter:62.7%\n",
      "[libx264 @ 0x6241cc648d80] coded y,uvDC,uvAC intra: 68.4% 70.5% 36.3% inter: 45.7% 17.8% 4.2%\n",
      "[libx264 @ 0x6241cc648d80] i16 v,h,dc,p: 65%  4% 30%  1%\n",
      "[libx264 @ 0x6241cc648d80] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 21% 10% 32%  5%  6%  5%  8%  5%  8%\n",
      "[libx264 @ 0x6241cc648d80] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 59%  9% 10%  3%  5%  3%  5%  3%  4%\n",
      "[libx264 @ 0x6241cc648d80] i8c dc,h,v,p: 55% 20% 22%  3%\n",
      "[libx264 @ 0x6241cc648d80] Weighted P-Frames: Y:6.4% UV:0.6%\n",
      "[libx264 @ 0x6241cc648d80] ref P L0: 60.6% 13.6% 16.1%  9.0%  0.6%\n",
      "[libx264 @ 0x6241cc648d80] ref B L0: 88.8%  8.4%  2.7%\n",
      "[libx264 @ 0x6241cc648d80] ref B L1: 96.0%  4.0%\n",
      "[libx264 @ 0x6241cc648d80] kb/s:553.48\n",
      "\u001b[36m[2024-07-05 21:12:37,637][25130] Replay video saved to /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/replay.mp4!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sample_factory.enjoy import enjoy\n",
    "\n",
    "cfg = parse_vizdoom_cfg(\n",
    "    argv=[f\"--env={env}\", \n",
    "          \"--experiment=conv_resnet\",\n",
    "          \"--num_workers=1\", \n",
    "          \"--save_video\", \n",
    "          \"--no_render\", \n",
    "          \"--max_num_episodes=10\"], evaluation=True\n",
    ")\n",
    "status = enjoy(cfg)\n",
    "\n",
    "# 10000000 - 13.522\n",
    "# 20000000 - 11.983\n",
    "# 70000000 - 20.935\n",
    "# 100000000 - 20.119\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the performance of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "\n",
    "mp4 = open(\"/content/train_dir/default_experiment/replay.mp4\", \"rb\").read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\n",
    "    \"\"\"\n",
    "<video width=640 controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\"\n",
    "    % data_url\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload results to HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc732098c1434130b524a040cd18eb22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2024-07-05 17:23:13,711][04005] Loading existing experiment configuration from /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/config.json\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:13,712][04005] Overriding arg 'num_workers' with value 1 passed from command line\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:13,712][04005] Adding new argument 'no_render'=True that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:13,713][04005] Adding new argument 'save_video'=True that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:13,713][04005] Adding new argument 'video_frames'=1000000000.0 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:13,714][04005] Adding new argument 'video_name'=None that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:13,714][04005] Adding new argument 'max_num_frames'=100000 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:13,715][04005] Adding new argument 'max_num_episodes'=10 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:13,715][04005] Adding new argument 'push_to_hub'=True that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:13,716][04005] Adding new argument 'hf_repository'='ra9hu/rl_course_vizdoom_health_gathering_supreme' that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:13,716][04005] Adding new argument 'policy_index'=0 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:13,717][04005] Adding new argument 'eval_deterministic'=False that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:13,717][04005] Adding new argument 'train_script'=None that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:13,718][04005] Adding new argument 'enjoy_script'=None that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:13,718][04005] Using frameskip 1 and render_action_repeat=4 for evaluation\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-05 17:23:13,735][04005] RunningMeanStd input shape: (3, 72, 128)\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:13,736][04005] RunningMeanStd input shape: (1,)\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:13,744][04005] Num input channels: 3\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:13,750][04005] Convolutional layer output size: 4608\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:13,762][04005] Policy head output size: 512\u001b[0m\n",
      "\u001b[33m[2024-07-05 17:23:13,829][04005] Loading state from checkpoint /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/checkpoint_p0/checkpoint_000017091_70004736.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:14,511][04005] Num frames 100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:14,603][04005] Num frames 200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:14,696][04005] Num frames 300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:14,787][04005] Num frames 400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:14,884][04005] Num frames 500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:14,978][04005] Num frames 600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:15,065][04005] Num frames 700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:15,156][04005] Num frames 800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:15,233][04005] Num frames 900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:15,310][04005] Num frames 1000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:15,387][04005] Num frames 1100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:15,458][04005] Num frames 1200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:15,531][04005] Num frames 1300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:15,604][04005] Num frames 1400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:15,683][04005] Num frames 1500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:15,755][04005] Num frames 1600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:15,834][04005] Num frames 1700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:15,910][04005] Num frames 1800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:15,983][04005] Num frames 1900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:16,059][04005] Num frames 2000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:16,138][04005] Num frames 2100...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 17:23:16,189][04005] Avg episode rewards: #0: 58.999, true rewards: #0: 21.000\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 17:23:16,191][04005] Avg episode reward: 58.999, avg true_objective: 21.000\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:16,266][04005] Num frames 2200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:16,340][04005] Num frames 2300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:16,414][04005] Num frames 2400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:16,486][04005] Num frames 2500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:16,558][04005] Num frames 2600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:16,635][04005] Num frames 2700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:16,711][04005] Num frames 2800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:16,786][04005] Num frames 2900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:16,867][04005] Num frames 3000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:16,942][04005] Num frames 3100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:17,016][04005] Num frames 3200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:17,087][04005] Num frames 3300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:17,161][04005] Num frames 3400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:17,234][04005] Num frames 3500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:17,309][04005] Num frames 3600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:17,383][04005] Num frames 3700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:17,457][04005] Num frames 3800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:17,530][04005] Num frames 3900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:17,604][04005] Num frames 4000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:17,677][04005] Num frames 4100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:17,753][04005] Num frames 4200...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 17:23:17,805][04005] Avg episode rewards: #0: 62.499, true rewards: #0: 21.000\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 17:23:17,806][04005] Avg episode reward: 62.499, avg true_objective: 21.000\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:17,882][04005] Num frames 4300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:17,954][04005] Num frames 4400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:18,028][04005] Num frames 4500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:18,101][04005] Num frames 4600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:18,175][04005] Num frames 4700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:18,248][04005] Num frames 4800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:18,322][04005] Num frames 4900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:18,396][04005] Num frames 5000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:18,469][04005] Num frames 5100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:18,542][04005] Num frames 5200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:18,616][04005] Num frames 5300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:18,697][04005] Num frames 5400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:18,771][04005] Num frames 5500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:18,842][04005] Num frames 5600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:18,914][04005] Num frames 5700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:18,986][04005] Num frames 5800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:19,060][04005] Num frames 5900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:19,132][04005] Num frames 6000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:19,203][04005] Num frames 6100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:19,273][04005] Num frames 6200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:19,347][04005] Num frames 6300...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 17:23:19,398][04005] Avg episode rewards: #0: 59.999, true rewards: #0: 21.000\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 17:23:19,399][04005] Avg episode reward: 59.999, avg true_objective: 21.000\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:19,472][04005] Num frames 6400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:19,542][04005] Num frames 6500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:19,616][04005] Num frames 6600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:19,686][04005] Num frames 6700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:19,757][04005] Num frames 6800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:19,829][04005] Num frames 6900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:19,901][04005] Num frames 7000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:19,972][04005] Num frames 7100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:20,043][04005] Num frames 7200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:20,113][04005] Num frames 7300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:20,183][04005] Num frames 7400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:20,253][04005] Num frames 7500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:20,323][04005] Num frames 7600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:20,392][04005] Num frames 7700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:20,463][04005] Num frames 7800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:20,536][04005] Num frames 7900...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 17:23:20,613][04005] Avg episode rewards: #0: 56.829, true rewards: #0: 19.830\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 17:23:20,615][04005] Avg episode reward: 56.829, avg true_objective: 19.830\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:20,663][04005] Num frames 8000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:20,735][04005] Num frames 8100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:20,812][04005] Num frames 8200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:20,886][04005] Num frames 8300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:20,964][04005] Num frames 8400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:21,038][04005] Num frames 8500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:21,114][04005] Num frames 8600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:21,186][04005] Num frames 8700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:21,266][04005] Num frames 8800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:21,338][04005] Num frames 8900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:21,412][04005] Num frames 9000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:21,485][04005] Num frames 9100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:21,557][04005] Num frames 9200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:21,629][04005] Num frames 9300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:21,702][04005] Num frames 9400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:21,778][04005] Num frames 9500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:21,849][04005] Num frames 9600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:21,922][04005] Num frames 9700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:21,994][04005] Num frames 9800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:22,069][04005] Num frames 9900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:22,142][04005] Num frames 10000...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 17:23:22,219][04005] Avg episode rewards: #0: 55.863, true rewards: #0: 20.064\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 17:23:22,220][04005] Avg episode reward: 55.863, avg true_objective: 20.064\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:22,272][04005] Num frames 10100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:22,345][04005] Num frames 10200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:22,418][04005] Num frames 10300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:22,496][04005] Num frames 10400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:22,569][04005] Num frames 10500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:22,640][04005] Num frames 10600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:22,711][04005] Num frames 10700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:22,785][04005] Num frames 10800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:22,859][04005] Num frames 10900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:22,931][04005] Num frames 11000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:23,002][04005] Num frames 11100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:23,074][04005] Num frames 11200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:23,144][04005] Num frames 11300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:23,216][04005] Num frames 11400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:23,291][04005] Num frames 11500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:23,364][04005] Num frames 11600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:23,439][04005] Num frames 11700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:23,509][04005] Num frames 11800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:23,581][04005] Num frames 11900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:23,658][04005] Num frames 12000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:23,735][04005] Num frames 12100...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 17:23:23,812][04005] Avg episode rewards: #0: 56.719, true rewards: #0: 20.220\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 17:23:23,814][04005] Avg episode reward: 56.719, avg true_objective: 20.220\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:23,863][04005] Num frames 12200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:23,933][04005] Num frames 12300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:24,006][04005] Num frames 12400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:24,079][04005] Num frames 12500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:24,150][04005] Num frames 12600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:24,219][04005] Num frames 12700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:24,289][04005] Num frames 12800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:24,361][04005] Num frames 12900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:24,431][04005] Num frames 13000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:24,501][04005] Num frames 13100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:24,574][04005] Num frames 13200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:24,642][04005] Num frames 13300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:24,714][04005] Num frames 13400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:24,786][04005] Num frames 13500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:24,859][04005] Num frames 13600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:24,932][04005] Num frames 13700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:25,006][04005] Num frames 13800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:25,081][04005] Num frames 13900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:25,156][04005] Num frames 14000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:25,227][04005] Num frames 14100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:25,301][04005] Num frames 14200...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 17:23:25,380][04005] Avg episode rewards: #0: 57.330, true rewards: #0: 20.331\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 17:23:25,381][04005] Avg episode reward: 57.330, avg true_objective: 20.331\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:25,430][04005] Num frames 14300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:25,502][04005] Num frames 14400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:25,574][04005] Num frames 14500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:25,646][04005] Num frames 14600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:25,716][04005] Num frames 14700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:25,802][04005] Num frames 14800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:25,876][04005] Num frames 14900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:25,950][04005] Num frames 15000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:26,022][04005] Num frames 15100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:26,096][04005] Num frames 15200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:26,171][04005] Num frames 15300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:26,243][04005] Num frames 15400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:26,315][04005] Num frames 15500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:26,389][04005] Num frames 15600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:26,464][04005] Num frames 15700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:26,538][04005] Num frames 15800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:26,614][04005] Num frames 15900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:26,691][04005] Num frames 16000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:26,793][04005] Num frames 16100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:26,897][04005] Num frames 16200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:26,972][04005] Num frames 16300...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 17:23:27,050][04005] Avg episode rewards: #0: 57.789, true rewards: #0: 20.415\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 17:23:27,051][04005] Avg episode reward: 57.789, avg true_objective: 20.415\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:27,107][04005] Num frames 16400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:27,189][04005] Num frames 16500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:27,260][04005] Num frames 16600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:27,335][04005] Num frames 16700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:27,412][04005] Num frames 16800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:27,486][04005] Num frames 16900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:27,559][04005] Num frames 17000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:27,632][04005] Num frames 17100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:27,704][04005] Num frames 17200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:27,776][04005] Num frames 17300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:27,846][04005] Num frames 17400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:27,919][04005] Num frames 17500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:27,994][04005] Num frames 17600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:28,068][04005] Num frames 17700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:28,141][04005] Num frames 17800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:28,213][04005] Num frames 17900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:28,287][04005] Num frames 18000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:28,361][04005] Num frames 18100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:28,432][04005] Num frames 18200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:28,506][04005] Num frames 18300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:28,582][04005] Num frames 18400...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 17:23:28,660][04005] Avg episode rewards: #0: 57.590, true rewards: #0: 20.480\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 17:23:28,661][04005] Avg episode reward: 57.590, avg true_objective: 20.480\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:28,711][04005] Num frames 18500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:28,783][04005] Num frames 18600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:28,854][04005] Num frames 18700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:28,928][04005] Num frames 18800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:29,002][04005] Num frames 18900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:29,077][04005] Num frames 19000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:29,148][04005] Num frames 19100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:29,229][04005] Num frames 19200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:29,303][04005] Num frames 19300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:29,373][04005] Num frames 19400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:29,445][04005] Num frames 19500...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:29,516][04005] Num frames 19600...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:29,586][04005] Num frames 19700...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:29,657][04005] Num frames 19800...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:29,728][04005] Num frames 19900...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:29,800][04005] Num frames 20000...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:29,872][04005] Num frames 20100...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:29,942][04005] Num frames 20200...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:30,012][04005] Num frames 20300...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:30,089][04005] Num frames 20400...\u001b[0m\n",
      "\u001b[36m[2024-07-05 17:23:30,162][04005] Num frames 20500...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 17:23:30,244][04005] Avg episode rewards: #0: 57.831, true rewards: #0: 20.532\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-05 17:23:30,245][04005] Avg episode reward: 57.831, avg true_objective: 20.532\u001b[0m\n",
      "ffmpeg version 9c33b2f Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 9.3.0 (crosstool-NG 1.24.0.133_b0863d8_dirty)\n",
      "  configuration: --prefix=/home/conda/feedstock_root/build_artifacts/ffmpeg_1627813612080/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1627813612080/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-gnutls --enable-gpl --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-libx264 --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1627813612080/_build_env/bin/pkg-config\n",
      "  libavutil      56. 51.100 / 56. 51.100\n",
      "  libavcodec     58. 91.100 / 58. 91.100\n",
      "  libavformat    58. 45.100 / 58. 45.100\n",
      "  libavdevice    58. 10.100 / 58. 10.100\n",
      "  libavfilter     7. 85.100 /  7. 85.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  7.100 /  5.  7.100\n",
      "  libswresample   3.  7.100 /  3.  7.100\n",
      "  libpostproc    55.  7.100 / 55.  7.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/tmp/sf2_raghu/replay.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf59.27.100\n",
      "  Duration: 00:09:46.92, start: 0.000000, bitrate: 1855 kb/s\n",
      "    Stream #0:0(und): Video: mpeg4 (Simple Profile) (mp4v / 0x7634706D), yuv420p, 240x180 [SAR 1:1 DAR 4:3], 1854 kb/s, 35 fps, 35 tbr, 17920 tbn, 35 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mpeg4 (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x64e73661df40] using SAR=1/1\n",
      "[libx264 @ 0x64e73661df40] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
      "[libx264 @ 0x64e73661df40] profile High, level 1.3, 4:2:0, 8-bit\n",
      "[libx264 @ 0x64e73661df40] 264 - core 161 r3030M 8bd6d28 - H.264/MPEG-4 AVC codec - Copyleft 2003-2020 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to '/home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/replay.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf58.45.100\n",
      "    Stream #0:0(und): Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 240x180 [SAR 1:1 DAR 4:3], q=-1--1, 35 fps, 17920 tbn, 35 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      encoder         : Lavc58.91.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "frame=20542 fps=1391 q=-1.0 Lsize=   40289kB time=00:09:46.82 bitrate= 562.4kbits/s speed=39.7x    \n",
      "video:40062kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.566000%\n",
      "[libx264 @ 0x64e73661df40] frame I:188   Avg QP:23.77  size:  5043\n",
      "[libx264 @ 0x64e73661df40] frame P:7504  Avg QP:26.77  size:  2506\n",
      "[libx264 @ 0x64e73661df40] frame B:12850 Avg QP:28.33  size:  1655\n",
      "[libx264 @ 0x64e73661df40] consecutive B-frames: 13.9%  6.1%  5.6% 74.4%\n",
      "[libx264 @ 0x64e73661df40] mb I  I16..4: 14.3% 73.0% 12.6%\n",
      "[libx264 @ 0x64e73661df40] mb P  I16..4:  3.6% 18.5%  5.6%  P16..4: 33.4% 23.8% 10.3%  0.0%  0.0%    skip: 4.8%\n",
      "[libx264 @ 0x64e73661df40] mb B  I16..4:  0.2%  5.2%  2.9%  B16..8: 37.7% 20.7%  6.7%  direct: 7.9%  skip:18.6%  L0:50.9% L1:34.6% BI:14.5%\n",
      "[libx264 @ 0x64e73661df40] 8x8 transform intra:65.9% inter:62.3%\n",
      "[libx264 @ 0x64e73661df40] coded y,uvDC,uvAC intra: 68.8% 70.3% 35.9% inter: 45.8% 18.0% 4.3%\n",
      "[libx264 @ 0x64e73661df40] i16 v,h,dc,p: 65%  4% 30%  1%\n",
      "[libx264 @ 0x64e73661df40] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 21% 10% 32%  5%  6%  5%  8%  5%  8%\n",
      "[libx264 @ 0x64e73661df40] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 59%  9% 10%  3%  5%  3%  6%  3%  4%\n",
      "[libx264 @ 0x64e73661df40] i8c dc,h,v,p: 55% 20% 22%  3%\n",
      "[libx264 @ 0x64e73661df40] Weighted P-Frames: Y:6.4% UV:0.5%\n",
      "[libx264 @ 0x64e73661df40] ref P L0: 59.8% 13.4% 16.6%  9.5%  0.6%\n",
      "[libx264 @ 0x64e73661df40] ref B L0: 88.7%  8.5%  2.8%\n",
      "[libx264 @ 0x64e73661df40] ref B L1: 95.9%  4.1%\n",
      "[libx264 @ 0x64e73661df40] kb/s:559.16\n",
      "\u001b[36m[2024-07-05 17:23:51,410][04005] Replay video saved to /home/raghu/DL/topics/RL/unit8B-AsyncPPO-SampleFactory/train_dir/conv_resnet/replay.mp4!\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e76789507c44b709b3c8eb873559dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1720155218.Raghu-Laptop:   0%|          | 0.00/6.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00552fce832a4f52837dc3f9e117c7c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1720155536.Raghu-Laptop:   0%|          | 0.00/2.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b27452c3cc4f55a2488f29436511ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1720156453.Raghu-Laptop:   0%|          | 0.00/511k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79fba3094fcb47c2b9f5e43fa16c84cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1720155624.Raghu-Laptop:   0%|          | 0.00/390k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa6abbce20044d08d099d282db79344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1720175313.Raghu-Laptop:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f816b44560f74996a4d77507d21fc8b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 9 LFS files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e9778a4c28f4468b8fb84384e0d672d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "best_000015017_61509632_reward_55.424.pth:   0%|          | 0.00/48.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec603ce3e4542ba8ba81d391236aa93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "checkpoint_000016993_69603328.pth:   0%|          | 0.00/48.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bebb34af91a24626aa85a3ea3e2a506a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "checkpoint_000017091_70004736.pth:   0%|          | 0.00/48.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c87524f2aeb4259919d5748e6303fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "replay.mp4:   0%|          | 0.00/41.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1m[2024-07-05 17:24:27,169][04005] The model has been pushed to https://huggingface.co/ra9hu/rl_course_vizdoom_health_gathering_supreme\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sample_factory.enjoy import enjoy\n",
    "\n",
    "hf_username = \"ra9hu\"  # insert your HuggingFace username here\n",
    "\n",
    "cfg = parse_vizdoom_cfg(\n",
    "    argv=[\n",
    "        f\"--env={env}\",\n",
    "        \"--experiment=conv_resnet\",\n",
    "        \"--num_workers=1\",\n",
    "        \"--save_video\",\n",
    "        \"--no_render\",\n",
    "        \"--max_num_episodes=10\",\n",
    "        \"--max_num_frames=100000\",\n",
    "        \"--push_to_hub\",\n",
    "        f\"--hf_repository={hf_username}/rl_course_vizdoom_health_gathering_supreme\",\n",
    "    ],\n",
    "    evaluation=True,\n",
    ")\n",
    "status = enjoy(cfg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download sample run from HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sample_factory.enjoy import enjoy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[2024-07-04 19:45:47,107][19305] register_encoder_factory: <function make_vizdoom_encoder at 0x76a196e72560>\u001b[0m\n",
      "\u001b[33m[2024-07-04 19:45:47,115][19305] Loading existing experiment configuration from train_dir/doom_health_gathering_supreme_2222/config.json\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:47,116][19305] Overriding arg 'experiment' with value 'doom_health_gathering_supreme_2222' passed from command line\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:47,117][19305] Overriding arg 'train_dir' with value 'train_dir' passed from command line\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:47,117][19305] Overriding arg 'num_workers' with value 1 passed from command line\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:47,118][19305] Adding new argument 'lr_adaptive_min'=1e-06 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:47,118][19305] Adding new argument 'lr_adaptive_max'=0.01 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:47,118][19305] Adding new argument 'env_gpu_observations'=True that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:47,119][19305] Adding new argument 'no_render'=True that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:47,119][19305] Adding new argument 'save_video'=True that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:47,119][19305] Adding new argument 'video_frames'=1000000000.0 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:47,120][19305] Adding new argument 'video_name'=None that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:47,120][19305] Adding new argument 'max_num_frames'=1000000000.0 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:47,121][19305] Adding new argument 'max_num_episodes'=10 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:47,121][19305] Adding new argument 'push_to_hub'=False that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:47,121][19305] Adding new argument 'hf_repository'=None that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:47,122][19305] Adding new argument 'policy_index'=0 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:47,122][19305] Adding new argument 'eval_deterministic'=False that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:47,123][19305] Adding new argument 'train_script'=None that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:47,123][19305] Adding new argument 'enjoy_script'=None that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:47,124][19305] Using frameskip 1 and render_action_repeat=4 for evaluation\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:47,150][19305] Doom resolution: 160x120, resize resolution: (128, 72)\u001b[0m\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/raghu/anaconda3/envs/rl/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[36m[2024-07-04 19:45:47,152][19305] RunningMeanStd input shape: (3, 72, 128)\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:47,160][19305] RunningMeanStd input shape: (1,)\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:47,173][19305] ConvEncoder: input_channels=3\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:47,287][19305] Conv encoder output size: 512\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:47,288][19305] Policy head output size: 512\u001b[0m\n",
      "\u001b[33m[2024-07-04 19:45:49,226][19305] Loading state from checkpoint train_dir/doom_health_gathering_supreme_2222/checkpoint_p0/checkpoint_000539850_4422451200.pth...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:50,394][19305] Num frames 100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:50,473][19305] Num frames 200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:50,559][19305] Num frames 300...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:50,644][19305] Num frames 400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:50,729][19305] Num frames 500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:50,813][19305] Num frames 600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:50,896][19305] Num frames 700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:50,981][19305] Num frames 800...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:51,060][19305] Num frames 900...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:51,141][19305] Num frames 1000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:51,217][19305] Num frames 1100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:51,297][19305] Num frames 1200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:51,374][19305] Num frames 1300...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:51,452][19305] Num frames 1400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:51,530][19305] Num frames 1500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:51,609][19305] Num frames 1600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:51,686][19305] Num frames 1700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:51,764][19305] Num frames 1800...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:51,839][19305] Num frames 1900...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:51,916][19305] Num frames 2000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:51,994][19305] Num frames 2100...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 19:45:52,046][19305] Avg episode rewards: #0: 65.999, true rewards: #0: 21.000\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 19:45:52,047][19305] Avg episode reward: 65.999, avg true_objective: 21.000\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:52,126][19305] Num frames 2200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:52,204][19305] Num frames 2300...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:52,281][19305] Num frames 2400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:52,358][19305] Num frames 2500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:52,437][19305] Num frames 2600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:52,515][19305] Num frames 2700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:52,597][19305] Num frames 2800...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:52,677][19305] Num frames 2900...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:52,759][19305] Num frames 3000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:52,842][19305] Num frames 3100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:52,920][19305] Num frames 3200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:52,996][19305] Num frames 3300...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:53,075][19305] Num frames 3400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:53,153][19305] Num frames 3500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:53,232][19305] Num frames 3600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:53,312][19305] Num frames 3700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:53,385][19305] Num frames 3800...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:53,459][19305] Num frames 3900...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:53,538][19305] Num frames 4000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:53,617][19305] Num frames 4100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:53,698][19305] Num frames 4200...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 19:45:53,750][19305] Avg episode rewards: #0: 64.999, true rewards: #0: 21.000\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 19:45:53,751][19305] Avg episode reward: 64.999, avg true_objective: 21.000\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:53,840][19305] Num frames 4300...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:53,916][19305] Num frames 4400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:53,990][19305] Num frames 4500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:54,068][19305] Num frames 4600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:54,145][19305] Num frames 4700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:54,221][19305] Num frames 4800...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:54,297][19305] Num frames 4900...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:54,376][19305] Num frames 5000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:54,451][19305] Num frames 5100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:54,539][19305] Num frames 5200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:54,617][19305] Num frames 5300...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:54,692][19305] Num frames 5400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:54,767][19305] Num frames 5500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:54,844][19305] Num frames 5600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:54,920][19305] Num frames 5700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:54,996][19305] Num frames 5800...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:55,074][19305] Num frames 5900...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:55,151][19305] Num frames 6000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:55,230][19305] Num frames 6100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:55,308][19305] Num frames 6200...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 19:45:55,373][19305] Avg episode rewards: #0: 64.052, true rewards: #0: 20.720\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 19:45:55,374][19305] Avg episode reward: 64.052, avg true_objective: 20.720\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:55,441][19305] Num frames 6300...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:55,518][19305] Num frames 6400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:55,595][19305] Num frames 6500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:55,671][19305] Num frames 6600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:55,750][19305] Num frames 6700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:55,827][19305] Num frames 6800...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:55,904][19305] Num frames 6900...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:55,982][19305] Num frames 7000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:56,058][19305] Num frames 7100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:56,135][19305] Num frames 7200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:56,213][19305] Num frames 7300...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:56,289][19305] Num frames 7400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:56,365][19305] Num frames 7500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:56,442][19305] Num frames 7600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:56,519][19305] Num frames 7700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:56,596][19305] Num frames 7800...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:56,673][19305] Num frames 7900...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:56,751][19305] Num frames 8000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:56,829][19305] Num frames 8100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:56,906][19305] Num frames 8200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:56,985][19305] Num frames 8300...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 19:45:57,051][19305] Avg episode rewards: #0: 64.289, true rewards: #0: 20.790\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 19:45:57,053][19305] Avg episode reward: 64.289, avg true_objective: 20.790\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:57,117][19305] Num frames 8400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:57,195][19305] Num frames 8500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:57,271][19305] Num frames 8600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:57,347][19305] Num frames 8700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:57,423][19305] Num frames 8800...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:57,498][19305] Num frames 8900...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:57,573][19305] Num frames 9000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:57,652][19305] Num frames 9100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:57,729][19305] Num frames 9200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:57,806][19305] Num frames 9300...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:57,883][19305] Num frames 9400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:57,959][19305] Num frames 9500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:58,035][19305] Num frames 9600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:58,113][19305] Num frames 9700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:58,189][19305] Num frames 9800...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:58,268][19305] Num frames 9900...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:58,347][19305] Num frames 10000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:58,424][19305] Num frames 10100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:58,501][19305] Num frames 10200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:58,580][19305] Num frames 10300...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:58,658][19305] Num frames 10400...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 19:45:58,726][19305] Avg episode rewards: #0: 64.631, true rewards: #0: 20.832\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 19:45:58,727][19305] Avg episode reward: 64.631, avg true_objective: 20.832\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:58,791][19305] Num frames 10500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:58,865][19305] Num frames 10600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:58,953][19305] Num frames 10700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:59,032][19305] Num frames 10800...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:59,107][19305] Num frames 10900...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:59,183][19305] Num frames 11000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:59,260][19305] Num frames 11100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:59,337][19305] Num frames 11200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:59,413][19305] Num frames 11300...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:59,489][19305] Num frames 11400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:59,564][19305] Num frames 11500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:59,642][19305] Num frames 11600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:59,720][19305] Num frames 11700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:59,797][19305] Num frames 11800...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:59,876][19305] Num frames 11900...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:45:59,956][19305] Num frames 12000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:00,035][19305] Num frames 12100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:00,114][19305] Num frames 12200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:00,193][19305] Num frames 12300...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:00,271][19305] Num frames 12400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:00,349][19305] Num frames 12500...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 19:46:00,415][19305] Avg episode rewards: #0: 62.859, true rewards: #0: 20.860\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 19:46:00,417][19305] Avg episode reward: 62.859, avg true_objective: 20.860\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:00,484][19305] Num frames 12600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:00,561][19305] Num frames 12700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:00,640][19305] Num frames 12800...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:00,719][19305] Num frames 12900...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:00,803][19305] Num frames 13000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:00,905][19305] Num frames 13100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:00,983][19305] Num frames 13200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:01,060][19305] Num frames 13300...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:01,138][19305] Num frames 13400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:01,216][19305] Num frames 13500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:01,296][19305] Num frames 13600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:01,376][19305] Num frames 13700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:01,457][19305] Num frames 13800...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:01,534][19305] Num frames 13900...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:01,608][19305] Num frames 14000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:01,684][19305] Num frames 14100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:01,760][19305] Num frames 14200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:01,836][19305] Num frames 14300...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:01,912][19305] Num frames 14400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:01,990][19305] Num frames 14500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:02,067][19305] Num frames 14600...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 19:46:02,133][19305] Avg episode rewards: #0: 62.879, true rewards: #0: 20.880\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 19:46:02,134][19305] Avg episode reward: 62.879, avg true_objective: 20.880\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:02,199][19305] Num frames 14700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:02,274][19305] Num frames 14800...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:02,350][19305] Num frames 14900...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:02,429][19305] Num frames 15000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:02,508][19305] Num frames 15100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:02,591][19305] Num frames 15200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:02,674][19305] Num frames 15300...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:02,754][19305] Num frames 15400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:02,838][19305] Num frames 15500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:02,921][19305] Num frames 15600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:03,002][19305] Num frames 15700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:03,081][19305] Num frames 15800...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:03,158][19305] Num frames 15900...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:03,240][19305] Num frames 16000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:03,320][19305] Num frames 16100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:03,409][19305] Num frames 16200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:03,493][19305] Num frames 16300...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:03,574][19305] Num frames 16400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:03,651][19305] Num frames 16500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:03,728][19305] Num frames 16600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:03,805][19305] Num frames 16700...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 19:46:03,870][19305] Avg episode rewards: #0: 63.394, true rewards: #0: 20.895\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 19:46:03,872][19305] Avg episode reward: 63.394, avg true_objective: 20.895\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:03,937][19305] Num frames 16800...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:04,013][19305] Num frames 16900...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:04,089][19305] Num frames 17000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:04,166][19305] Num frames 17100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:04,243][19305] Num frames 17200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:04,320][19305] Num frames 17300...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:04,398][19305] Num frames 17400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:04,475][19305] Num frames 17500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:04,555][19305] Num frames 17600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:04,632][19305] Num frames 17700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:04,707][19305] Num frames 17800...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:04,785][19305] Num frames 17900...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:04,862][19305] Num frames 18000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:04,940][19305] Num frames 18100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:05,019][19305] Num frames 18200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:05,098][19305] Num frames 18300...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:05,177][19305] Num frames 18400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:05,254][19305] Num frames 18500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:05,332][19305] Num frames 18600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:05,411][19305] Num frames 18700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:05,490][19305] Num frames 18800...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 19:46:05,556][19305] Avg episode rewards: #0: 63.128, true rewards: #0: 20.907\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 19:46:05,558][19305] Avg episode reward: 63.128, avg true_objective: 20.907\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:05,623][19305] Num frames 18900...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:05,700][19305] Num frames 19000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:05,779][19305] Num frames 19100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:05,858][19305] Num frames 19200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:05,939][19305] Num frames 19300...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:06,017][19305] Num frames 19400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:06,095][19305] Num frames 19500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:06,173][19305] Num frames 19600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:06,250][19305] Num frames 19700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:06,327][19305] Num frames 19800...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:06,406][19305] Num frames 19900...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:06,484][19305] Num frames 20000...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:06,562][19305] Num frames 20100...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:06,642][19305] Num frames 20200...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:06,720][19305] Num frames 20300...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:06,796][19305] Num frames 20400...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:06,875][19305] Num frames 20500...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:06,953][19305] Num frames 20600...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:07,029][19305] Num frames 20700...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:07,108][19305] Num frames 20800...\u001b[0m\n",
      "\u001b[36m[2024-07-04 19:46:07,187][19305] Num frames 20900...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 19:46:07,254][19305] Avg episode rewards: #0: 63.315, true rewards: #0: 20.916\u001b[0m\n",
      "\u001b[37m\u001b[1m[2024-07-04 19:46:07,254][19305] Avg episode reward: 63.315, avg true_objective: 20.916\u001b[0m\n",
      "ffmpeg version 9c33b2f Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 9.3.0 (crosstool-NG 1.24.0.133_b0863d8_dirty)\n",
      "  configuration: --prefix=/home/conda/feedstock_root/build_artifacts/ffmpeg_1627813612080/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1627813612080/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-gnutls --enable-gpl --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-libx264 --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1627813612080/_build_env/bin/pkg-config\n",
      "  libavutil      56. 51.100 / 56. 51.100\n",
      "  libavcodec     58. 91.100 / 58. 91.100\n",
      "  libavformat    58. 45.100 / 58. 45.100\n",
      "  libavdevice    58. 10.100 / 58. 10.100\n",
      "  libavfilter     7. 85.100 /  7. 85.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  7.100 /  5.  7.100\n",
      "  libswresample   3.  7.100 /  3.  7.100\n",
      "  libpostproc    55.  7.100 / 55.  7.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/tmp/sf2_raghu/replay.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf59.27.100\n",
      "  Duration: 00:09:57.89, start: 0.000000, bitrate: 1860 kb/s\n",
      "    Stream #0:0(und): Video: mpeg4 (Simple Profile) (mp4v / 0x7634706D), yuv420p, 240x180 [SAR 1:1 DAR 4:3], 1858 kb/s, 35 fps, 35 tbr, 17920 tbn, 35 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mpeg4 (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x5daa6b1f6140] using SAR=1/1\n",
      "[libx264 @ 0x5daa6b1f6140] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
      "[libx264 @ 0x5daa6b1f6140] profile High, level 1.3, 4:2:0, 8-bit\n",
      "[libx264 @ 0x5daa6b1f6140] 264 - core 161 r3030M 8bd6d28 - H.264/MPEG-4 AVC codec - Copyleft 2003-2020 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'train_dir/doom_health_gathering_supreme_2222/replay.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf58.45.100\n",
      "    Stream #0:0(und): Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 240x180 [SAR 1:1 DAR 4:3], q=-1--1, 35 fps, 17920 tbn, 35 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      encoder         : Lavc58.91.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "frame=20926 fps=1195 q=-1.0 Lsize=   41017kB time=00:09:57.80 bitrate= 562.1kbits/s speed=34.1x    \n",
      "video:40786kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.566304%\n",
      "[libx264 @ 0x5daa6b1f6140] frame I:190   Avg QP:23.81  size:  5040\n",
      "[libx264 @ 0x5daa6b1f6140] frame P:7740  Avg QP:26.80  size:  2509\n",
      "[libx264 @ 0x5daa6b1f6140] frame B:12996 Avg QP:28.31  size:  1646\n",
      "[libx264 @ 0x5daa6b1f6140] consecutive B-frames: 14.2%  7.3%  5.4% 73.1%\n",
      "[libx264 @ 0x5daa6b1f6140] mb I  I16..4: 15.2% 72.2% 12.6%\n",
      "[libx264 @ 0x5daa6b1f6140] mb P  I16..4:  3.8% 17.9%  5.5%  P16..4: 33.3% 24.2% 10.6%  0.0%  0.0%    skip: 4.7%\n",
      "[libx264 @ 0x5daa6b1f6140] mb B  I16..4:  0.2%  5.1%  2.8%  B16..8: 38.2% 20.6%  6.6%  direct: 7.8%  skip:18.6%  L0:50.8% L1:34.8% BI:14.4%\n",
      "[libx264 @ 0x5daa6b1f6140] 8x8 transform intra:65.1% inter:62.5%\n",
      "[libx264 @ 0x5daa6b1f6140] coded y,uvDC,uvAC intra: 68.4% 70.7% 36.3% inter: 45.8% 17.9% 4.3%\n",
      "[libx264 @ 0x5daa6b1f6140] i16 v,h,dc,p: 65%  4% 30%  1%\n",
      "[libx264 @ 0x5daa6b1f6140] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 21% 10% 32%  5%  6%  5%  8%  5%  8%\n",
      "[libx264 @ 0x5daa6b1f6140] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 59%  9% 10%  3%  4%  3%  5%  3%  4%\n",
      "[libx264 @ 0x5daa6b1f6140] i8c dc,h,v,p: 55% 20% 22%  2%\n",
      "[libx264 @ 0x5daa6b1f6140] Weighted P-Frames: Y:5.8% UV:0.3%\n",
      "[libx264 @ 0x5daa6b1f6140] ref P L0: 60.1% 13.6% 16.5%  9.2%  0.5%\n",
      "[libx264 @ 0x5daa6b1f6140] ref B L0: 88.8%  8.5%  2.7%\n",
      "[libx264 @ 0x5daa6b1f6140] ref B L1: 96.1%  3.9%\n",
      "[libx264 @ 0x5daa6b1f6140] kb/s:558.82\n",
      "\u001b[36m[2024-07-04 19:46:31,259][19305] Replay video saved to train_dir/doom_health_gathering_supreme_2222/replay.mp4!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## Start the training, this should take around 15 minutes\n",
    "register_vizdoom_components()\n",
    "\n",
    "env = \"doom_health_gathering_supreme\"\n",
    "\n",
    "cfg = parse_vizdoom_cfg(\n",
    "    argv=[\n",
    "        f\"--env={env}\",\n",
    "        \"--num_workers=1\",\n",
    "        \"--save_video\",\n",
    "        \"--no_render\",\n",
    "        \"--max_num_episodes=10\",\n",
    "        \"--experiment=doom_health_gathering_supreme_2222\",\n",
    "        \"--train_dir=train_dir\",\n",
    "    ],\n",
    "    evaluation=True,\n",
    ")\n",
    "status = enjoy(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
